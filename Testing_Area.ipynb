{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Compression Testing"
      ],
      "metadata": {
        "id": "4xYfPKO1s-Nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install -q --upgrade pip\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse\n",
        "from scipy.sparse import coo_matrix\n",
        "import pickle\n",
        "!pip install zarr -q\n",
        "import zarr\n",
        "import gzip\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "!pip install tables -q\n",
        "!pip install pyarrow -q\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import shutil\n",
        "!pip install zstandard -q\n",
        "import zstandard as zstd\n",
        "import scipy.sparse as sp"
      ],
      "metadata": {
        "id": "2kefuIeCGHyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialization**"
      ],
      "metadata": {
        "id": "dyijMBhLbYc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if we are in Google Colab (google.colab will not be available locally)\n",
        "try:\n",
        "    import google.colab\n",
        "    is_colab = True\n",
        "except ImportError:\n",
        "    is_colab = False\n",
        "\n",
        "if is_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Define source and destination paths\n",
        "    source_path = '/content/drive/MyDrive/FH OberÃ¶sterreich/Master Thesis/Data'\n",
        "    destination_path = '/content/Data'\n",
        "\n",
        "    # Copy the entire directory\n",
        "    shutil.copytree(source_path, destination_path, dirs_exist_ok=True)"
      ],
      "metadata": {
        "id": "d2Rvn4ItBLos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sparse_3d_array(filename, precision=None):\n",
        "    # Load the saved sparse data\n",
        "    data = np.loadtxt(filename)\n",
        "    indices = data[:, 0].astype(int)\n",
        "    values = data[:, 1]\n",
        "\n",
        "    # Apply precision if specified\n",
        "    if precision is not None:\n",
        "        values = np.round(values, decimals=precision)\n",
        "\n",
        "    # Extract shape from the file using regular expression\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            match = re.search(r\"# shape:\\s*\\((.*?)\\)\", line)\n",
        "            if match:\n",
        "                shape = tuple(map(int, match.group(1).split(\",\")))\n",
        "\n",
        "    # Reconstruct the full flattened array with zeros\n",
        "    flat_array = np.zeros(np.prod(shape), dtype=values.dtype)\n",
        "    flat_array[indices] = values\n",
        "\n",
        "    # Reshape back to original 3D shape\n",
        "    return flat_array.reshape(shape)\n",
        "\n",
        "# Convert sparse matrix to DataFrame\n",
        "def sparse_array_to_dataframe(sparse_array):\n",
        "    # Convert sparse CSR matrix to COO format for easy DataFrame creation\n",
        "    sparse_coo = sparse_array.tocoo()\n",
        "\n",
        "    # Create DataFrame with 'row', 'col', and 'value' columns\n",
        "    df = pd.DataFrame({\n",
        "        'row': sparse_coo.row,\n",
        "        'col': sparse_coo.col,\n",
        "        'value': sparse_coo.data\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "# Convert dense array to DataFrame\n",
        "def dense_array_to_dataframe(dense_array):\n",
        "    df = pd.DataFrame(dense_array.flatten(), columns=['value']).reset_index()\n",
        "    df['row'], df['col'] = np.divmod(df['index'].values, dense_array.shape[1])\n",
        "    df = df.drop(columns=['index'])\n",
        "    return df\n",
        "\n",
        "# Add Zarr Saving and Size Calculation\n",
        "def save_dataframe_to_zarr(df, filename):\n",
        "    store = zarr.DirectoryStore(filename)\n",
        "    group = zarr.group(store)\n",
        "\n",
        "    # Check if the path 'data' exists and overwrite\n",
        "    if 'data' in group:\n",
        "        del group['data']  # Remove existing array to avoid error\n",
        "\n",
        "    group.array('data',\n",
        "                df.to_numpy(),\n",
        "                compressor=zarr.Blosc(cname='zstd', clevel=5, shuffle=zarr.Blosc.SHUFFLE),\n",
        "                overwrite=True)  # Allow overwriting\n",
        "\n",
        "    return store\n",
        "\n",
        "def compress_dataframe_parquet(df, compressionType):\n",
        "    # Serialize DataFrame to Parquet format in memory with built-in compression\n",
        "    buffer = io.BytesIO()\n",
        "    pq.write_table(pa.Table.from_pandas(df), buffer, compression=compressionType)  # Parquet's own efficient compression\n",
        "\n",
        "    return buffer.getvalue()\n",
        "\n",
        "# Function to compress DataFrame using pickle and gzip\n",
        "def compress_dataframe(df):\n",
        "    df = df.astype(pd.SparseDtype(\"float32\", np.nan))\n",
        "    return gzip.compress(pickle.dumps(df))\n",
        "\n",
        "def compress_dataframe_parquet_gzip(df, compressionType):\n",
        "    # Serialize DataFrame to Parquet format in memory with built-in compression\n",
        "    buffer = io.BytesIO()\n",
        "    pq.write_table(pa.Table.from_pandas(df), buffer, compression=compressionType)  # Parquet's own efficient compression\n",
        "\n",
        "    # Further compress with gzip\n",
        "    return gzip.compress(buffer.getvalue())\n",
        "\n",
        "def print_reduction(original_size, compressed_size, format_name):\n",
        "    reduction = (original_size - compressed_size) / original_size * 100\n",
        "    print(f\"Reduction with {format_name}: {reduction:.2f}%\")"
      ],
      "metadata": {
        "id": "ZPdVrzAfbXu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test between Sparse and Dense Array Compression"
      ],
      "metadata": {
        "id": "W1mj_JRiptRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert a dense array for comparison\n",
        "dense_array = load_sparse_3d_array('Data/Wiki_SparseArray_0.txt')\n",
        "df_dense = dense_array_to_dataframe(dense_array)\n",
        "\n",
        "# Reshape the dense array to 2D (if it's 3D, flatten it accordingly)\n",
        "dense_array_2d = dense_array.reshape(-1, dense_array.shape[-1])  # Flatten to 2D if necessary\n",
        "\n",
        "# Convert the 2D array to a sparse matrix using CSR format\n",
        "sparse_array = scipy.sparse.csr_matrix(dense_array_2d)\n",
        "df_sparse = sparse_array_to_dataframe(sparse_array)\n",
        "\n",
        "for df, array_type in zip([df_sparse, df_dense], ['Sparse Array', 'Dense Array']):\n",
        "    print(f\"\\n--- Processing {array_type} ---\")\n",
        "\n",
        "    output_dir = os.path.expanduser('~/Temp')\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Calculate original DataFrame size in memory\n",
        "    original_size = df.memory_usage(deep=True).sum()\n",
        "\n",
        "    # Parquet\n",
        "    parquet_gzip_filename = os.path.join(output_dir, f'data_{array_type.lower().replace(\" \", \"_\")}_gzip.parquet')\n",
        "    df.to_parquet(parquet_gzip_filename, compression='gzip')\n",
        "    parquet_gzip_size = os.path.getsize(parquet_gzip_filename)\n",
        "    os.remove(parquet_gzip_filename)\n",
        "\n",
        "    # HDF5\n",
        "    hdf5_filename = os.path.join(output_dir, f'data_{array_type.lower().replace(\" \", \"_\")}.h5')\n",
        "    with pd.HDFStore(hdf5_filename, mode='w', complib='blosc') as store:\n",
        "        store.put('dataset', df, format='table', data_columns=True)\n",
        "    hdf5_size = os.path.getsize(hdf5_filename)\n",
        "    os.remove(hdf5_filename)\n",
        "\n",
        "    # CSV\n",
        "    csv_filename = os.path.join(output_dir, f'data_{array_type.lower().replace(\" \", \"_\")}.csv.gz')\n",
        "    df.to_csv(csv_filename, index=False, compression='gzip')\n",
        "    csv_size = os.path.getsize(csv_filename)\n",
        "    os.remove(csv_filename)\n",
        "\n",
        "    # Zarr\n",
        "    zarr_filename = os.path.join(output_dir, f'data_{array_type.lower().replace(\" \", \"_\")}.zarr')\n",
        "    save_dataframe_to_zarr(df, zarr_filename)\n",
        "    zarr_size = sum(os.path.getsize(os.path.join(dirpath, file)) for dirpath, _, files in os.walk(zarr_filename) for file in files)\n",
        "    shutil.rmtree(zarr_filename)\n",
        "\n",
        "    shutil.rmtree(output_dir)\n",
        "\n",
        "    # Pickle + Compression Sizes\n",
        "    pickle_size = len(compress_dataframe(df))\n",
        "    snappy_size = len(compress_dataframe_parquet(df, 'snappy'))\n",
        "    zstd_size = len(compress_dataframe_parquet(df, 'zstd'))\n",
        "    snappy_gzip_size = len(compress_dataframe_parquet_gzip(df, 'snappy'))\n",
        "    zstd_gzip_size = len(compress_dataframe_parquet_gzip(df, 'zstd'))\n",
        "\n",
        "    # Output Results\n",
        "    print(f\"Original size (bytes): {original_size}\")\n",
        "    print(f\"Parquet (gzip) size (bytes): {parquet_gzip_size}\")\n",
        "    print(f\"HDF5 size (bytes): {hdf5_size}\")\n",
        "    print(f\"CSV size (bytes): {csv_size}\")\n",
        "    print(f\"Zarr size (bytes): {zarr_size}\")\n",
        "    print(f\"Parquet (Snappy) size (bytes): {snappy_size}\")\n",
        "    print(f\"Parquet (ZSTD) size (bytes): {zstd_size}\")\n",
        "    print(f\"Pickle size (bytes): {pickle_size}\")\n",
        "    print(f\"Pickle with Parquet (Snappy) size (bytes): {snappy_gzip_size}\")\n",
        "    print(f\"Pickle with Parquet (ZSTD) size (bytes): {zstd_gzip_size}\")\n",
        "\n",
        "    # Print Reductions\n",
        "    print_reduction(original_size, parquet_gzip_size, 'Parquet (gzip)')\n",
        "    print_reduction(original_size, hdf5_size, 'HDF5')\n",
        "    print_reduction(original_size, csv_size, 'CSV')\n",
        "    print_reduction(original_size, snappy_size, 'Snappy')\n",
        "    print_reduction(original_size, zstd_size, 'ZSTD')\n",
        "    print_reduction(original_size, zarr_size, 'Zarr')\n",
        "    print_reduction(original_size, pickle_size, 'Pickle')\n",
        "    print_reduction(original_size, snappy_gzip_size, 'Parquet & GZIP (Snappy)')\n",
        "    print_reduction(original_size, zstd_gzip_size, 'Parquet & GZIP (ZSTD)')\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09eEyhn-dlxi",
        "outputId": "9fdae43e-0764-4ca5-bbff-ca4d493067d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processing Sparse Array ---\n",
            "Original size (bytes): 3400896\n",
            "Parquet (gzip) size (bytes): 1668160\n",
            "HDF5 size (bytes): 6975322\n",
            "CSV size (bytes): 1341197\n",
            "Zarr size (bytes): 1492893\n",
            "Parquet (Snappy) size (bytes): 2147925\n",
            "Parquet (ZSTD) size (bytes): 1829172\n",
            "Pickle size (bytes): 1775284\n",
            "Pickle with Parquet (Snappy) size (bytes): 1652950\n",
            "Pickle with Parquet (ZSTD) size (bytes): 1788891\n",
            "Reduction with Parquet (gzip): 50.95%\n",
            "Reduction with HDF5: -105.10%\n",
            "Reduction with CSV: 60.56%\n",
            "Reduction with Snappy: 36.84%\n",
            "Reduction with ZSTD: 46.21%\n",
            "Reduction with Zarr: 56.10%\n",
            "Reduction with Pickle: 47.80%\n",
            "Reduction with Parquet & GZIP (Snappy): 51.40%\n",
            "Reduction with Parquet & GZIP (ZSTD): 47.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compression with different normalizations"
      ],
      "metadata": {
        "id": "qi13q-dG-t9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Integer Normalization**"
      ],
      "metadata": {
        "id": "YRusrU7Jg6b4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_array_compression(df_dense, df_sparse, fixed=True):\n",
        "    # Iterate over the dense and sparse arrays\n",
        "    for df, array_type in zip([df_sparse, df_dense], ['Sparse Array', 'Dense Array']):\n",
        "        print(f\"\\n--- Processing {array_type} ---\")\n",
        "\n",
        "        # Test with different integer types\n",
        "        for int_type in ['uint8', 'int8', 'uint16', 'int16', 'uint32', 'int32']:\n",
        "            print(f\"\\n--- Testing with {int_type} ---\")\n",
        "\n",
        "            # Output directory\n",
        "            output_dir = os.path.expanduser('~/Temp')\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "            # Normalize the data\n",
        "            if fixed:\n",
        "                normalized_data, denormalized_data = normalize_to_integer(df.values, int_type)\n",
        "            else:\n",
        "                normalized_data, denormalized_data = normalize_dynamic_ranges(df.values, int_type)\n",
        "\n",
        "            # Convert the normalized data back into a DataFrame to be saved\n",
        "            df_normalized = pd.DataFrame(normalized_data, columns=df.columns)\n",
        "\n",
        "            # Calculate the MAE and MSE\n",
        "            mae, mse = calculate_loss(df.values, denormalized_data)\n",
        "\n",
        "            # Output the MAE and MSE\n",
        "            print(f\"MAE: {mae}\")\n",
        "            print(f\"MSE: {mse}\")\n",
        "\n",
        "            # Calculate original DataFrame size in memory\n",
        "            original_size = df.memory_usage(deep=True).sum()\n",
        "\n",
        "            # Step 1: Save DataFrame to Parquet with gzip compression\n",
        "            parquet_gzip_filename = os.path.join(output_dir, f'data_{array_type.lower().replace(\" \", \"_\")}_gzip.parquet')\n",
        "            df_normalized.to_parquet(parquet_gzip_filename, compression='gzip')\n",
        "            parquet_gzip_size = os.path.getsize(parquet_gzip_filename)\n",
        "\n",
        "            # Remove the Parquet file after processing\n",
        "            os.remove(parquet_gzip_filename)\n",
        "\n",
        "            # Step 2: Save DataFrame to HDF5 with gzip compression\n",
        "            hdf5_filename = os.path.join(output_dir, f'data_{array_type.lower().replace(\" \", \"_\")}.h5')\n",
        "            with pd.HDFStore(hdf5_filename, mode='w', complib='blosc') as store:\n",
        "                store.put('dataset', df_normalized, format='table', data_columns=True)\n",
        "            hdf5_size = os.path.getsize(hdf5_filename)\n",
        "\n",
        "            # Remove the HDF5 file after processing\n",
        "            os.remove(hdf5_filename)\n",
        "\n",
        "            # Step 3: Save DataFrame to CSV (compressed)\n",
        "            csv_filename = os.path.join(output_dir, f'data_{array_type.lower().replace(\" \", \"_\")}.csv.gz')\n",
        "            df_normalized.to_csv(csv_filename, index=False, compression='gzip')\n",
        "            csv_size = os.path.getsize(csv_filename)\n",
        "\n",
        "            # Remove the CSV file after processing\n",
        "            os.remove(csv_filename)\n",
        "\n",
        "            # Remove the output directory after processing\n",
        "            shutil.rmtree(output_dir)\n",
        "\n",
        "            # Pickle + Compression Sizes\n",
        "            pickle_size = len(compress_dataframe(df_normalized))\n",
        "            snappy_size = len(compress_dataframe_parquet(df_normalized, 'snappy'))\n",
        "            zstd_size = len(compress_dataframe_parquet(df_normalized, 'zstd'))\n",
        "            snappy_gzip_size = len(compress_dataframe_parquet_gzip(df_normalized, 'snappy'))\n",
        "            zstd_gzip_size = len(compress_dataframe_parquet_gzip(df_normalized, 'zstd'))\n",
        "\n",
        "            # Output Results\n",
        "            print(f\"Original size (bytes): {original_size}\")\n",
        "            print(f\"Parquet (gzip) size (bytes): {parquet_gzip_size}\")\n",
        "            print(f\"HDF5 size (bytes): {hdf5_size}\")\n",
        "            print(f\"CSV size (bytes): {csv_size}\")\n",
        "            print(f\"Zarr size (bytes): {zarr_size}\")\n",
        "            print(f\"Parquet (Snappy) size (bytes): {snappy_size}\")\n",
        "            print(f\"Parquet (ZSTD) size (bytes): {zstd_size}\")\n",
        "            print(f\"Pickle size (bytes): {pickle_size}\")\n",
        "            print(f\"Pickle with Parquet (Snappy) size (bytes): {snappy_gzip_size}\")\n",
        "            print(f\"Pickle with Parquet (ZSTD) size (bytes): {zstd_gzip_size}\")\n",
        "\n",
        "            # Print Reductions\n",
        "            print_reduction(original_size, parquet_gzip_size, 'Parquet (gzip)')\n",
        "            print_reduction(original_size, hdf5_size, 'HDF5')\n",
        "            print_reduction(original_size, csv_size, 'CSV')\n",
        "            print_reduction(original_size, snappy_size, 'Snappy')\n",
        "            print_reduction(original_size, zstd_size, 'ZSTD')\n",
        "            print_reduction(original_size, zarr_size, 'Zarr')\n",
        "            print_reduction(original_size, pickle_size, 'Pickle')\n",
        "            print_reduction(original_size, snappy_gzip_size, 'Parquet & GZIP (Snappy)')\n",
        "            print_reduction(original_size, zstd_gzip_size, 'Parquet & GZIP (ZSTD)')\n",
        "        break\n",
        "\n",
        "def calculate_loss(original_data, denormalized_data):\n",
        "    # Calculate the absolute difference between the original and denormalized data\n",
        "    loss = np.abs(original_data - denormalized_data)\n",
        "\n",
        "    # Calculate the Mean Absolute Error (MAE)\n",
        "    mae = np.mean(loss)\n",
        "\n",
        "    # Calculate the percentage loss in terms of the range of the original data\n",
        "    max_val = np.max(original_data)\n",
        "    min_val = np.min(original_data)\n",
        "    range_val = max_val - min_val\n",
        "    loss_percentage = (mae / range_val) * 100\n",
        "\n",
        "    return mae, loss_percentage\n",
        "\n",
        "# Define integer ranges based on the selected type\n",
        "int_ranges = {\n",
        "    'uint8': (0, 255),\n",
        "    'int8': (-128, 127),\n",
        "    'uint16': (0, 65535),\n",
        "    'int16': (-32768, 32767),\n",
        "    'uint32': (0, 4294967295),\n",
        "    'int32': (-2147483648, 2147483647)\n",
        "}"
      ],
      "metadata": {
        "id": "gV3VNkNj0WZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 Fixed Ranges**"
      ],
      "metadata": {
        "id": "GpKDzH8kzhQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_to_integer(data, int_type='uint8'):\n",
        "    if int_type not in int_ranges:\n",
        "        raise ValueError(\"Unsupported integer type. Choose from 'uint8', 'int8', 'uint16', 'int16', 'uint32', 'int32'.\")\n",
        "\n",
        "    min_int, max_int = int_ranges[int_type]\n",
        "\n",
        "    # Calculate min and max of the data\n",
        "    min_val = np.min(data)\n",
        "    max_val = np.max(data)\n",
        "\n",
        "    # Normalize data to integer range\n",
        "    normalized = np.round((data - min_val) * (max_int - min_int) / (max_val - min_val) + min_int).astype(int)\n",
        "\n",
        "    # Denormalize back to the original float range for comparison\n",
        "    denormalized = (normalized - min_int) * (max_val - min_val) / (max_int - min_int) + min_val\n",
        "\n",
        "    return normalized, denormalized\n",
        "\n",
        "process_array_compression(df_dense, df_sparse, fixed=True)"
      ],
      "metadata": {
        "id": "o5nq9rxh-1bj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f59c818-2232-4930-e8dc-c09a0beb2544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processing Sparse Array ---\n",
            "\n",
            "--- Testing with uint8 ---\n",
            "MAE: 36.33142398571479\n",
            "MSE: 0.072276918333444\n",
            "Original size (bytes): 3400896\n",
            "Parquet (gzip) size (bytes): 3836\n",
            "HDF5 size (bytes): 7238937\n",
            "CSV size (bytes): 4554\n",
            "Zarr size (bytes): 1492893\n",
            "Parquet (Snappy) size (bytes): 4811\n",
            "Parquet (ZSTD) size (bytes): 3767\n",
            "Pickle size (bytes): 892268\n",
            "Pickle with Parquet (Snappy) size (bytes): 2553\n",
            "Pickle with Parquet (ZSTD) size (bytes): 2388\n",
            "Reduction with Parquet (gzip): 99.89%\n",
            "Reduction with HDF5: -112.85%\n",
            "Reduction with CSV: 99.87%\n",
            "Reduction with Snappy: 99.86%\n",
            "Reduction with ZSTD: 99.89%\n",
            "Reduction with Zarr: 56.10%\n",
            "Reduction with Pickle: 73.76%\n",
            "Reduction with Parquet & GZIP (Snappy): 99.92%\n",
            "Reduction with Parquet & GZIP (ZSTD): 99.93%\n",
            "\n",
            "--- Testing with int8 ---\n",
            "MAE: 36.33142398571479\n",
            "MSE: 0.072276918333444\n",
            "Original size (bytes): 3400896\n",
            "Parquet (gzip) size (bytes): 3842\n",
            "HDF5 size (bytes): 7239970\n",
            "CSV size (bytes): 8222\n",
            "Zarr size (bytes): 1492893\n",
            "Parquet (Snappy) size (bytes): 4809\n",
            "Parquet (ZSTD) size (bytes): 3810\n",
            "Pickle size (bytes): 891494\n",
            "Pickle with Parquet (Snappy) size (bytes): 2558\n",
            "Pickle with Parquet (ZSTD) size (bytes): 2442\n",
            "Reduction with Parquet (gzip): 99.89%\n",
            "Reduction with HDF5: -112.88%\n",
            "Reduction with CSV: 99.76%\n",
            "Reduction with Snappy: 99.86%\n",
            "Reduction with ZSTD: 99.89%\n",
            "Reduction with Zarr: 56.10%\n",
            "Reduction with Pickle: 73.79%\n",
            "Reduction with Parquet & GZIP (Snappy): 99.92%\n",
            "Reduction with Parquet & GZIP (ZSTD): 99.93%\n",
            "\n",
            "--- Testing with uint16 ---\n",
            "MAE: 0.196142740390279\n",
            "MSE: 0.0003902019594514165\n",
            "Original size (bytes): 3400896\n",
            "Parquet (gzip) size (bytes): 352577\n",
            "HDF5 size (bytes): 7476773\n",
            "CSV size (bytes): 578282\n",
            "Zarr size (bytes): 1492893\n",
            "Parquet (Snappy) size (bytes): 604025\n",
            "Parquet (ZSTD) size (bytes): 248437\n",
            "Pickle size (bytes): 1130935\n",
            "Pickle with Parquet (Snappy) size (bytes): 279188\n",
            "Pickle with Parquet (ZSTD) size (bytes): 190844\n",
            "Reduction with Parquet (gzip): 89.63%\n",
            "Reduction with HDF5: -119.85%\n",
            "Reduction with CSV: 83.00%\n",
            "Reduction with Snappy: 82.24%\n",
            "Reduction with ZSTD: 92.69%\n",
            "Reduction with Zarr: 56.10%\n",
            "Reduction with Pickle: 66.75%\n",
            "Reduction with Parquet & GZIP (Snappy): 91.79%\n",
            "Reduction with Parquet & GZIP (ZSTD): 94.39%\n",
            "\n",
            "--- Testing with int16 ---\n",
            "MAE: 0.196142740390279\n",
            "MSE: 0.0003902019594514165\n",
            "Original size (bytes): 3400896\n",
            "Parquet (gzip) size (bytes): 352606\n",
            "HDF5 size (bytes): 7477748\n",
            "CSV size (bytes): 617707\n",
            "Zarr size (bytes): 1492893\n",
            "Parquet (Snappy) size (bytes): 604026\n",
            "Parquet (ZSTD) size (bytes): 248520\n",
            "Pickle size (bytes): 1121045\n",
            "Pickle with Parquet (Snappy) size (bytes): 279272\n",
            "Pickle with Parquet (ZSTD) size (bytes): 191019\n",
            "Reduction with Parquet (gzip): 89.63%\n",
            "Reduction with HDF5: -119.88%\n",
            "Reduction with CSV: 81.84%\n",
            "Reduction with Snappy: 82.24%\n",
            "Reduction with ZSTD: 92.69%\n",
            "Reduction with Zarr: 56.10%\n",
            "Reduction with Pickle: 67.04%\n",
            "Reduction with Parquet & GZIP (Snappy): 91.79%\n",
            "Reduction with Parquet & GZIP (ZSTD): 94.38%\n",
            "\n",
            "--- Testing with uint32 ---\n",
            "MAE: 2.9986796745609365e-06\n",
            "MSE: 5.9655059496599344e-09\n",
            "Original size (bytes): 3400896\n",
            "Parquet (gzip) size (bytes): 1236189\n",
            "HDF5 size (bytes): 7935276\n",
            "CSV size (bytes): 1740753\n",
            "Zarr size (bytes): 1492893\n",
            "Parquet (Snappy) size (bytes): 1643183\n",
            "Parquet (ZSTD) size (bytes): 1171265\n",
            "Pickle size (bytes): 1877131\n",
            "Pickle with Parquet (Snappy) size (bytes): 1290944\n",
            "Pickle with Parquet (ZSTD) size (bytes): 1145819\n",
            "Reduction with Parquet (gzip): 63.65%\n",
            "Reduction with HDF5: -133.33%\n",
            "Reduction with CSV: 48.81%\n",
            "Reduction with Snappy: 51.68%\n",
            "Reduction with ZSTD: 65.56%\n",
            "Reduction with Zarr: 56.10%\n",
            "Reduction with Pickle: 44.80%\n",
            "Reduction with Parquet & GZIP (Snappy): 62.04%\n",
            "Reduction with Parquet & GZIP (ZSTD): 66.31%\n",
            "\n",
            "--- Testing with int32 ---\n",
            "MAE: 2.9986796745609365e-06\n",
            "MSE: 5.9655059496599344e-09\n",
            "Original size (bytes): 3400896\n",
            "Parquet (gzip) size (bytes): 1235401\n",
            "HDF5 size (bytes): 7936788\n",
            "CSV size (bytes): 1817165\n",
            "Zarr size (bytes): 1492893\n",
            "Parquet (Snappy) size (bytes): 1643209\n",
            "Parquet (ZSTD) size (bytes): 1189454\n",
            "Pickle size (bytes): 1655451\n",
            "Pickle with Parquet (Snappy) size (bytes): 1291004\n",
            "Pickle with Parquet (ZSTD) size (bytes): 1165310\n",
            "Reduction with Parquet (gzip): 63.67%\n",
            "Reduction with HDF5: -133.37%\n",
            "Reduction with CSV: 46.57%\n",
            "Reduction with Snappy: 51.68%\n",
            "Reduction with ZSTD: 65.03%\n",
            "Reduction with Zarr: 56.10%\n",
            "Reduction with Pickle: 51.32%\n",
            "Reduction with Parquet & GZIP (Snappy): 62.04%\n",
            "Reduction with Parquet & GZIP (ZSTD): 65.74%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dynamic Ranges**"
      ],
      "metadata": {
        "id": "n1_D-Sbozc2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_dynamic_ranges(data, int_type='uint8'):\n",
        "    if int_type not in int_ranges:\n",
        "        raise ValueError(\"Unsupported integer type. Choose from 'uint8', 'int8', 'uint16', 'int16', 'uint32', 'int32'.\")\n",
        "\n",
        "    min_int, max_int = int_ranges[int_type]\n",
        "\n",
        "    # Get the dynamic ranges split points\n",
        "    min_val = np.min(data)\n",
        "    max_val = np.max(data)\n",
        "\n",
        "    # Define split points based on your logic (-1, 0, 1)\n",
        "    split_points = [-1, 0, 1]\n",
        "\n",
        "    # Create a placeholder for normalized data\n",
        "    normalized_data = np.zeros_like(data, dtype=int)\n",
        "\n",
        "    def normalize_range(sub_data, start, end):\n",
        "        # Normalize within the specific range\n",
        "        return np.round((sub_data - start) * (max_int - min_int) / (end - start) + min_int).astype(int)\n",
        "\n",
        "    # Apply normalization for each dynamic range\n",
        "    for start, end in zip([min_val] + split_points, split_points + [max_val]):\n",
        "        mask = (data >= start) & (data < end)\n",
        "        normalized_data[mask] = normalize_range(data[mask], start, end)\n",
        "\n",
        "    # Denormalize back to original range for comparison\n",
        "    denormalized_data = np.zeros_like(data, dtype=float)\n",
        "    for start, end in zip([min_val] + split_points, split_points + [max_val]):\n",
        "        mask = (normalized_data >= min_int) & (normalized_data <= max_int)\n",
        "        denormalized_data[mask] = (normalized_data[mask] - min_int) * (end - start) / (max_int - min_int) + start\n",
        "\n",
        "    return normalized_data, denormalized_data\n",
        "\n",
        "process_array_compression(df_dense, df_sparse, fixed=False)"
      ],
      "metadata": {
        "id": "CPzS22j7zXmr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5087d22-5f56-4e3b-d3b8-3b9ca917e54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processing Sparse Array ---\n",
            "\n",
            "--- Testing with uint8 ---\n",
            "MAE: 8425.839175954501\n",
            "MSE: 16.762175087071945\n",
            "Original size (bytes): 3400896\n",
            "Parquet (gzip) size (bytes): 180849\n",
            "HDF5 size (bytes): 7399018\n",
            "CSV size (bytes): 345732\n",
            "Zarr size (bytes): 1492893\n",
            "Parquet (Snappy) size (bytes): 216945\n",
            "Parquet (ZSTD) size (bytes): 157202\n",
            "Pickle size (bytes): 1143634\n",
            "Pickle with Parquet (Snappy) size (bytes): 181209\n",
            "Pickle with Parquet (ZSTD) size (bytes): 155714\n",
            "Reduction with Parquet (gzip): 94.68%\n",
            "Reduction with HDF5: -117.56%\n",
            "Reduction with CSV: 89.83%\n",
            "Reduction with Snappy: 93.62%\n",
            "Reduction with ZSTD: 95.38%\n",
            "Reduction with Zarr: 56.10%\n",
            "Reduction with Pickle: 66.37%\n",
            "Reduction with Parquet & GZIP (Snappy): 94.67%\n",
            "Reduction with Parquet & GZIP (ZSTD): 95.42%\n",
            "\n",
            "--- Testing with int8 ---\n",
            "MAE: 8425.760053311724\n",
            "MSE: 16.762017682264673\n",
            "Original size (bytes): 3400896\n",
            "Parquet (gzip) size (bytes): 180921\n",
            "HDF5 size (bytes): 7400076\n",
            "CSV size (bytes): 360959\n",
            "Zarr size (bytes): 1492893\n",
            "Parquet (Snappy) size (bytes): 216925\n",
            "Parquet (ZSTD) size (bytes): 157291\n",
            "Pickle size (bytes): 1143611\n",
            "Pickle with Parquet (Snappy) size (bytes): 181377\n",
            "Pickle with Parquet (ZSTD) size (bytes): 155813\n",
            "Reduction with Parquet (gzip): 94.68%\n",
            "Reduction with HDF5: -117.59%\n",
            "Reduction with CSV: 89.39%\n",
            "Reduction with Snappy: 93.62%\n",
            "Reduction with ZSTD: 95.38%\n",
            "Reduction with Zarr: 56.10%\n",
            "Reduction with Pickle: 66.37%\n",
            "Reduction with Parquet & GZIP (Snappy): 94.67%\n",
            "Reduction with Parquet & GZIP (ZSTD): 95.42%\n",
            "\n",
            "--- Testing with uint16 ---\n",
            "MAE: 8395.880406022325\n",
            "MSE: 16.70257578348809\n",
            "Original size (bytes): 3400896\n",
            "Parquet (gzip) size (bytes): 804860\n",
            "HDF5 size (bytes): 7683969\n",
            "CSV size (bytes): 1056745\n",
            "Zarr size (bytes): 1492893\n",
            "Parquet (Snappy) size (bytes): 1116787\n",
            "Parquet (ZSTD) size (bytes): 592848\n",
            "Pickle size (bytes): 1550229\n",
            "Pickle with Parquet (Snappy) size (bytes): 741713\n",
            "Pickle with Parquet (ZSTD) size (bytes): 534740\n",
            "Reduction with Parquet (gzip): 76.33%\n",
            "Reduction with HDF5: -125.94%\n",
            "Reduction with CSV: 68.93%\n",
            "Reduction with Snappy: 67.16%\n",
            "Reduction with ZSTD: 82.57%\n",
            "Reduction with Zarr: 56.10%\n",
            "Reduction with Pickle: 54.42%\n",
            "Reduction with Parquet & GZIP (Snappy): 78.19%\n",
            "Reduction with Parquet & GZIP (ZSTD): 84.28%\n",
            "\n",
            "--- Testing with int16 ---\n",
            "MAE: 8395.801591249754\n",
            "MSE: 16.70241899115089\n",
            "Original size (bytes): 3400896\n",
            "Parquet (gzip) size (bytes): 805494\n",
            "HDF5 size (bytes): 7684918\n",
            "CSV size (bytes): 1103392\n",
            "Zarr size (bytes): 1492893\n",
            "Parquet (Snappy) size (bytes): 1116843\n",
            "Parquet (ZSTD) size (bytes): 620854\n",
            "Pickle size (bytes): 1548035\n",
            "Pickle with Parquet (Snappy) size (bytes): 741977\n",
            "Pickle with Parquet (ZSTD) size (bytes): 562675\n",
            "Reduction with Parquet (gzip): 76.32%\n",
            "Reduction with HDF5: -125.97%\n",
            "Reduction with CSV: 67.56%\n",
            "Reduction with Snappy: 67.16%\n",
            "Reduction with ZSTD: 81.74%\n",
            "Reduction with Zarr: 56.10%\n",
            "Reduction with Pickle: 54.48%\n",
            "Reduction with Parquet & GZIP (Snappy): 78.18%\n",
            "Reduction with Parquet & GZIP (ZSTD): 83.46%\n",
            "\n",
            "--- Testing with uint32 ---\n",
            "MAE: 8395.760964536245\n",
            "MSE: 16.702338169280114\n",
            "Original size (bytes): 3400896\n",
            "Parquet (gzip) size (bytes): 1533880\n",
            "HDF5 size (bytes): 8227563\n",
            "CSV size (bytes): 2123814\n",
            "Zarr size (bytes): 1492893\n",
            "Parquet (Snappy) size (bytes): 1960147\n",
            "Parquet (ZSTD) size (bytes): 1380814\n",
            "Pickle size (bytes): 1970489\n",
            "Pickle with Parquet (Snappy) size (bytes): 1598407\n",
            "Pickle with Parquet (ZSTD) size (bytes): 1341423\n",
            "Reduction with Parquet (gzip): 54.90%\n",
            "Reduction with HDF5: -141.92%\n",
            "Reduction with CSV: 37.55%\n",
            "Reduction with Snappy: 42.36%\n",
            "Reduction with ZSTD: 59.40%\n",
            "Reduction with Zarr: 56.10%\n",
            "Reduction with Pickle: 42.06%\n",
            "Reduction with Parquet & GZIP (Snappy): 53.00%\n",
            "Reduction with Parquet & GZIP (ZSTD): 60.56%\n",
            "\n",
            "--- Testing with int32 ---\n",
            "MAE: 8395.682150966271\n",
            "MSE: 16.702181379335332\n",
            "Original size (bytes): 3400896\n",
            "Parquet (gzip) size (bytes): 1534153\n",
            "HDF5 size (bytes): 8228623\n",
            "CSV size (bytes): 2190989\n",
            "Zarr size (bytes): 1492893\n",
            "Parquet (Snappy) size (bytes): 1954677\n",
            "Parquet (ZSTD) size (bytes): 1487555\n",
            "Pickle size (bytes): 1980310\n",
            "Pickle with Parquet (Snappy) size (bytes): 1595585\n",
            "Pickle with Parquet (ZSTD) size (bytes): 1453310\n",
            "Reduction with Parquet (gzip): 54.89%\n",
            "Reduction with HDF5: -141.95%\n",
            "Reduction with CSV: 35.58%\n",
            "Reduction with Snappy: 42.52%\n",
            "Reduction with ZSTD: 56.26%\n",
            "Reduction with Zarr: 56.10%\n",
            "Reduction with Pickle: 41.77%\n",
            "Reduction with Parquet & GZIP (Snappy): 53.08%\n",
            "Reduction with Parquet & GZIP (ZSTD): 57.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Testing"
      ],
      "metadata": {
        "id": "mbsj-jO6tUHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import urllib.request\n",
        "!pip install -q stanza\n",
        "import stanza\n",
        "# Suppress logging from stanza\n",
        "nlp = stanza.Pipeline('en', verbose=False)\n",
        "stanza.download('en', verbose=False)\n",
        "# Compute cosine similarity\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "from collections import defaultdict\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances"
      ],
      "metadata": {
        "id": "OcwDp2YEJRj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create train set**"
      ],
      "metadata": {
        "id": "4xVOsKYBIaJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def createTrainSet():\n",
        "    global sentences, sentencesStructure\n",
        "\n",
        "    file_path = \"TheVerdict.txt\"\n",
        "    url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            text_data = response.read().decode('utf-8')\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text_data)\n",
        "    else:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text_data = file.read()\n",
        "\n",
        "    # Remove any newline characters and extra spaces\n",
        "    text_data = text_data.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "    # Process the text with Stanza to extract sentences\n",
        "    doc = nlp(text_data)\n",
        "    sentencesStructure = [[sentence.text for sentence in doc.sentences]]\n",
        "\n",
        "    # Flatten the list of sentences correctly\n",
        "    sentences = [sentence for sublist in sentencesStructure for sentence in sublist]\n",
        "\n",
        "    print(f\"Created a train set with {len(sentences)} sentences\")\n",
        "    return sentences\n",
        "\n",
        "#sentences = createTrainSet()"
      ],
      "metadata": {
        "id": "y1PkoWyYId8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wikipedia API endpoint for querying\n",
        "WIKIPEDIA_API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "def fetch_article_titles(category):\n",
        "    \"\"\"Fetch a list of article titles from a Wikipedia category.\"\"\"\n",
        "    titles = []\n",
        "    params = {\n",
        "        'action': 'query',\n",
        "        'format': 'json',\n",
        "        'list': 'categorymembers',\n",
        "        'cmtitle': f'Category:{category}',\n",
        "        'cmlimit': 'max'  # Get the maximum number of articles\n",
        "    }\n",
        "    response = requests.get(WIKIPEDIA_API_URL, params=params).json()\n",
        "    for page in response.get('query', {}).get('categorymembers', []):\n",
        "        titles.append(page['title'])\n",
        "    return titles\n",
        "\n",
        "def fetch_and_parse_content(title):\n",
        "    \"\"\"Fetch the content of a Wikipedia article and parse it.\"\"\"\n",
        "    params = {\n",
        "        'action': 'query',\n",
        "        'format': 'json',\n",
        "        'titles': title,\n",
        "        'prop': 'extracts',\n",
        "        'explaintext': True\n",
        "    }\n",
        "    response = requests.get(WIKIPEDIA_API_URL, params=params).json()\n",
        "    pages = response.get('query', {}).get('pages', {})\n",
        "    page = next(iter(pages.values()))\n",
        "    content = page.get('extract', '')\n",
        "\n",
        "    return content\n",
        "\n",
        "def clean_wikipedia_content(sentences):\n",
        "    cleaned_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Remove references, citations, and template placeholders\n",
        "        sentence = re.sub(r'\\[\\[File:.*?\\]\\]', '', sentence)  # Remove file/image links (e.g., [[File:Image.jpg]])\n",
        "        sentence = re.sub(r'\\[\\[.*?\\|.*?\\]\\]', '', sentence)  # Remove internal links with aliases (e.g., [[Link|Alias]])\n",
        "        sentence = re.sub(r'\\[\\[.*?\\]\\]', '', sentence)      # Remove internal links (e.g., [[Link]])\n",
        "\n",
        "        sentence = re.sub(r'\\{.*?\\}', '', sentence)      # Remove templates (e.g., {{Citation needed}} or any template)\n",
        "        sentence = re.sub(r'==+.*?==+', '', sentence)    # Remove headings (e.g., == Heading ==)\n",
        "        sentence = re.sub(r'===+.*?===+', '', sentence)  # Remove subheadings (e.g., === Subheading ===)\n",
        "        sentence = re.sub(r'<!--.*?-->', '', sentence, flags=re.DOTALL)  # Remove comments\n",
        "        sentence = re.sub(r'\\<.*?\\>', '', sentence)      # Remove any HTML tags (just in case)\n",
        "\n",
        "        # Remove irrelevant sections like \"See also\", \"External Links\", \"References\", \"Further reading\"\n",
        "        sentence = re.sub(r'\\n(See also|External links|References|Further reading)\\n.*?(\\n|$)', '', sentence, flags=re.DOTALL)\n",
        "\n",
        "        # Remove anything inside curly brackets (often for templates or references)\n",
        "        sentence = re.sub(r'\\{.*?\\}', '', sentence)\n",
        "\n",
        "        # Clean up extra spaces\n",
        "        sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "\n",
        "        # Remove short sentences or fragments (less than 3 words)\n",
        "        if len(sentence.split()) > 2:\n",
        "            cleaned_sentences.append(sentence.strip())\n",
        "\n",
        "    return cleaned_sentences\n",
        "\n",
        "# Function to split cleaned content into sentences\n",
        "def split_sentences(content, nlp):\n",
        "    # Process the content with Stanza\n",
        "    doc = nlp(content)\n",
        "    # Remove non-informative sentences (those with just punctuation or short sentences)\n",
        "    sentences = [sentence.text for sentence in doc.sentences if len(sentence.text.split()) > 2]\n",
        "    return sentences\n",
        "\n",
        "def createWikiTrainSet(category):\n",
        "    global sentences, sentencesStructure\n",
        "\n",
        "    # Fetch a list of article titles from the specified category\n",
        "    titles = fetch_article_titles(category)\n",
        "    print(f\"Number of titles fetched from category '{category}': {len(titles)}\")\n",
        "\n",
        "    sentencesStructure = []  # Store all sentences from fetched articles\n",
        "\n",
        "    # Fetch content from each title and split into sentences\n",
        "    for title in titles:\n",
        "        content = fetch_and_parse_content(title)\n",
        "\n",
        "        # Tokenize the paragraph into sentences\n",
        "        sentence_data = split_sentences(content, nlp)\n",
        "\n",
        "        # Clean the Wikipedia content\n",
        "        sentence_data = clean_wikipedia_content(sentence_data)\n",
        "\n",
        "        # Add the list of sentences for this paragraph\n",
        "        sentencesStructure.append(sentence_data)\n",
        "\n",
        "    sentences = [sentence for sublist in sentencesStructure for sentence in sublist]\n",
        "    print(\"Created a training set with \" + str(len(sentences)) + \" sentences\")\n",
        "    return sentences\n",
        "\n",
        "sentences = createWikiTrainSet(\"sports\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmLnrBXeejg_",
        "outputId": "64c13f20-cd59-4eb0-ea42-58f316299243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of titles fetched from category 'sports': 59\n",
            "Created a training set with 444 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generatedSourceSentences = ['meeting, sports meeting admitting a single, meeting this meeting meeting this meeting meeting this meeting meeting some cases meeting meeting meeting this meeting meeting this definition.', 'with major competitions sports this meeting meeting this and skills, generally., some are cases, some as system meeting meeting this meeting meeting.', ', sports meeting, format, sports meeting some format, producing meeting sports meeting sports participant athleticism major competitions sports sports meeting.', 'sports provide tie-breaking methods any ensure one winner., some with some sports, which done.', ', which different.', 'as sports, such, such, some individuals, some, such, regular as the, such some admitting admitting, some, some individuals, some, such as sports meeting sports, such- some, suchdraw sports, some meeting some the some and, some sports as sports, sports season.. as sports meeting, such as the some cases, such numbers individuals cases, some, such, such as sports as the, such as sports meeting some individuals cases,, some, some as sports meeting, sports.', 'draw one, such form and skills.', ', are others one.', 'a spectators, followed sports being sports.', '. a single skills individuals, which a single person person sports different meeting sports.']"
      ],
      "metadata": {
        "id": "R6sXbmqzFqS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layerAmount = 2\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,   # Vocabulary size\n",
        "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
        "    \"emb_dim\": 768,        # Embedding dimension\n",
        "    \"n_heads\": layerAmount,         # Number of attention heads\n",
        "    \"n_layers\": layerAmount,        # Number of layers\n",
        "    \"drop_rate\": 0.1,      # Dropout rate\n",
        "    \"qkv_bias\": False      # Query-key-value bias\n",
        "}\n",
        "\n",
        "LLM_Layers = [[('Embedding', GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"]),\n",
        "         ('Embedding', GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"]),\n",
        "         ('Dropout', GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"drop_rate\"])\n",
        "         ], [('Sequential', GPT_CONFIG_124M[\"emb_dim\"], 4 * GPT_CONFIG_124M[\"emb_dim\"]),\n",
        "         ('LayerNorm', GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"]),\n",
        "         ('Linear', GPT_CONFIG_124M[\"vocab_size\"], GPT_CONFIG_124M[\"emb_dim\"])]]\n",
        "\n",
        "TransformerBlockLayer = [('LayerNorm', GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"]),\n",
        "('Linear', GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"]),\n",
        "('Linear', GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"]),\n",
        "('Linear', GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"]),\n",
        "('Dropout', GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"drop_rate\"]),\n",
        "('Linear', GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"]),\n",
        "('MultiHeadAttention', GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"]),\n",
        "('Dropout', GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"drop_rate\"]),\n",
        "('LayerNorm', GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"]),\n",
        "('Linear', GPT_CONFIG_124M[\"emb_dim\"], 4 * GPT_CONFIG_124M[\"emb_dim\"]),\n",
        "('GELU', 4 * GPT_CONFIG_124M[\"emb_dim\"], 4 * GPT_CONFIG_124M[\"emb_dim\"]),\n",
        "('Linear', 4 * GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"]),\n",
        "('Sequential', GPT_CONFIG_124M[\"emb_dim\"], 4 * GPT_CONFIG_124M[\"emb_dim\"]),\n",
        "('FeedForward', GPT_CONFIG_124M[\"emb_dim\"], 4 * GPT_CONFIG_124M[\"emb_dim\"]),\n",
        "('Dropout', GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"drop_rate\"]),\n",
        "('TransformerBlock', GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"])\n",
        "]\n",
        "\n",
        "hidden_sizes = []\n",
        "hidden_sizes.append(LLM_Layers[0])\n",
        "for _ in range(layerAmount):\n",
        "    hidden_sizes.append(TransformerBlockLayer)\n",
        "hidden_sizes.append(LLM_Layers[1])\n",
        "hidden_sizes = [item for sublist in hidden_sizes for item in sublist]"
      ],
      "metadata": {
        "id": "vT4SY5kEdKu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading the dataframes**"
      ],
      "metadata": {
        "id": "IG5pq0A9GkVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the Parquet file into a DataFrame\n",
        "evalDf = pq.read_table('./Data/identifiedClosestEvalSourcesWiki1010.parquet').to_pandas(safe=False)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(\"Evaluation Sample\")\n",
        "print(evalDf.head(10), \"\\n\") # View the first 10 rows\n",
        "print(evalDf.tail(10), \"\\n\") # View the last 10 rows\n",
        "print(evalDf.info(), \"\\n\")  # Show column types and non-null counts\n",
        "\n",
        "# Read the Parquet file into a DataFrame\n",
        "generatedEvalDf = pq.read_table('./Data/identifiedClosestGeneratedEvalSourcesWiki1010.parquet').to_pandas(safe=False)\n",
        "\n",
        "# Display the DataFrame\n",
        "#print(\"Generated Evaluation Sample\")\n",
        "#print(generatedEvalDf.head(10), \"\\n\") # View the first 10 rows\n",
        "#print(generatedEvalDf.tail(10), \"\\n\") # View the last 10 rows\n",
        "#print(generatedEvalDf.info())  # Show column types and non-null counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCKGUYW-tZmR",
        "outputId": "f947dd4b-f850-49f5-952a-984c9b82fdfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Sample\n",
            "            layer  neuron source  eval_neuron_value  neuron_value  difference\n",
            "evalSample                                                                   \n",
            "0               0       0    0:3           0.828838     -0.787064   -1.054130\n",
            "0               0       0    0:7           0.828838     -0.787064   -1.054130\n",
            "0               0       0    0:4           0.828838     -0.719139   -0.922670\n",
            "0               0       0    0:1           0.828838     -0.685918   -0.861161\n",
            "0               0       0    0:2           0.828838     -0.051844   -0.037843\n",
            "0               0       0    0:6           0.828838      0.969789    0.113296\n",
            "0               0       0    0:0           0.828838      1.064033    0.207421\n",
            "0               0       0    0:8           0.828838      1.064033    0.207421\n",
            "0               0       1    0:6          -0.169635      0.600388   -0.078424\n",
            "0               0       1    0:0          -0.169635      0.302192   -0.024187 \n",
            "\n",
            "            layer  neuron source  eval_neuron_value  neuron_value  difference\n",
            "evalSample                                                                   \n",
            "9              37   50255    0:7          -7.913807     -9.027027   79.526350\n",
            "9              37   50256    0:0          -7.485575     -7.757697   15.802394\n",
            "9              37   50256    0:4          -7.485575     -7.801656   18.459136\n",
            "9              37   50256    0:6          -7.485575     -8.018208   31.969106\n",
            "9              37   50256    0:5          -7.485575     -8.024781   32.390156\n",
            "9              37   50256    0:2          -7.485575     -6.463689   49.443349\n",
            "9              37   50256    0:8          -7.485575     -6.376272   52.947061\n",
            "9              37   50256    0:7          -7.485575     -8.402067   57.642144\n",
            "9              37   50256    0:3          -7.485575     -8.448498   60.897097\n",
            "9              37   50256    0:1          -7.485575     -5.763809   74.286296 \n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 7584069 entries, 0 to 9\n",
            "Data columns (total 6 columns):\n",
            " #   Column             Dtype  \n",
            "---  ------             -----  \n",
            " 0   layer              int64  \n",
            " 1   neuron             int32  \n",
            " 2   source             object \n",
            " 3   eval_neuron_value  float64\n",
            " 4   neuron_value       float64\n",
            " 5   difference         float64\n",
            "dtypes: float64(3), int32(1), int64(1), object(1)\n",
            "memory usage: 376.1+ MB\n",
            "None \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Most Relevant Sources with different modes"
      ],
      "metadata": {
        "id": "ke9UcgCDHAVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluationSamples = len(evalDf.index.unique())\n",
        "trainSamples = len(evalDf['source'].unique())\n",
        "\n",
        "# Exponential decay with a capped value to avoid overflow\n",
        "def exponential_decay(weights, decay_rate=0.05, max_val=1e100):\n",
        "    decay_values = np.exp(-decay_rate * np.array(weights))\n",
        "    decay_values[decay_values > max_val] = max_val  # Cap values\n",
        "    return decay_values\n",
        "\n",
        "# Function to handle mode calculation safely\n",
        "def safe_mode(values):\n",
        "    try:\n",
        "        mode_value = stats.mode(values)[0][0]\n",
        "        return mode_value\n",
        "    except:\n",
        "        # Return a fallback value (e.g., 0) if mode cannot be calculated\n",
        "        return 0\n",
        "\n",
        "# Function to handle percentile calculation safely\n",
        "def safe_percentile(values, percentile=90):\n",
        "    try:\n",
        "        return np.percentile(values, percentile)\n",
        "    except:\n",
        "        # Return a fallback value (e.g., the median) if percentile cannot be calculated\n",
        "        return np.median(values)\n",
        "\n",
        "def getMostUsedFromDataFrame(df, evalSample, closestSources, weightedMode=\"\", info=True):\n",
        "    # Filter entries for the specific evalSample\n",
        "    relevant_entries = df[df.index.get_level_values('evalSample') == evalSample]\n",
        "\n",
        "    # Use value_counts to count occurrences of each source directly\n",
        "    sources = relevant_entries['source']\n",
        "\n",
        "    # Filter out invalid sources ('None')\n",
        "    valid_entries = relevant_entries[sources != 'None']\n",
        "    ascending_order = True  # Sort by ascending for lowest average weights\n",
        "\n",
        "    if valid_entries.empty:\n",
        "        # Handle cases where there are no valid entries\n",
        "        print(f\"No valid entries for evalSample {evalSample}\")\n",
        "        return 0, []\n",
        "\n",
        "    # Initialize weighted_counts to None for error handling\n",
        "    weighted_counts = None\n",
        "\n",
        "    if weightedMode == \"Sum\":\n",
        "        # Group by 'source' and sum the 'difference' column as weights\n",
        "        weighted_counts = valid_entries.groupby('source')['difference'].sum()\n",
        "\n",
        "    elif weightedMode == \"Mean\":\n",
        "        # Group by 'source' and calculate the average of 'difference'\n",
        "        weighted_counts = valid_entries.groupby('source')['difference'].mean()\n",
        "\n",
        "    elif weightedMode == \"Median\":\n",
        "        # Group by 'source' and calculate the median of 'difference'\n",
        "        weighted_counts = valid_entries.groupby('source')['difference'].median()\n",
        "\n",
        "    elif weightedMode == \"Mode\":\n",
        "        # Get the most frequent source (mode)\n",
        "        weighted_counts = valid_entries['source'].mode()\n",
        "        if weighted_counts.empty:\n",
        "            weighted_counts = pd.Series([0])  # Fallback if no mode found\n",
        "\n",
        "    elif weightedMode == \"ExponentialDecay\":\n",
        "        # Apply exponential decay to the 'difference' values\n",
        "        valid_entries['weighted_difference'] = exponential_decay(valid_entries['difference'])\n",
        "        weighted_counts = valid_entries.groupby('source')['weighted_difference'].sum()\n",
        "\n",
        "    elif weightedMode == \"Percentile\":\n",
        "        # Use percentile-based filtering (example: Top 75%)\n",
        "        percentile_threshold = 0.75  # For example, top 25%\n",
        "        cutoff_value = safe_percentile(valid_entries['difference'], percentile_threshold * 100)\n",
        "        filtered_entries = valid_entries[valid_entries['difference'] >= cutoff_value]\n",
        "        weighted_counts = filtered_entries['source'].value_counts()\n",
        "\n",
        "    else:\n",
        "        # Default behavior: Count occurrences\n",
        "        weighted_counts = valid_entries['source'].value_counts()\n",
        "        ascending_order = False  # Sort by descending for highest counts\n",
        "\n",
        "    # Handle case when weighted_counts is None (for non-valid weighted modes)\n",
        "    if weighted_counts is None or weighted_counts.empty:\n",
        "        print(f\"No valid weighted counts for evalSample {evalSample} with mode {weightedMode}\")\n",
        "        return 0, []\n",
        "\n",
        "    # Sort weighted sources by the determined order\n",
        "    sorted_sources = weighted_counts.sort_values(ascending=ascending_order).head(closestSources)\n",
        "    # Total weight (sum, mean, median, or total count for closest sources)\n",
        "    total_weight = sorted_sources.sum()\n",
        "\n",
        "    # Convert to a Counter-like output (sorted already by the determined order)\n",
        "    counter = []\n",
        "    for source, weight in sorted_sources.items():\n",
        "        if isinstance(source, str):  # If source is a string (e.g., \"0:123\")\n",
        "            source = int(source.replace(\"0:\", \"\"))\n",
        "        counter.append((source, weight))\n",
        "\n",
        "    if info:\n",
        "        # Print the total weight (sum, mean, or total count depending on the mode)\n",
        "        print(f\"Total Weight for Weighted Mode={weightedMode}: {total_weight}\")\n",
        "\n",
        "    if info:\n",
        "        print(f\"Total closest Sources (Weighted Mode={weightedMode}):\", total_weight,\n",
        "              \"|\", closestSources, \"closest Sources in format [SourceNumber, Weight]:\", counter)\n",
        "\n",
        "    # Return the sorted sources and the counter\n",
        "    return sorted_sources, counter\n",
        "\n",
        "# Example usage of the updated function\n",
        "for evaluationSample in range(evaluationSamples):\n",
        "    print(f\"Most used for Evaluation Sample {evaluationSample+6}\")\n",
        "    getMostUsedFromDataFrame(evalDf, evaluationSample, trainSamples, \"Mean\", True)\n",
        "    getMostUsedFromDataFrame(evalDf, evaluationSample, trainSamples, \"Sum\", True)\n",
        "    getMostUsedFromDataFrame(evalDf, evaluationSample, trainSamples, \"Median\", True)\n",
        "    #getMostUsedFromDataFrame(evalDf, evaluationSample, trainSamples, \"Mode\", True)\n",
        "    getMostUsedFromDataFrame(evalDf, evaluationSample, trainSamples, \"ExponentialDecay\", True)\n",
        "    getMostUsedFromDataFrame(evalDf, evaluationSample, trainSamples, \"Percentile\", True)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l8oIagAvT6O",
        "outputId": "cf2691e9-4d2a-4f40-c4a6-bbe2841bb5f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most used for Evaluation Sample 6\n",
            "Total Weight for Weighted Mode=Mean: 434.80067818436305\n",
            "Total closest Sources (Weighted Mode=Mean): 434.80067818436305 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -213.31245858155924), (1, -38.445456779020056), (8, 28.93729636015812), (6, 49.959052998861395), (5, 52.5449816569437), (4, 88.44749899615151), (2, 110.4298298415942), (3, 168.83703597344913), (7, 187.40289771778424)]\n",
            "Total Weight for Weighted Mode=Sum: 36580759.55538227\n",
            "Total closest Sources (Weighted Mode=Sum): 36580759.55538227 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -18010824.127875373), (1, -3247795.297778056), (8, 2442944.4333172687), (6, 4204803.695649169), (5, 4397331.8799246475), (4, 7441530.328041207), (2, 9323369.673866116), (3, 14235832.362173311), (7, 15793566.608063985)]\n",
            "Total Weight for Weighted Mode=Median: 302.53021362423897\n",
            "Total closest Sources (Weighted Mode=Median): 302.53021362423897 | 10 closest Sources in format [SourceNumber, Weight]: [(0, 7.23631243803689), (6, 9.836032880335521), (5, 10.572184091921581), (4, 23.545171348890985), (8, 35.65382820513611), (3, 46.01024078661152), (2, 49.321035724918346), (1, 59.1649999370264), (7, 61.190408211361614)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_12/829290980.py:6: RuntimeWarning: overflow encountered in exp\n",
            "  decay_values = np.exp(-decay_rate * np.array(weights))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Weight for Weighted Mode=ExponentialDecay: 4.9112963525165944e+103\n",
            "Total closest Sources (Weighted Mode=ExponentialDecay): 4.9112963525165944e+103 | 10 closest Sources in format [SourceNumber, Weight]: [(5, 2.1748912316192388e+102), (7, 3.357652786266127e+102), (3, 3.591775771053462e+102), (4, 4.525911972536088e+102), (2, 4.843545232443671e+102), (6, 6.077491574544518e+102), (8, 6.207282089642434e+102), (1, 7.453458360925516e+102), (0, 1.0880954506134885e+103)]\n",
            "Total Weight for Weighted Mode=Percentile: 189586\n",
            "Total closest Sources (Weighted Mode=Percentile): 189586 | 10 closest Sources in format [SourceNumber, Weight]: [(0, 4148), (5, 4746), (6, 6286), (4, 10407), (8, 16680), (3, 27258), (2, 30305), (1, 44014), (7, 45742)]\n",
            "\n",
            "Most used for Evaluation Sample 7\n",
            "Total Weight for Weighted Mode=Mean: 434.80067818436305\n",
            "Total closest Sources (Weighted Mode=Mean): 434.80067818436305 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -213.31245858155924), (1, -38.445456779020056), (8, 28.93729636015812), (6, 49.959052998861395), (5, 52.5449816569437), (4, 88.44749899615151), (2, 110.4298298415942), (3, 168.83703597344913), (7, 187.40289771778424)]\n",
            "Total Weight for Weighted Mode=Sum: 36580759.55538227\n",
            "Total closest Sources (Weighted Mode=Sum): 36580759.55538227 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -18010824.127875373), (1, -3247795.297778056), (8, 2442944.4333172687), (6, 4204803.695649169), (5, 4397331.8799246475), (4, 7441530.328041207), (2, 9323369.673866116), (3, 14235832.362173311), (7, 15793566.608063985)]\n",
            "Total Weight for Weighted Mode=Median: 302.53021362423897\n",
            "Total closest Sources (Weighted Mode=Median): 302.53021362423897 | 10 closest Sources in format [SourceNumber, Weight]: [(0, 7.23631243803689), (6, 9.836032880335521), (5, 10.572184091921581), (4, 23.545171348890985), (8, 35.65382820513611), (3, 46.01024078661152), (2, 49.321035724918346), (1, 59.1649999370264), (7, 61.190408211361614)]\n",
            "Total Weight for Weighted Mode=ExponentialDecay: 4.9112963525165944e+103\n",
            "Total closest Sources (Weighted Mode=ExponentialDecay): 4.9112963525165944e+103 | 10 closest Sources in format [SourceNumber, Weight]: [(5, 2.1748912316192388e+102), (7, 3.357652786266127e+102), (3, 3.591775771053462e+102), (4, 4.525911972536088e+102), (2, 4.843545232443671e+102), (6, 6.077491574544518e+102), (8, 6.207282089642434e+102), (1, 7.453458360925516e+102), (0, 1.0880954506134885e+103)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_12/829290980.py:6: RuntimeWarning: overflow encountered in exp\n",
            "  decay_values = np.exp(-decay_rate * np.array(weights))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Weight for Weighted Mode=Percentile: 189586\n",
            "Total closest Sources (Weighted Mode=Percentile): 189586 | 10 closest Sources in format [SourceNumber, Weight]: [(0, 4148), (5, 4746), (6, 6286), (4, 10407), (8, 16680), (3, 27258), (2, 30305), (1, 44014), (7, 45742)]\n",
            "\n",
            "Most used for Evaluation Sample 8\n",
            "Total Weight for Weighted Mode=Mean: 434.80067818436305\n",
            "Total closest Sources (Weighted Mode=Mean): 434.80067818436305 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -213.31245858155924), (1, -38.445456779020056), (8, 28.93729636015812), (6, 49.959052998861395), (5, 52.5449816569437), (4, 88.44749899615151), (2, 110.4298298415942), (3, 168.83703597344913), (7, 187.40289771778424)]\n",
            "Total Weight for Weighted Mode=Sum: 36580759.55538227\n",
            "Total closest Sources (Weighted Mode=Sum): 36580759.55538227 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -18010824.127875373), (1, -3247795.297778056), (8, 2442944.4333172687), (6, 4204803.695649169), (5, 4397331.8799246475), (4, 7441530.328041207), (2, 9323369.673866116), (3, 14235832.362173311), (7, 15793566.608063985)]\n",
            "Total Weight for Weighted Mode=Median: 302.53021362423897\n",
            "Total closest Sources (Weighted Mode=Median): 302.53021362423897 | 10 closest Sources in format [SourceNumber, Weight]: [(0, 7.23631243803689), (6, 9.836032880335521), (5, 10.572184091921581), (4, 23.545171348890985), (8, 35.65382820513611), (3, 46.01024078661152), (2, 49.321035724918346), (1, 59.1649999370264), (7, 61.190408211361614)]\n",
            "Total Weight for Weighted Mode=ExponentialDecay: 4.9112963525165944e+103\n",
            "Total closest Sources (Weighted Mode=ExponentialDecay): 4.9112963525165944e+103 | 10 closest Sources in format [SourceNumber, Weight]: [(5, 2.1748912316192388e+102), (7, 3.357652786266127e+102), (3, 3.591775771053462e+102), (4, 4.525911972536088e+102), (2, 4.843545232443671e+102), (6, 6.077491574544518e+102), (8, 6.207282089642434e+102), (1, 7.453458360925516e+102), (0, 1.0880954506134885e+103)]\n",
            "Total Weight for Weighted Mode=Percentile: 189586\n",
            "Total closest Sources (Weighted Mode=Percentile): 189586 | 10 closest Sources in format [SourceNumber, Weight]: [(0, 4148), (5, 4746), (6, 6286), (4, 10407), (8, 16680), (3, 27258), (2, 30305), (1, 44014), (7, 45742)]\n",
            "\n",
            "Most used for Evaluation Sample 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_12/829290980.py:6: RuntimeWarning: overflow encountered in exp\n",
            "  decay_values = np.exp(-decay_rate * np.array(weights))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Weight for Weighted Mode=Mean: 434.80067818436305\n",
            "Total closest Sources (Weighted Mode=Mean): 434.80067818436305 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -213.31245858155924), (1, -38.445456779020056), (8, 28.93729636015812), (6, 49.959052998861395), (5, 52.5449816569437), (4, 88.44749899615151), (2, 110.4298298415942), (3, 168.83703597344913), (7, 187.40289771778424)]\n",
            "Total Weight for Weighted Mode=Sum: 36580759.55538227\n",
            "Total closest Sources (Weighted Mode=Sum): 36580759.55538227 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -18010824.127875373), (1, -3247795.297778056), (8, 2442944.4333172687), (6, 4204803.695649169), (5, 4397331.8799246475), (4, 7441530.328041207), (2, 9323369.673866116), (3, 14235832.362173311), (7, 15793566.608063985)]\n",
            "Total Weight for Weighted Mode=Median: 302.53021362423897\n",
            "Total closest Sources (Weighted Mode=Median): 302.53021362423897 | 10 closest Sources in format [SourceNumber, Weight]: [(0, 7.23631243803689), (6, 9.836032880335521), (5, 10.572184091921581), (4, 23.545171348890985), (8, 35.65382820513611), (3, 46.01024078661152), (2, 49.321035724918346), (1, 59.1649999370264), (7, 61.190408211361614)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_12/829290980.py:6: RuntimeWarning: overflow encountered in exp\n",
            "  decay_values = np.exp(-decay_rate * np.array(weights))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Weight for Weighted Mode=ExponentialDecay: 4.9112963525165944e+103\n",
            "Total closest Sources (Weighted Mode=ExponentialDecay): 4.9112963525165944e+103 | 10 closest Sources in format [SourceNumber, Weight]: [(5, 2.1748912316192388e+102), (7, 3.357652786266127e+102), (3, 3.591775771053462e+102), (4, 4.525911972536088e+102), (2, 4.843545232443671e+102), (6, 6.077491574544518e+102), (8, 6.207282089642434e+102), (1, 7.453458360925516e+102), (0, 1.0880954506134885e+103)]\n",
            "Total Weight for Weighted Mode=Percentile: 189586\n",
            "Total closest Sources (Weighted Mode=Percentile): 189586 | 10 closest Sources in format [SourceNumber, Weight]: [(0, 4148), (5, 4746), (6, 6286), (4, 10407), (8, 16680), (3, 27258), (2, 30305), (1, 44014), (7, 45742)]\n",
            "\n",
            "Most used for Evaluation Sample 10\n",
            "Total Weight for Weighted Mode=Mean: 166.7276917866429\n",
            "Total closest Sources (Weighted Mode=Mean): 166.7276917866429 | 10 closest Sources in format [SourceNumber, Weight]: [(9, -277.9249410218093), (0, -210.29878299965097), (1, -38.35060398107231), (8, 31.44384978117684), (6, 51.10298369599898), (5, 53.40939567198738), (4, 90.14131552792281), (2, 110.43834737896246), (3, 169.13331080426727), (7, 187.6328169288597)]\n",
            "Total Weight for Weighted Mode=Sum: 37197844.74167281\n",
            "Total closest Sources (Weighted Mode=Sum): 37197844.74167281 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -17755315.94987753), (1, -3239667.2713010837), (9, -190100.65965891755), (8, 2633045.0929761864), (6, 4300827.107855274), (5, 4469191.41104056), (4, 7584039.581941786), (2, 9323757.477468906), (3, 14260813.367083402), (7, 15811254.584144222)]\n",
            "Total Weight for Weighted Mode=Median: 303.4233581279741\n",
            "Total closest Sources (Weighted Mode=Median): 303.4233581279741 | 10 closest Sources in format [SourceNumber, Weight]: [(9, 0.9207127641688139), (0, 7.228983014324618), (6, 9.833218802785193), (5, 10.564527332794466), (4, 23.560051662476415), (8, 35.747448512017726), (3, 45.97822620609125), (2, 49.322245359541434), (1, 59.124840598107326), (7, 61.143103875666874)]\n",
            "Total Weight for Weighted Mode=ExponentialDecay: 4.8346852121535536e+103\n",
            "Total closest Sources (Weighted Mode=ExponentialDecay): 4.8346852121535536e+103 | 10 closest Sources in format [SourceNumber, Weight]: [(9, 9.00004397857834e+100), (5, 2.0410979468721548e+102), (7, 3.296967732339879e+102), (3, 3.54177559492618e+102), (4, 4.375788882086329e+102), (2, 4.843545232443671e+102), (6, 5.996210465362959e+102), (8, 6.11728164985665e+102), (1, 7.453458360925501e+102), (0, 1.059072581693643e+103)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_12/829290980.py:6: RuntimeWarning: overflow encountered in exp\n",
            "  decay_values = np.exp(-decay_rate * np.array(weights))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Weight for Weighted Mode=Percentile: 189578\n",
            "Total closest Sources (Weighted Mode=Percentile): 189578 | 10 closest Sources in format [SourceNumber, Weight]: [(9, 175), (0, 4104), (5, 4712), (6, 6268), (4, 10424), (8, 16532), (3, 27278), (2, 30360), (1, 44010), (7, 45715)]\n",
            "\n",
            "Most used for Evaluation Sample 11\n",
            "Total Weight for Weighted Mode=Mean: 432.89977109432687\n",
            "Total closest Sources (Weighted Mode=Mean): 432.89977109432687 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -215.26585852798138), (1, -38.4454763724273), (9, 0.05185828168335879), (8, 28.936890992119885), (6, 49.958846066204195), (5, 52.54596996490704), (4, 88.44756395121007), (2, 110.42973798694625), (3, 168.8371914979333), (7, 187.40304725373142)]\n",
            "Total Weight for Weighted Mode=Sum: 36580759.84980041\n",
            "Total closest Sources (Weighted Mode=Sum): 36580759.84980041 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -18010863.851319145), (1, -3247796.9529899135), (9, 39.723443769452835), (8, 2442910.211336745), (6, 4204786.279162076), (5, 4397362.042483211), (4, 7441535.793035059), (2, 9323361.918761898), (3, 14235845.475531243), (7, 15793579.210355468)]\n",
            "Total Weight for Weighted Mode=Median: 302.6907792148419\n",
            "Total closest Sources (Weighted Mode=Median): 302.6907792148419 | 10 closest Sources in format [SourceNumber, Weight]: [(9, 0.007318927118648847), (0, 7.389212176977837), (6, 9.836032880335521), (5, 10.572531016464904), (4, 23.545171348890985), (8, 35.65382820513611), (3, 46.01024078661152), (2, 49.321035724918346), (1, 59.1649999370264), (7, 61.190408211361614)]\n",
            "Total Weight for Weighted Mode=ExponentialDecay: 4.9112963525165944e+103\n",
            "Total closest Sources (Weighted Mode=ExponentialDecay): 4.9112963525165944e+103 | 10 closest Sources in format [SourceNumber, Weight]: [(9, 764.2132481950653), (5, 2.1748912316192388e+102), (7, 3.357652786266127e+102), (3, 3.591775771053462e+102), (4, 4.525911972536088e+102), (2, 4.843545232443671e+102), (6, 6.077491574544518e+102), (8, 6.207282089642434e+102), (1, 7.453458360925516e+102), (0, 1.0880954506134885e+103)]\n",
            "Total Weight for Weighted Mode=Percentile: 189586\n",
            "Total closest Sources (Weighted Mode=Percentile): 189586 | 10 closest Sources in format [SourceNumber, Weight]: [(0, 4148), (5, 4746), (6, 6286), (4, 10407), (8, 16680), (3, 27258), (2, 30305), (1, 44014), (7, 45742)]\n",
            "\n",
            "Most used for Evaluation Sample 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_12/829290980.py:6: RuntimeWarning: overflow encountered in exp\n",
            "  decay_values = np.exp(-decay_rate * np.array(weights))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Weight for Weighted Mode=Mean: 434.80067818436305\n",
            "Total closest Sources (Weighted Mode=Mean): 434.80067818436305 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -213.31245858155924), (1, -38.445456779020056), (8, 28.93729636015812), (6, 49.959052998861395), (5, 52.5449816569437), (4, 88.44749899615151), (2, 110.4298298415942), (3, 168.83703597344913), (7, 187.40289771778424)]\n",
            "Total Weight for Weighted Mode=Sum: 36580759.55538227\n",
            "Total closest Sources (Weighted Mode=Sum): 36580759.55538227 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -18010824.127875373), (1, -3247795.297778056), (8, 2442944.4333172687), (6, 4204803.695649169), (5, 4397331.8799246475), (4, 7441530.328041207), (2, 9323369.673866116), (3, 14235832.362173311), (7, 15793566.608063985)]\n",
            "Total Weight for Weighted Mode=Median: 302.53021362423897\n",
            "Total closest Sources (Weighted Mode=Median): 302.53021362423897 | 10 closest Sources in format [SourceNumber, Weight]: [(0, 7.23631243803689), (6, 9.836032880335521), (5, 10.572184091921581), (4, 23.545171348890985), (8, 35.65382820513611), (3, 46.01024078661152), (2, 49.321035724918346), (1, 59.1649999370264), (7, 61.190408211361614)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_12/829290980.py:6: RuntimeWarning: overflow encountered in exp\n",
            "  decay_values = np.exp(-decay_rate * np.array(weights))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Weight for Weighted Mode=ExponentialDecay: 4.9112963525165944e+103\n",
            "Total closest Sources (Weighted Mode=ExponentialDecay): 4.9112963525165944e+103 | 10 closest Sources in format [SourceNumber, Weight]: [(5, 2.1748912316192388e+102), (7, 3.357652786266127e+102), (3, 3.591775771053462e+102), (4, 4.525911972536088e+102), (2, 4.843545232443671e+102), (6, 6.077491574544518e+102), (8, 6.207282089642434e+102), (1, 7.453458360925516e+102), (0, 1.0880954506134885e+103)]\n",
            "Total Weight for Weighted Mode=Percentile: 189586\n",
            "Total closest Sources (Weighted Mode=Percentile): 189586 | 10 closest Sources in format [SourceNumber, Weight]: [(0, 4148), (5, 4746), (6, 6286), (4, 10407), (8, 16680), (3, 27258), (2, 30305), (1, 44014), (7, 45742)]\n",
            "\n",
            "Most used for Evaluation Sample 13\n",
            "Total Weight for Weighted Mode=Mean: 401.5360922082798\n",
            "Total closest Sources (Weighted Mode=Mean): 401.5360922082798 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -213.31245858155924), (1, -38.445456779020056), (8, 28.93729636015812), (6, 49.959052998861395), (5, 52.5449816569437), (2, 77.165243865511), (4, 88.44749899615151), (3, 168.83703597344913), (7, 187.40289771778424)]\n",
            "Total Weight for Weighted Mode=Sum: 33825078.11739753\n",
            "Total closest Sources (Weighted Mode=Sum): 33825078.11739753 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -18010824.127875373), (1, -3247795.297778056), (8, 2442944.4333172687), (6, 4204803.695649169), (5, 4397331.8799246475), (2, 6567688.235881372), (4, 7441530.328041207), (3, 14235832.362173311), (7, 15793566.608063985)]\n",
            "Total Weight for Weighted Mode=Median: 302.5195282568053\n",
            "Total closest Sources (Weighted Mode=Median): 302.5195282568053 | 10 closest Sources in format [SourceNumber, Weight]: [(0, 7.23631243803689), (6, 9.836032880335521), (5, 10.572184091921581), (4, 23.545171348890985), (8, 35.65382820513611), (3, 46.01024078661152), (2, 49.31035035748471), (1, 59.1649999370264), (7, 61.190408211361614)]\n",
            "Total Weight for Weighted Mode=ExponentialDecay: 5.061297095716803e+103\n",
            "Total closest Sources (Weighted Mode=ExponentialDecay): 5.061297095716803e+103 | 10 closest Sources in format [SourceNumber, Weight]: [(5, 2.1748912316192388e+102), (7, 3.357652786266127e+102), (3, 3.591775771053462e+102), (4, 4.525911972536088e+102), (6, 6.077491574544518e+102), (8, 6.207282089642434e+102), (2, 6.343552664445753e+102), (1, 7.453458360925516e+102), (0, 1.0880954506134885e+103)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_12/829290980.py:6: RuntimeWarning: overflow encountered in exp\n",
            "  decay_values = np.exp(-decay_rate * np.array(weights))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Weight for Weighted Mode=Percentile: 189757\n",
            "Total closest Sources (Weighted Mode=Percentile): 189757 | 10 closest Sources in format [SourceNumber, Weight]: [(0, 4148), (5, 4743), (6, 6285), (4, 10395), (8, 16659), (3, 27234), (2, 30593), (1, 43981), (7, 45719)]\n",
            "\n",
            "Most used for Evaluation Sample 14\n",
            "Total Weight for Weighted Mode=Mean: 434.80067818436305\n",
            "Total closest Sources (Weighted Mode=Mean): 434.80067818436305 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -213.31245858155924), (1, -38.445456779020056), (8, 28.93729636015812), (6, 49.959052998861395), (5, 52.5449816569437), (4, 88.44749899615151), (2, 110.4298298415942), (3, 168.83703597344913), (7, 187.40289771778424)]\n",
            "Total Weight for Weighted Mode=Sum: 36580759.55538227\n",
            "Total closest Sources (Weighted Mode=Sum): 36580759.55538227 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -18010824.127875373), (1, -3247795.297778056), (8, 2442944.4333172687), (6, 4204803.695649169), (5, 4397331.8799246475), (4, 7441530.328041207), (2, 9323369.673866116), (3, 14235832.362173311), (7, 15793566.608063985)]\n",
            "Total Weight for Weighted Mode=Median: 302.53021362423897\n",
            "Total closest Sources (Weighted Mode=Median): 302.53021362423897 | 10 closest Sources in format [SourceNumber, Weight]: [(0, 7.23631243803689), (6, 9.836032880335521), (5, 10.572184091921581), (4, 23.545171348890985), (8, 35.65382820513611), (3, 46.01024078661152), (2, 49.321035724918346), (1, 59.1649999370264), (7, 61.190408211361614)]\n",
            "Total Weight for Weighted Mode=ExponentialDecay: 4.9112963525165944e+103\n",
            "Total closest Sources (Weighted Mode=ExponentialDecay): 4.9112963525165944e+103 | 10 closest Sources in format [SourceNumber, Weight]: [(5, 2.1748912316192388e+102), (7, 3.357652786266127e+102), (3, 3.591775771053462e+102), (4, 4.525911972536088e+102), (2, 4.843545232443671e+102), (6, 6.077491574544518e+102), (8, 6.207282089642434e+102), (1, 7.453458360925516e+102), (0, 1.0880954506134885e+103)]\n",
            "Total Weight for Weighted Mode=Percentile: 189586\n",
            "Total closest Sources (Weighted Mode=Percentile): 189586 | 10 closest Sources in format [SourceNumber, Weight]: [(0, 4148), (5, 4746), (6, 6286), (4, 10407), (8, 16680), (3, 27258), (2, 30305), (1, 44014), (7, 45742)]\n",
            "\n",
            "Most used for Evaluation Sample 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_12/829290980.py:6: RuntimeWarning: overflow encountered in exp\n",
            "  decay_values = np.exp(-decay_rate * np.array(weights))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Weight for Weighted Mode=Mean: 434.80067818436305\n",
            "Total closest Sources (Weighted Mode=Mean): 434.80067818436305 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -213.31245858155924), (1, -38.445456779020056), (8, 28.93729636015812), (6, 49.959052998861395), (5, 52.5449816569437), (4, 88.44749899615151), (2, 110.4298298415942), (3, 168.83703597344913), (7, 187.40289771778424)]\n",
            "Total Weight for Weighted Mode=Sum: 36580759.55538227\n",
            "Total closest Sources (Weighted Mode=Sum): 36580759.55538227 | 10 closest Sources in format [SourceNumber, Weight]: [(0, -18010824.127875373), (1, -3247795.297778056), (8, 2442944.4333172687), (6, 4204803.695649169), (5, 4397331.8799246475), (4, 7441530.328041207), (2, 9323369.673866116), (3, 14235832.362173311), (7, 15793566.608063985)]\n",
            "Total Weight for Weighted Mode=Median: 302.53021362423897\n",
            "Total closest Sources (Weighted Mode=Median): 302.53021362423897 | 10 closest Sources in format [SourceNumber, Weight]: [(0, 7.23631243803689), (6, 9.836032880335521), (5, 10.572184091921581), (4, 23.545171348890985), (8, 35.65382820513611), (3, 46.01024078661152), (2, 49.321035724918346), (1, 59.1649999370264), (7, 61.190408211361614)]\n",
            "Total Weight for Weighted Mode=ExponentialDecay: 4.9112963525165944e+103\n",
            "Total closest Sources (Weighted Mode=ExponentialDecay): 4.9112963525165944e+103 | 10 closest Sources in format [SourceNumber, Weight]: [(5, 2.1748912316192388e+102), (7, 3.357652786266127e+102), (3, 3.591775771053462e+102), (4, 4.525911972536088e+102), (2, 4.843545232443671e+102), (6, 6.077491574544518e+102), (8, 6.207282089642434e+102), (1, 7.453458360925516e+102), (0, 1.0880954506134885e+103)]\n",
            "Total Weight for Weighted Mode=Percentile: 189586\n",
            "Total closest Sources (Weighted Mode=Percentile): 189586 | 10 closest Sources in format [SourceNumber, Weight]: [(0, 4148), (5, 4746), (6, 6286), (4, 10407), (8, 16680), (3, 27258), (2, 30305), (1, 44014), (7, 45742)]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_12/829290980.py:6: RuntimeWarning: overflow encountered in exp\n",
            "  decay_values = np.exp(-decay_rate * np.array(weights))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Sentence Similarity with different metrics"
      ],
      "metadata": {
        "id": "lGwfbjeFizfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the sentence-transformers library if you haven't already\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "# Import the required library\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a pre-trained model (e.g., 'all-MiniLM-L6-v2')\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def compute_cosine_similarity(embedding1, embedding2):\n",
        "    return dot(embedding1, embedding2) / (norm(embedding1) * norm(embedding2))\n",
        "\n",
        "# Function to compute both cosine and Euclidean similarity\n",
        "def compute_similarity(evalEmbedding, trainEmbedding, metrics=['cosine', 'euclidean']):\n",
        "    similarities = []\n",
        "\n",
        "    if 'cosine' in metrics:\n",
        "        # Cosine similarity\n",
        "        cosine_sim = cosine_similarity([evalEmbedding], [trainEmbedding])[0][0]\n",
        "        similarities.append(('cosine', cosine_sim))\n",
        "\n",
        "    if 'euclidean' in metrics:\n",
        "        # Euclidean distance (we invert it to make it a similarity)\n",
        "        euclidean_dist = euclidean_distances([evalEmbedding], [trainEmbedding])[0][0]\n",
        "        euclidean_sim = -euclidean_dist  # Invert distance to create a similarity score\n",
        "        similarities.append(('euclidean', euclidean_sim))\n",
        "\n",
        "    return similarities\n",
        "\n",
        "# Updated function to extract sentence similarity\n",
        "def extractSentenceSimilarity(name, info=True, metrics=['cosine', 'euclidean']):\n",
        "    if info:\n",
        "        print(f\"Checking Similarity for {name}-Sample\")\n",
        "\n",
        "    # Dictionary to store all similarities for evaluation samples\n",
        "    similarities_dict = {}\n",
        "\n",
        "    # Iterate over each evaluation sample\n",
        "    for evaluationSample in range(evaluationSamples):  # Adjust range based on your dataset\n",
        "        evalSample = sentences[evaluationSample + 6]  # Adjust index based on your dataset\n",
        "        evalEmbedding = model.encode(evalSample)\n",
        "\n",
        "        # List to store similarities for each training sample\n",
        "        similarities = []\n",
        "\n",
        "        # Iterate over each training sample\n",
        "        for trainingSample in range(trainSamples):  # Adjust based on your dataset\n",
        "            trainSample = sentences[trainingSample]\n",
        "            trainEmbedding = model.encode(trainSample)\n",
        "\n",
        "            # Compute both cosine and Euclidean similarity\n",
        "            metric_similarities = compute_similarity(evalEmbedding, trainEmbedding, metrics)\n",
        "\n",
        "            # Combine the metrics (here we average them for simplicity)\n",
        "            combined_similarity = np.mean([sim for _, sim in metric_similarities])\n",
        "\n",
        "            # Append the combined similarity score along with the training sample index\n",
        "            similarities.append((trainingSample, combined_similarity))\n",
        "\n",
        "        # Sort the training samples by their combined similarity to the evaluation sample (highest similarity first)\n",
        "        sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Save the sorted similarities to the dictionary\n",
        "        similarities_dict[evaluationSample] = sorted_similarities\n",
        "\n",
        "        if info:\n",
        "            # Print the sorted train samples by their combined similarity to the evaluation sample\n",
        "            print(f\"Sorted relevance for evaluation sample {evaluationSample + 6}:\")\n",
        "            for trainingSample, similarity in sorted_similarities:\n",
        "                print(f\"Training sample: '{trainingSample}' - Combined Similarity: {similarity:.4f}\")\n",
        "            print()\n",
        "\n",
        "    # Return the dictionary of similarities\n",
        "    return similarities_dict\n",
        "\n",
        "# Example function calls with dataframes\n",
        "sentenceSimilarity = extractSentenceSimilarity(\"Evaluation\", True, metrics=['cosine', 'euclidean'])\n",
        "#extractSentenceSimilarity(\"Generated Evaluation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTRqmmtvKcx2",
        "outputId": "5721046a-a638-44e2-c684-6e3311ab8d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-06 10:31:25.703534: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-01-06 10:31:26.056518: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-06 10:31:26.056595: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-06 10:31:26.058474: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-06 10:31:26.218928: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking Similarity for Evaluation-Sample\n",
            "Sorted relevance for evaluation sample 6:\n",
            "Training sample: '6' - Combined Similarity: 0.5000\n",
            "Training sample: '7' - Combined Similarity: -0.1120\n",
            "Training sample: '5' - Combined Similarity: -0.1586\n",
            "Training sample: '3' - Combined Similarity: -0.2395\n",
            "Training sample: '4' - Combined Similarity: -0.2580\n",
            "Training sample: '1' - Combined Similarity: -0.3670\n",
            "Training sample: '8' - Combined Similarity: -0.3923\n",
            "Training sample: '2' - Combined Similarity: -0.4165\n",
            "Training sample: '9' - Combined Similarity: -0.4429\n",
            "Training sample: '0' - Combined Similarity: -0.4851\n",
            "\n",
            "Sorted relevance for evaluation sample 7:\n",
            "Training sample: '7' - Combined Similarity: 0.5000\n",
            "Training sample: '6' - Combined Similarity: -0.1120\n",
            "Training sample: '4' - Combined Similarity: -0.1435\n",
            "Training sample: '5' - Combined Similarity: -0.2346\n",
            "Training sample: '3' - Combined Similarity: -0.2691\n",
            "Training sample: '8' - Combined Similarity: -0.2882\n",
            "Training sample: '1' - Combined Similarity: -0.3024\n",
            "Training sample: '0' - Combined Similarity: -0.3500\n",
            "Training sample: '9' - Combined Similarity: -0.3893\n",
            "Training sample: '2' - Combined Similarity: -0.4840\n",
            "\n",
            "Sorted relevance for evaluation sample 8:\n",
            "Training sample: '8' - Combined Similarity: 0.5000\n",
            "Training sample: '0' - Combined Similarity: 0.0970\n",
            "Training sample: '4' - Combined Similarity: -0.1175\n",
            "Training sample: '9' - Combined Similarity: -0.1206\n",
            "Training sample: '1' - Combined Similarity: -0.2110\n",
            "Training sample: '3' - Combined Similarity: -0.2463\n",
            "Training sample: '5' - Combined Similarity: -0.2732\n",
            "Training sample: '7' - Combined Similarity: -0.2882\n",
            "Training sample: '6' - Combined Similarity: -0.3923\n",
            "Training sample: '2' - Combined Similarity: -0.4349\n",
            "\n",
            "Sorted relevance for evaluation sample 9:\n",
            "Training sample: '9' - Combined Similarity: 0.5000\n",
            "Training sample: '8' - Combined Similarity: -0.1206\n",
            "Training sample: '0' - Combined Similarity: -0.2002\n",
            "Training sample: '4' - Combined Similarity: -0.2097\n",
            "Training sample: '1' - Combined Similarity: -0.2660\n",
            "Training sample: '3' - Combined Similarity: -0.2999\n",
            "Training sample: '5' - Combined Similarity: -0.3587\n",
            "Training sample: '7' - Combined Similarity: -0.3893\n",
            "Training sample: '2' - Combined Similarity: -0.4269\n",
            "Training sample: '6' - Combined Similarity: -0.4429\n",
            "\n",
            "Sorted relevance for evaluation sample 10:\n",
            "Training sample: '8' - Combined Similarity: -0.1011\n",
            "Training sample: '0' - Combined Similarity: -0.1373\n",
            "Training sample: '9' - Combined Similarity: -0.1749\n",
            "Training sample: '4' - Combined Similarity: -0.2094\n",
            "Training sample: '3' - Combined Similarity: -0.2634\n",
            "Training sample: '1' - Combined Similarity: -0.2813\n",
            "Training sample: '2' - Combined Similarity: -0.3246\n",
            "Training sample: '5' - Combined Similarity: -0.3586\n",
            "Training sample: '6' - Combined Similarity: -0.4311\n",
            "Training sample: '7' - Combined Similarity: -0.4362\n",
            "\n",
            "Sorted relevance for evaluation sample 11:\n",
            "Training sample: '8' - Combined Similarity: -0.2362\n",
            "Training sample: '4' - Combined Similarity: -0.2789\n",
            "Training sample: '9' - Combined Similarity: -0.2847\n",
            "Training sample: '0' - Combined Similarity: -0.3073\n",
            "Training sample: '5' - Combined Similarity: -0.3631\n",
            "Training sample: '3' - Combined Similarity: -0.3959\n",
            "Training sample: '7' - Combined Similarity: -0.4020\n",
            "Training sample: '1' - Combined Similarity: -0.4196\n",
            "Training sample: '6' - Combined Similarity: -0.4210\n",
            "Training sample: '2' - Combined Similarity: -0.4746\n",
            "\n",
            "Sorted relevance for evaluation sample 12:\n",
            "Training sample: '0' - Combined Similarity: -0.1837\n",
            "Training sample: '8' - Combined Similarity: -0.1937\n",
            "Training sample: '9' - Combined Similarity: -0.1967\n",
            "Training sample: '4' - Combined Similarity: -0.2608\n",
            "Training sample: '3' - Combined Similarity: -0.2767\n",
            "Training sample: '5' - Combined Similarity: -0.3229\n",
            "Training sample: '1' - Combined Similarity: -0.3279\n",
            "Training sample: '6' - Combined Similarity: -0.4427\n",
            "Training sample: '7' - Combined Similarity: -0.4525\n",
            "Training sample: '2' - Combined Similarity: -0.4969\n",
            "\n",
            "Sorted relevance for evaluation sample 13:\n",
            "Training sample: '3' - Combined Similarity: -0.2365\n",
            "Training sample: '9' - Combined Similarity: -0.2571\n",
            "Training sample: '4' - Combined Similarity: -0.2578\n",
            "Training sample: '8' - Combined Similarity: -0.2874\n",
            "Training sample: '0' - Combined Similarity: -0.2912\n",
            "Training sample: '7' - Combined Similarity: -0.3030\n",
            "Training sample: '5' - Combined Similarity: -0.3154\n",
            "Training sample: '2' - Combined Similarity: -0.3229\n",
            "Training sample: '6' - Combined Similarity: -0.3637\n",
            "Training sample: '1' - Combined Similarity: -0.4052\n",
            "\n",
            "Sorted relevance for evaluation sample 14:\n",
            "Training sample: '8' - Combined Similarity: -0.0789\n",
            "Training sample: '0' - Combined Similarity: -0.0837\n",
            "Training sample: '4' - Combined Similarity: -0.0922\n",
            "Training sample: '5' - Combined Similarity: -0.2099\n",
            "Training sample: '3' - Combined Similarity: -0.2361\n",
            "Training sample: '1' - Combined Similarity: -0.2450\n",
            "Training sample: '9' - Combined Similarity: -0.2532\n",
            "Training sample: '7' - Combined Similarity: -0.2899\n",
            "Training sample: '6' - Combined Similarity: -0.3004\n",
            "Training sample: '2' - Combined Similarity: -0.3246\n",
            "\n",
            "Sorted relevance for evaluation sample 15:\n",
            "Training sample: '5' - Combined Similarity: -0.0992\n",
            "Training sample: '6' - Combined Similarity: -0.3147\n",
            "Training sample: '4' - Combined Similarity: -0.3367\n",
            "Training sample: '8' - Combined Similarity: -0.3495\n",
            "Training sample: '1' - Combined Similarity: -0.3682\n",
            "Training sample: '7' - Combined Similarity: -0.3759\n",
            "Training sample: '3' - Combined Similarity: -0.3910\n",
            "Training sample: '0' - Combined Similarity: -0.4327\n",
            "Training sample: '9' - Combined Similarity: -0.4948\n",
            "Training sample: '2' - Combined Similarity: -0.5509\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Mode Consistency to determine best Mode"
      ],
      "metadata": {
        "id": "4ZRfXptQlYlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mode_consistency(similarity_scores, relevance_scores_sum, relevance_scores_mean, relevance_scores_median, k=5):\n",
        "    # Helper function to normalize\n",
        "    def normalize(scores):\n",
        "        # Normalize values to [0, 1] range\n",
        "        values = [v for _, v in scores]\n",
        "        min_val = min(values)\n",
        "        max_val = max(values)\n",
        "        return {k: (v - min_val) / (max_val - min_val) for k, v in scores}\n",
        "\n",
        "    def spearman_rank_correlation(x, y):\n",
        "        if len(x) != len(y):\n",
        "            raise ValueError(\"Both lists must have the same length.\")\n",
        "\n",
        "        n = len(x)\n",
        "\n",
        "        # Rank the values\n",
        "        rank_x = [sorted(x).index(val) + 1 for val in x]\n",
        "        rank_y = [sorted(y).index(val) + 1 for val in y]\n",
        "\n",
        "        # Calculate the differences in ranks\n",
        "        d_squared = [(rx - ry) ** 2 for rx, ry in zip(rank_x, rank_y)]\n",
        "\n",
        "        # Sum of squared differences\n",
        "        sum_d_squared = sum(d_squared)\n",
        "\n",
        "        # Spearman rank correlation coefficient formula\n",
        "        rho = 1 - (6 * sum_d_squared) / (n * (n**2 - 1))\n",
        "\n",
        "        return rho\n",
        "\n",
        "    # Helper function to calculate standard deviation\n",
        "    def calculate_std(scores1, scores2):\n",
        "        values1 = [v for _, v in scores1]\n",
        "        values2 = [v for _, v in scores2]\n",
        "        assert len(values1) == len(values2), \"Scores must have the same length\"\n",
        "        return np.std(np.array(values1) - np.array(values2))\n",
        "\n",
        "    # Helper function to calculate mean absolute error (MAE)\n",
        "    def calculate_mae(scores1, scores2):\n",
        "        values1 = [v for _, v in scores1]\n",
        "        values2 = [v for _, v in scores2]\n",
        "        assert len(values1) == len(values2), \"Scores must have the same length\"\n",
        "        return np.mean(np.abs(np.array(values1) - np.array(values2)))\n",
        "\n",
        "    # Helper function to calculate Pearson correlation\n",
        "    def calculate_pearson(scores1, scores2):\n",
        "        values1 = [v for _, v in scores1]\n",
        "        values2 = [v for _, v in scores2]\n",
        "        return pearsonr(values1, values2)[0]\n",
        "\n",
        "    # Helper function to calculate median\n",
        "    def calculate_median(scores1, scores2):\n",
        "        values1 = [v for _, v in scores1]\n",
        "        values2 = [v for _, v in scores2]\n",
        "        return np.median(np.abs(np.array(values1) - np.array(values2)))\n",
        "\n",
        "    # Normalize all scores (only inverted relevance scores)\n",
        "    similarity_scores_normalized = normalize(similarity_scores)\n",
        "    relevance_scores_sum_normalized = normalize(relevance_scores_sum)\n",
        "    relevance_scores_mean_normalized = normalize(relevance_scores_mean)\n",
        "    relevance_scores_median_normalized = normalize(relevance_scores_median)\n",
        "\n",
        "    # Convert lists of tuples to dictionaries for easier processing\n",
        "    similarity_scores = dict(similarity_scores_normalized)\n",
        "    relevance_scores_sum = dict(relevance_scores_sum_normalized)\n",
        "    relevance_scores_mean = dict(relevance_scores_mean_normalized)\n",
        "    relevance_scores_median = dict(relevance_scores_median_normalized)\n",
        "\n",
        "    # Invert relevance rankings since lower relevance is better\n",
        "    inverted_relevance_sum = {k: 1 - v for k, v in relevance_scores_sum_normalized.items()}\n",
        "    inverted_relevance_mean = {k: 1 - v for k, v in relevance_scores_mean_normalized.items()}\n",
        "    inverted_relevance_median = {k: 1 - v for k, v in relevance_scores_median_normalized.items()}\n",
        "\n",
        "    # Compute Spearman correlation for sum, mean, and median\n",
        "    # Align keys between similarity scores and inverted relevance scores\n",
        "    common_keys = set(similarity_scores.keys()) & set(inverted_relevance_sum.keys())\n",
        "    similarity_scores = {k: similarity_scores[k] for k in common_keys}\n",
        "    inverted_relevance_sum = {k: inverted_relevance_sum[k] for k in common_keys}\n",
        "    inverted_relevance_mean = {k: inverted_relevance_mean[k] for k in common_keys}\n",
        "    inverted_relevance_median = {k: inverted_relevance_median[k] for k in common_keys}\n",
        "\n",
        "    # Compute Spearman correlation for sum, mean, and median\n",
        "    spearman_inverted_sum, _ = spearmanr(list(similarity_scores.values()), list(inverted_relevance_sum.values()))\n",
        "    spearman_inverted_mean, _ = spearmanr(list(similarity_scores.values()), list(inverted_relevance_mean.values()))\n",
        "    spearman_inverted_median, _ = spearmanr(list(similarity_scores.values()), list(inverted_relevance_median.values()))\n",
        "\n",
        "    # Calculate Pearson correlation\n",
        "    pearson_inverted_sum = calculate_pearson(list(similarity_scores.items()), list(inverted_relevance_sum.items()))\n",
        "    pearson_inverted_mean = calculate_pearson(list(similarity_scores.items()), list(inverted_relevance_mean.items()))\n",
        "    pearson_inverted_median = calculate_pearson(list(similarity_scores.items()), list(inverted_relevance_median.items()))\n",
        "\n",
        "    # Calculate standard deviation between similarity and inverted relevance scores\n",
        "    std_inverted_sum = calculate_std(list(similarity_scores.items()), list(inverted_relevance_sum.items()))\n",
        "    std_inverted_mean = calculate_std(list(similarity_scores.items()), list(inverted_relevance_mean.items()))\n",
        "    std_inverted_median = calculate_std(list(similarity_scores.items()), list(inverted_relevance_median.items()))\n",
        "\n",
        "    # Calculate Mean Absolute Error (MAE)\n",
        "    mae_inverted_sum = calculate_mae(list(similarity_scores.items()), list(inverted_relevance_sum.items()))\n",
        "    mae_inverted_mean = calculate_mae(list(similarity_scores.items()), list(inverted_relevance_mean.items()))\n",
        "    mae_inverted_median = calculate_mae(list(similarity_scores.items()), list(inverted_relevance_median.items()))\n",
        "\n",
        "    # Calculate Median\n",
        "    median_inverted_sum = calculate_median(list(similarity_scores.items()), list(inverted_relevance_sum.items()))\n",
        "    median_inverted_mean = calculate_median(list(similarity_scores.items()), list(inverted_relevance_mean.items()))\n",
        "    median_inverted_median = calculate_median(list(similarity_scores.items()), list(inverted_relevance_median.items()))\n",
        "\n",
        "    # Max and Min Difference\n",
        "    max_diff_inverted_sum = np.max(np.abs(np.array([v for _, v in similarity_scores.items()]) - np.array([v for _, v in inverted_relevance_sum.items()])))\n",
        "    min_diff_inverted_sum = np.min(np.abs(np.array([v for _, v in similarity_scores.items()]) - np.array([v for _, v in inverted_relevance_sum.items()])))\n",
        "\n",
        "    max_diff_inverted_mean = np.max(np.abs(np.array([v for _, v in similarity_scores.items()]) - np.array([v for _, v in inverted_relevance_mean.items()])))\n",
        "    min_diff_inverted_mean = np.min(np.abs(np.array([v for _, v in similarity_scores.items()]) - np.array([v for _, v in inverted_relevance_mean.items()])))\n",
        "\n",
        "    max_diff_inverted_median = np.max(np.abs(np.array([v for _, v in similarity_scores.items()]) - np.array([v for _, v in inverted_relevance_median.items()])))\n",
        "    min_diff_inverted_median = np.min(np.abs(np.array([v for _, v in similarity_scores.items()]) - np.array([v for _, v in inverted_relevance_median.items()])))\n",
        "\n",
        "    # Store results for sum, mean, and median separately\n",
        "    result_sum = {\n",
        "        'spearman_inverted_sum': spearman_inverted_sum,\n",
        "        'pearson_inverted_sum': pearson_inverted_sum,\n",
        "        'std_inverted_sum': std_inverted_sum,\n",
        "        'mae_inverted_sum': mae_inverted_sum,\n",
        "        'median_inverted_sum': median_inverted_sum,\n",
        "        'max_diff_inverted_sum': max_diff_inverted_sum,\n",
        "        'min_diff_inverted_sum': min_diff_inverted_sum\n",
        "    }\n",
        "\n",
        "    result_mean = {\n",
        "        'spearman_inverted_mean': spearman_inverted_mean,\n",
        "        'pearson_inverted_mean': pearson_inverted_mean,\n",
        "        'std_inverted_mean': std_inverted_mean,\n",
        "        'mae_inverted_mean': mae_inverted_mean,\n",
        "        'median_inverted_mean': median_inverted_mean,\n",
        "        'max_diff_inverted_mean': max_diff_inverted_mean,\n",
        "        'min_diff_inverted_mean': min_diff_inverted_mean\n",
        "    }\n",
        "\n",
        "    result_median = {\n",
        "        'spearman_inverted_median': spearman_inverted_median,\n",
        "        'pearson_inverted_median': pearson_inverted_median,\n",
        "        'std_inverted_median': std_inverted_median,\n",
        "        'mae_inverted_median': mae_inverted_median,\n",
        "        'median_inverted_median': median_inverted_median,\n",
        "        'max_diff_inverted_median': max_diff_inverted_median,\n",
        "        'min_diff_inverted_median': min_diff_inverted_median\n",
        "    }\n",
        "\n",
        "    # Print the results for each mode\n",
        "    print(f\"Mode consistency evaluation results (Sum): {result_sum}\")\n",
        "    print(f\"Mode consistency evaluation results (Mean): {result_mean}\")\n",
        "    print(f\"Mode consistency evaluation results (Median): {result_median}\")\n",
        "    print()\n",
        "\n",
        "     # Return the results in a dictionary\n",
        "    result = {\n",
        "        'spearman_inverted_sum': spearman_inverted_sum,\n",
        "        'spearman_inverted_mean': spearman_inverted_mean,\n",
        "        'spearman_inverted_median': spearman_inverted_median,\n",
        "        'pearson_inverted_sum': pearson_inverted_sum,\n",
        "        'pearson_inverted_mean': pearson_inverted_mean,\n",
        "        'pearson_inverted_median': pearson_inverted_median,\n",
        "        'std_inverted_sum': std_inverted_sum,\n",
        "        'std_inverted_mean': std_inverted_mean,\n",
        "        'std_inverted_median': std_inverted_median,\n",
        "        'mae_inverted_sum': mae_inverted_sum,\n",
        "        'mae_inverted_mean': mae_inverted_mean,\n",
        "        'mae_inverted_median': mae_inverted_median,\n",
        "        'median_inverted_sum': median_inverted_sum,\n",
        "        'median_inverted_mean': median_inverted_mean,\n",
        "        'median_inverted_median': median_inverted_median,\n",
        "        'max_diff_inverted_sum': max_diff_inverted_sum,\n",
        "        'min_diff_inverted_sum': min_diff_inverted_sum,\n",
        "        'max_diff_inverted_mean': max_diff_inverted_mean,\n",
        "        'min_diff_inverted_mean': min_diff_inverted_mean,\n",
        "        'max_diff_inverted_median': max_diff_inverted_median,\n",
        "        'min_diff_inverted_median': min_diff_inverted_median\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "def getBestModeForSimilarityCheck():\n",
        "    # Get cosine similarities\n",
        "    similarities = extractSentenceSimilarity(\"Evaluation\", False, metrics=['cosine', 'euclidean'])\n",
        "\n",
        "    # Initialize dictionaries to store the aggregated scores for each mode\n",
        "    mode_scores = {'sum': 0, 'mean': 0, 'median': 0}\n",
        "\n",
        "    # Initialize dictionaries to store the best values for each metric (we use negative infinity or positive infinity based on metrics)\n",
        "    best_values = {\n",
        "        'spearman': {'sum': float('-inf'), 'mean': float('-inf'), 'median': float('-inf')},\n",
        "        'pearson': {'sum': float('-inf'), 'mean': float('-inf'), 'median': float('-inf')},\n",
        "        'std': {'sum': float('inf'), 'mean': float('inf'), 'median': float('inf')},\n",
        "        'mae': {'sum': float('inf'), 'mean': float('inf'), 'median': float('inf')},\n",
        "        'median': {'sum': float('inf'), 'mean': float('inf'), 'median': float('inf')},\n",
        "        'max_diff': {'sum': float('-inf'), 'mean': float('-inf'), 'median': float('-inf')},\n",
        "        'min_diff': {'sum': float('inf'), 'mean': float('inf'), 'median': float('inf')}\n",
        "    }\n",
        "\n",
        "    best_modes = {\n",
        "        'spearman': {'sum': None, 'mean': None, 'median': None},\n",
        "        'pearson': {'sum': None, 'mean': None, 'median': None},\n",
        "        'std': {'sum': None, 'mean': None, 'median': None},\n",
        "        'mae': {'sum': None, 'mean': None, 'median': None},\n",
        "        'median': {'sum': None, 'mean': None, 'median': None},\n",
        "        'max_diff': {'sum': None, 'mean': None, 'median': None},\n",
        "        'min_diff': {'sum': None, 'mean': None, 'median': None}\n",
        "    }\n",
        "\n",
        "    # Loop through each evaluation sample\n",
        "    for evaluationSample in range(evaluationSamples):\n",
        "        print(f\"Evaluation Sample {evaluationSample + 1}:\")\n",
        "\n",
        "        # Get the most relevant sources by sum, mean, and median\n",
        "        total_weight_sum, counter_sum = getMostUsedFromDataFrame(evalDf, evaluationSample, trainSamples, \"Sum\", False)\n",
        "        total_weight_mean, counter_mean = getMostUsedFromDataFrame(evalDf, evaluationSample, trainSamples, \"Mean\", False)\n",
        "        total_weight_median, counter_median = getMostUsedFromDataFrame(evalDf, evaluationSample, trainSamples, \"Median\", False)\n",
        "\n",
        "        sentenceSimilarity = similarities[evaluationSample]\n",
        "\n",
        "        # Get the evaluation results for the current sample\n",
        "        result = evaluate_mode_consistency(sentenceSimilarity, counter_sum, counter_mean, counter_median)\n",
        "\n",
        "        # Update best values for each metric and mode in one go (minimize checks inside loops)\n",
        "        for metric in best_values:\n",
        "            for mode in best_values[metric]:\n",
        "                result_key = f'{metric}_inverted_{mode}'\n",
        "\n",
        "                # Update the best values based on the metric\n",
        "                current_value = result[result_key]\n",
        "                if metric in ['spearman', 'pearson', 'max_diff']:  # Maximizing\n",
        "                    if current_value > best_values[metric][mode]:\n",
        "                        best_values[metric][mode] = current_value\n",
        "                        best_modes[metric][mode] = evaluationSample\n",
        "                else:  # Minimizing (std, mae, median, min_diff)\n",
        "                    if current_value < best_values[metric][mode]:\n",
        "                        best_values[metric][mode] = current_value\n",
        "                        best_modes[metric][mode] = evaluationSample\n",
        "\n",
        "        # Aggregate the results for each mode\n",
        "        for mode in ['sum', 'mean', 'median']:\n",
        "            mode_score = 0\n",
        "            # Sum all the metrics for the current mode to compute the score\n",
        "            for metric in best_values:\n",
        "                if metric in ['spearman', 'pearson', 'max_diff']:  # Maximizing correlation\n",
        "                    mode_score += best_values[metric][mode]\n",
        "                else:  # Minimizing error\n",
        "                    mode_score -= best_values[metric][mode]\n",
        "            mode_scores[mode] += mode_score\n",
        "\n",
        "    # After all evaluation samples, determine the best mode based on the aggregated score\n",
        "    best_mode = max(mode_scores, key=mode_scores.get)\n",
        "\n",
        "    # Print the best mode and corresponding details\n",
        "    print(\"\\nBest Mode Evaluation Results:\")\n",
        "    print(f\"Best Mode: {best_mode.capitalize()} with an aggregated score of {mode_scores[best_mode]}\")\n",
        "\n",
        "    # Print detailed results for each metric and best value for the best mode\n",
        "    for metric in best_values:\n",
        "        print(f\"  - Best {metric.capitalize()} for {best_mode.capitalize()}: {best_values[metric][best_mode]}\")\n",
        "\n",
        "    # Ranking: Print the ranking of the modes based on aggregated scores\n",
        "    print(\"\\nRanking of Modes Based on Aggregated Scores:\")\n",
        "    ranked_modes = sorted(mode_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    for rank, (mode, score) in enumerate(ranked_modes, 1):\n",
        "        print(f\"{rank}. {mode.capitalize()}: Score = {score}\")\n",
        "\n",
        "    # Pareto Analysis: Find top contributors based on the 80/20 rule\n",
        "    pareto_cutoff = 0.8 * sum(mode_scores.values())\n",
        "    pareto_modes = []\n",
        "    cumulative_score = 0\n",
        "    for mode, score in ranked_modes:\n",
        "        cumulative_score += score\n",
        "        pareto_modes.append((mode, score))\n",
        "        if cumulative_score >= pareto_cutoff:\n",
        "            break\n",
        "\n",
        "    print(\"\\nPareto Analysis - Top Contributing Modes:\")\n",
        "    for mode, score in pareto_modes:\n",
        "        print(f\"{mode.capitalize()}: Score = {score}\")\n",
        "\n",
        "getBestModeForSimilarityCheck()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU-EN0G1EhEX",
        "outputId": "508ac505-9da0-4fcd-e593-edf3c61efd99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Sample 1:\n",
            "Mode consistency evaluation results (Sum): {'spearman_inverted_sum': -0.4666666666666666, 'pearson_inverted_sum': -0.3022804544485473, 'std_inverted_sum': 0.4560715595417233, 'mae_inverted_sum': 0.34746280290315207, 'median_inverted_sum': 0.300657461344375, 'max_diff_inverted_sum': 1.0, 'min_diff_inverted_sum': 0.005645183043843205}\n",
            "Mode consistency evaluation results (Mean): {'spearman_inverted_mean': -0.4666666666666666, 'pearson_inverted_mean': -0.3025750992513467, 'std_inverted_mean': 0.4561015884005892, 'mae_inverted_mean': 0.34750812600492537, 'median_inverted_mean': 0.30117671550594927, 'max_diff_inverted_mean': 1.0, 'min_diff_inverted_mean': 0.005065188441419566}\n",
            "Mode consistency evaluation results (Median): {'spearman_inverted_median': -0.06666666666666667, 'pearson_inverted_median': 0.30735484466000906, 'std_inverted_median': 0.39497335000742984, 'mae_inverted_median': 0.3494028994682919, 'median_inverted_median': 0.37878867983818054, 'max_diff_inverted_median': 1.0, 'min_diff_inverted_median': 0.03203941852804826}\n",
            "\n",
            "Evaluation Sample 2:\n",
            "Mode consistency evaluation results (Sum): {'spearman_inverted_sum': -0.45, 'pearson_inverted_sum': -0.45812744188962606, 'std_inverted_sum': 0.47070695889464603, 'mae_inverted_sum': 0.33557556409838596, 'median_inverted_sum': 0.19140108114177712, 'max_diff_inverted_sum': 1.0, 'min_diff_inverted_sum': 0.03523565859224065}\n",
            "Mode consistency evaluation results (Mean): {'spearman_inverted_mean': -0.45, 'pearson_inverted_mean': -0.4585874607093762, 'std_inverted_mean': 0.47075810707450044, 'mae_inverted_mean': 0.3356481562177781, 'median_inverted_mean': 0.19208913925097848, 'max_diff_inverted_mean': 1.0, 'min_diff_inverted_mean': 0.035057732816540566}\n",
            "Mode consistency evaluation results (Median): {'spearman_inverted_median': -0.016666666666666666, 'pearson_inverted_median': -0.27535769882689864, 'std_inverted_median': 0.5188317020698752, 'mae_inverted_median': 0.46426456781540526, 'median_inverted_median': 0.35171853984739276, 'max_diff_inverted_median': 1.0, 'min_diff_inverted_median': 0.06300045521017716}\n",
            "\n",
            "Evaluation Sample 3:\n",
            "Mode consistency evaluation results (Sum): {'spearman_inverted_sum': 0.5666666666666667, 'pearson_inverted_sum': 0.43049118925498664, 'std_inverted_sum': 0.30703892023380835, 'mae_inverted_sum': 0.26861525886007936, 'median_inverted_sum': 0.19140108114177712, 'max_diff_inverted_sum': 0.6050624820002181, 'min_diff_inverted_sum': 0.09237211696385483}\n",
            "Mode consistency evaluation results (Mean): {'spearman_inverted_mean': 0.5666666666666667, 'pearson_inverted_mean': 0.4307096726538401, 'std_inverted_mean': 0.30696640709092043, 'mae_inverted_mean': 0.268612000227055, 'median_inverted_mean': 0.19208913925097848, 'max_diff_inverted_mean': 0.6045432278386438, 'min_diff_inverted_mean': 0.09249482754313965}\n",
            "Mode consistency evaluation results (Median): {'spearman_inverted_median': 0.26666666666666666, 'pearson_inverted_median': 0.15933147215393845, 'std_inverted_median': 0.43817358127310346, 'mae_inverted_median': 0.40511451560096745, 'median_inverted_median': 0.35828545178319904, 'max_diff_inverted_median': 0.9062165373509813, 'min_diff_inverted_median': 0.07967376680132554}\n",
            "\n",
            "Evaluation Sample 4:\n",
            "Mode consistency evaluation results (Sum): {'spearman_inverted_sum': 0.5166666666666667, 'pearson_inverted_sum': 0.4631494261424674, 'std_inverted_sum': 0.2515766982211524, 'mae_inverted_sum': 0.23324941387504763, 'median_inverted_sum': 0.17440655639464386, 'max_diff_inverted_sum': 0.7426499426364899, 'min_diff_inverted_sum': 0.0003022675919447382}\n",
            "Mode consistency evaluation results (Mean): {'spearman_inverted_mean': 0.5166666666666667, 'pearson_inverted_mean': 0.4632556225913067, 'std_inverted_mean': 0.25153883128279936, 'mae_inverted_mean': 0.23336154505570653, 'median_inverted_mean': 0.17509461450384523, 'max_diff_inverted_mean': 0.7426499426364899, 'min_diff_inverted_mean': 0.00042497817122955084}\n",
            "Mode consistency evaluation results (Median): {'spearman_inverted_median': 0.18333333333333335, 'pearson_inverted_median': 0.10461334973912279, 'std_inverted_median': 0.38118261919235785, 'mae_inverted_median': 0.40720684399850715, 'median_inverted_median': 0.20299567833790422, 'max_diff_inverted_median': 0.9518160687333036, 'min_diff_inverted_median': 0.056835293769836426}\n",
            "\n",
            "Evaluation Sample 5:\n",
            "Mode consistency evaluation results (Sum): {'spearman_inverted_sum': 0.5636363636363636, 'pearson_inverted_sum': 0.5430646684211389, 'std_inverted_sum': 0.29299146032190243, 'mae_inverted_sum': 0.25986870287648256, 'median_inverted_sum': 0.22140838498071402, 'max_diff_inverted_sum': 0.6074007775738924, 'min_diff_inverted_sum': 0.0}\n",
            "Mode consistency evaluation results (Mean): {'spearman_inverted_mean': 0.5878787878787878, 'pearson_inverted_mean': 0.5698985392519131, 'std_inverted_mean': 0.2988265593202672, 'mae_inverted_mean': 0.23898669231218, 'median_inverted_mean': 0.19362962678855555, 'max_diff_inverted_mean': 0.6645121588453202, 'min_diff_inverted_mean': 0.0}\n",
            "Mode consistency evaluation results (Median): {'spearman_inverted_median': 0.41818181818181815, 'pearson_inverted_median': 0.2785619514103483, 'std_inverted_median': 0.4151462977115263, 'mae_inverted_median': 0.31287575281386093, 'median_inverted_median': 0.24196782615502044, 'max_diff_inverted_median': 0.8367074111305164, 'min_diff_inverted_median': 0.0}\n",
            "\n",
            "Evaluation Sample 6:\n",
            "Mode consistency evaluation results (Sum): {'spearman_inverted_sum': 0.28484848484848485, 'pearson_inverted_sum': 0.3610350771294961, 'std_inverted_sum': 0.3283778208221321, 'mae_inverted_sum': 0.3167013144979235, 'median_inverted_sum': 0.3012809008359909, 'max_diff_inverted_sum': 0.6050617081706965, 'min_diff_inverted_sum': 0.11782975438255872}\n",
            "Mode consistency evaluation results (Mean): {'spearman_inverted_mean': 0.28484848484848485, 'pearson_inverted_mean': 0.3605739840964916, 'std_inverted_mean': 0.328406245426632, 'mae_inverted_mean': 0.31697188086780653, 'median_inverted_mean': 0.3012809008359909, 'max_diff_inverted_mean': 0.6064604095665728, 'min_diff_inverted_mean': 0.11634416089455757}\n",
            "Mode consistency evaluation results (Median): {'spearman_inverted_median': 0.43030303030303024, 'pearson_inverted_median': 0.49116546844875414, 'std_inverted_median': 0.3367864812975781, 'mae_inverted_median': 0.2921109218670449, 'median_inverted_median': 0.20443427184952168, 'max_diff_inverted_median': 0.6143668319792112, 'min_diff_inverted_median': 0.0820954381567307}\n",
            "\n",
            "Evaluation Sample 7:\n",
            "Mode consistency evaluation results (Sum): {'spearman_inverted_sum': 0.5333333333333333, 'pearson_inverted_sum': 0.5442889039110393, 'std_inverted_sum': 0.30274069273350285, 'mae_inverted_sum': 0.2757443884746962, 'median_inverted_sum': 0.19140108114177712, 'max_diff_inverted_sum': 0.6569225209687694, 'min_diff_inverted_sum': 0.0}\n",
            "Mode consistency evaluation results (Mean): {'spearman_inverted_mean': 0.5333333333333333, 'pearson_inverted_mean': 0.5441043439798702, 'std_inverted_mean': 0.3027911218723563, 'mae_inverted_mean': 0.2758700175310993, 'median_inverted_mean': 0.19208913925097848, 'max_diff_inverted_mean': 0.6566715519127214, 'min_diff_inverted_mean': 0.0}\n",
            "Mode consistency evaluation results (Median): {'spearman_inverted_median': 0.5666666666666667, 'pearson_inverted_median': 0.3600141203212364, 'std_inverted_median': 0.4073019916652426, 'mae_inverted_median': 0.33306267006852663, 'median_inverted_median': 0.3827922324480232, 'max_diff_inverted_median': 0.7788429723626066, 'min_diff_inverted_median': 0.0}\n",
            "\n",
            "Evaluation Sample 8:\n",
            "Mode consistency evaluation results (Sum): {'spearman_inverted_sum': -0.38333333333333336, 'pearson_inverted_sum': -0.29392780713607763, 'std_inverted_sum': 0.455875751216896, 'mae_inverted_sum': 0.43159519360566745, 'median_inverted_sum': 0.32425910234451294, 'max_diff_inverted_sum': 0.9539191740487559, 'min_diff_inverted_sum': 0.09716114371393458}\n",
            "Mode consistency evaluation results (Mean): {'spearman_inverted_mean': -0.38333333333333336, 'pearson_inverted_mean': -0.2943550705331143, 'std_inverted_mean': 0.45590468838738246, 'mae_inverted_mean': 0.43140185884349713, 'median_inverted_mean': 0.32425910234451294, 'max_diff_inverted_mean': 0.953668204992708, 'min_diff_inverted_mean': 0.09733906948963467}\n",
            "Mode consistency evaluation results (Median): {'spearman_inverted_median': 0.13333333333333333, 'pearson_inverted_median': 0.12446535807253885, 'std_inverted_median': 0.44430689945850926, 'mae_inverted_median': 0.38519388217089484, 'median_inverted_median': 0.32425910234451294, 'max_diff_inverted_median': 0.7186466160321553, 'min_diff_inverted_median': 0.03753947212542463}\n",
            "\n",
            "Evaluation Sample 9:\n",
            "Mode consistency evaluation results (Sum): {'spearman_inverted_sum': 0.4666666666666666, 'pearson_inverted_sum': 0.5246361367554155, 'std_inverted_sum': 0.33081070631623843, 'mae_inverted_sum': 0.2870446923444734, 'median_inverted_sum': 0.2391754378820088, 'max_diff_inverted_sum': 0.6985085879969511, 'min_diff_inverted_sum': 0.019667446613311768}\n",
            "Mode consistency evaluation results (Mean): {'spearman_inverted_mean': 0.4666666666666666, 'pearson_inverted_mean': 0.5244145323370748, 'std_inverted_mean': 0.33087486211849804, 'mae_inverted_mean': 0.28717032140087656, 'median_inverted_mean': 0.2395076335406493, 'max_diff_inverted_mean': 0.698631298576236, 'min_diff_inverted_mean': 0.019667446613311768}\n",
            "Mode consistency evaluation results (Median): {'spearman_inverted_median': 0.43333333333333335, 'pearson_inverted_median': 0.43451841752259496, 'std_inverted_median': 0.3996225999854559, 'mae_inverted_median': 0.31619770636484124, 'median_inverted_median': 0.24785101924989728, 'max_diff_inverted_median': 0.8532204495316434, 'min_diff_inverted_median': 0.019667446613311768}\n",
            "\n",
            "Evaluation Sample 10:\n",
            "Mode consistency evaluation results (Sum): {'spearman_inverted_sum': 0.15, 'pearson_inverted_sum': -0.03252321249820025, 'std_inverted_sum': 0.38284877833862524, 'mae_inverted_sum': 0.3227626549287608, 'median_inverted_sum': 0.2271206354308043, 'max_diff_inverted_sum': 0.7383825480937958, 'min_diff_inverted_sum': 0.050835904718266156}\n",
            "Mode consistency evaluation results (Mean): {'spearman_inverted_mean': 0.15, 'pearson_inverted_mean': -0.03352362936645526, 'std_inverted_mean': 0.3830124221654334, 'mae_inverted_mean': 0.32284874492389715, 'median_inverted_mean': 0.2272433460100891, 'max_diff_inverted_mean': 0.7383825480937958, 'min_diff_inverted_mean': 0.05031665055669188}\n",
            "Mode consistency evaluation results (Median): {'spearman_inverted_median': 0.35, 'pearson_inverted_median': 0.4582707579111183, 'std_inverted_median': 0.34312428711857873, 'mae_inverted_median': 0.28078750103279615, 'median_inverted_median': 0.22353693331624958, 'max_diff_inverted_median': 0.7383825480937958, 'min_diff_inverted_median': 0.027528550750497538}\n",
            "\n",
            "\n",
            "Best Mode Evaluation Results:\n",
            "Best Mode: Mean with an aggregated score of 9.960267386451616\n",
            "  - Best Spearman for Mean: 0.5878787878787878\n",
            "  - Best Pearson for Mean: 0.5698985392519131\n",
            "  - Best Std for Mean: 0.25153883128279936\n",
            "  - Best Mae for Mean: 0.23336154505570653\n",
            "  - Best Median for Mean: 0.17509461450384523\n",
            "  - Best Max_diff for Mean: 1.0\n",
            "  - Best Min_diff for Mean: 0.0\n",
            "\n",
            "Ranking of Modes Based on Aggregated Scores:\n",
            "1. Mean: Score = 9.960267386451616\n",
            "2. Sum: Score = 9.682869540687278\n",
            "3. Median: Score = 8.135463664601943\n",
            "\n",
            "Pareto Analysis - Top Contributing Modes:\n",
            "Mean: Score = 9.960267386451616\n",
            "Sum: Score = 9.682869540687278\n",
            "Median: Score = 8.135463664601943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Relevant Source score for each layer"
      ],
      "metadata": {
        "id": "fRdaWxN9liVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extractRelevantSourceScores(df, dfName):\n",
        "    print(f\"Relevant source scores per layer and overall for {dfName}\")\n",
        "\n",
        "    # Dictionary to store the occurrence count of each source per layer\n",
        "    source_occurrences_per_layer = defaultdict(lambda: defaultdict(int))\n",
        "    # Dictionary to store the total occurrence count of each source across all layers\n",
        "    total_source_occurrences = defaultdict(int)\n",
        "    # Dictionary to track source occurrences for each evaluation sample\n",
        "    source_occurrences_per_eval = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    # Iterate over each evaluation sample\n",
        "    for evaluationSample in range(evaluationSamples):  # Adjust range based on your dataset\n",
        "        eval_df = df[df.index == (evaluationSample + 6)]  # Filter rows for the evaluation sample\n",
        "\n",
        "        # For each layer, track the source occurrences\n",
        "        for layer in eval_df['layer'].unique():\n",
        "            layer_data = eval_df[eval_df['layer'] == layer]  # Data for the current layer\n",
        "\n",
        "            # Get the sources for the current layer\n",
        "            sources = layer_data['source'].values\n",
        "\n",
        "            # Count the occurrences of each source\n",
        "            for source in sources:\n",
        "                source_occurrences_per_layer[layer][source] += 1\n",
        "                total_source_occurrences[source] += 1\n",
        "                source_occurrences_per_eval[evaluationSample][source] += 1\n",
        "\n",
        "    # Print the relevance of sources for each evaluation sample\n",
        "    print(\"Relevant sources for each evaluation sample:\")\n",
        "    for eval_sample, source_counts in source_occurrences_per_eval.items():\n",
        "        total_occurrences = sum(source_counts.values())  # Total occurrences for this evaluation sample\n",
        "        print(f\"Evaluation Sample {eval_sample}:\")\n",
        "        for source, count in source_counts.items():\n",
        "            relevance_score = count / total_occurrences  # Relevance based on proportion for this eval sample\n",
        "            print(f\"  - Source {source}: Relevance = {relevance_score:.4f}\")\n",
        "        print()\n",
        "\n",
        "    # Calculate and print total relevance scores across all layers\n",
        "    print(f\"Overall relevant source scores:\")\n",
        "    total_occurrences_in_all_layers = sum(total_source_occurrences.values())\n",
        "    for source, count in total_source_occurrences.items():\n",
        "        overall_relevance_score = count / total_occurrences_in_all_layers  # Relevance based on total occurrences across all layers\n",
        "        print(f\"  - Source {source} overall relevance score: {overall_relevance_score:.4f}\")\n",
        "    print()\n",
        "\n",
        "    # Print relevant sources for each layer\n",
        "    print(\"Relevant sources per layer:\")\n",
        "    for layer, source_counts in source_occurrences_per_layer.items():\n",
        "        total_occurrences = sum(source_counts.values())\n",
        "        print(f\"Layer {layer}:\")\n",
        "        for source, count in source_counts.items():\n",
        "            relevance_score = count / total_occurrences  # Relevance based on occurrence ratio\n",
        "            print(f\"  - Source {source} relevance score for this layer: {relevance_score:.4f}\")\n",
        "        print()\n",
        "\n",
        "# Example function calls with dataframes\n",
        "extractRelevantSourceScores(evalDf, \"Evaluation\")\n",
        "#extractRelevantSourceScores(generatedEvalDf, \"Generated Evaluation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIkSFB-w6gHw",
        "outputId": "9f2b45c4-a8f4-4c46-b610-2f8f338becd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevant source scores per layer and overall for Evaluation\n",
            "Relevant sources for each evaluation sample:\n",
            "Evaluation Sample 0:\n",
            "  - Source 0:3: Relevance = 0.1112\n",
            "  - Source 0:7: Relevance = 0.1111\n",
            "  - Source 0:4: Relevance = 0.1109\n",
            "  - Source 0:1: Relevance = 0.1114\n",
            "  - Source 0:2: Relevance = 0.1113\n",
            "  - Source 0:6: Relevance = 0.1110\n",
            "  - Source 0:0: Relevance = 0.1113\n",
            "  - Source 0:8: Relevance = 0.1113\n",
            "  - Source 0:5: Relevance = 0.1104\n",
            "\n",
            "Evaluation Sample 1:\n",
            "  - Source 0:3: Relevance = 0.1111\n",
            "  - Source 0:7: Relevance = 0.1110\n",
            "  - Source 0:4: Relevance = 0.1108\n",
            "  - Source 0:1: Relevance = 0.1113\n",
            "  - Source 0:2: Relevance = 0.1121\n",
            "  - Source 0:6: Relevance = 0.1109\n",
            "  - Source 0:0: Relevance = 0.1112\n",
            "  - Source 0:8: Relevance = 0.1112\n",
            "  - Source 0:5: Relevance = 0.1103\n",
            "\n",
            "Evaluation Sample 2:\n",
            "  - Source 0:3: Relevance = 0.1112\n",
            "  - Source 0:7: Relevance = 0.1111\n",
            "  - Source 0:4: Relevance = 0.1109\n",
            "  - Source 0:1: Relevance = 0.1114\n",
            "  - Source 0:2: Relevance = 0.1113\n",
            "  - Source 0:6: Relevance = 0.1110\n",
            "  - Source 0:0: Relevance = 0.1113\n",
            "  - Source 0:8: Relevance = 0.1113\n",
            "  - Source 0:5: Relevance = 0.1104\n",
            "\n",
            "Evaluation Sample 3:\n",
            "  - Source 0:3: Relevance = 0.1112\n",
            "  - Source 0:7: Relevance = 0.1111\n",
            "  - Source 0:4: Relevance = 0.1109\n",
            "  - Source 0:1: Relevance = 0.1114\n",
            "  - Source 0:2: Relevance = 0.1113\n",
            "  - Source 0:6: Relevance = 0.1110\n",
            "  - Source 0:0: Relevance = 0.1113\n",
            "  - Source 0:8: Relevance = 0.1113\n",
            "  - Source 0:5: Relevance = 0.1104\n",
            "\n",
            "Overall relevant source scores:\n",
            "  - Source 0:3 overall relevance score: 0.1112\n",
            "  - Source 0:7 overall relevance score: 0.1111\n",
            "  - Source 0:4 overall relevance score: 0.1109\n",
            "  - Source 0:1 overall relevance score: 0.1114\n",
            "  - Source 0:2 overall relevance score: 0.1115\n",
            "  - Source 0:6 overall relevance score: 0.1110\n",
            "  - Source 0:0 overall relevance score: 0.1113\n",
            "  - Source 0:8 overall relevance score: 0.1113\n",
            "  - Source 0:5 overall relevance score: 0.1103\n",
            "\n",
            "Relevant sources per layer:\n",
            "Layer 0:\n",
            "  - Source 0:3 relevance score for this layer: 0.1250\n",
            "  - Source 0:7 relevance score for this layer: 0.1250\n",
            "  - Source 0:4 relevance score for this layer: 0.1251\n",
            "  - Source 0:1 relevance score for this layer: 0.1251\n",
            "  - Source 0:2 relevance score for this layer: 0.1250\n",
            "  - Source 0:6 relevance score for this layer: 0.1250\n",
            "  - Source 0:0 relevance score for this layer: 0.1250\n",
            "  - Source 0:8 relevance score for this layer: 0.1250\n",
            "\n",
            "Layer 2:\n",
            "  - Source 0:7 relevance score for this layer: 0.1107\n",
            "  - Source 0:3 relevance score for this layer: 0.1113\n",
            "  - Source 0:4 relevance score for this layer: 0.1112\n",
            "  - Source 0:1 relevance score for this layer: 0.1108\n",
            "  - Source 0:2 relevance score for this layer: 0.1112\n",
            "  - Source 0:5 relevance score for this layer: 0.1112\n",
            "  - Source 0:6 relevance score for this layer: 0.1113\n",
            "  - Source 0:8 relevance score for this layer: 0.1112\n",
            "  - Source 0:0 relevance score for this layer: 0.1112\n",
            "\n",
            "Layer 3:\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1112\n",
            "  - Source 0:5 relevance score for this layer: 0.1112\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 4:\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:0 relevance score for this layer: 0.1112\n",
            "  - Source 0:5 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 5:\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1112\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1112\n",
            "  - Source 0:8 relevance score for this layer: 0.1112\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 6:\n",
            "  - Source 0:1 relevance score for this layer: 0.1112\n",
            "  - Source 0:8 relevance score for this layer: 0.1112\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1112\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 7:\n",
            "  - Source 0:4 relevance score for this layer: 0.2500\n",
            "  - Source 0:0 relevance score for this layer: 0.2500\n",
            "  - Source 0:7 relevance score for this layer: 0.5000\n",
            "\n",
            "Layer 8:\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1112\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 9:\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1112\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 10:\n",
            "  - Source 0:3 relevance score for this layer: 0.1107\n",
            "  - Source 0:6 relevance score for this layer: 0.1115\n",
            "  - Source 0:7 relevance score for this layer: 0.1121\n",
            "  - Source 0:8 relevance score for this layer: 0.1107\n",
            "  - Source 0:4 relevance score for this layer: 0.1089\n",
            "  - Source 0:2 relevance score for this layer: 0.1112\n",
            "  - Source 0:5 relevance score for this layer: 0.1121\n",
            "  - Source 0:1 relevance score for this layer: 0.1112\n",
            "  - Source 0:0 relevance score for this layer: 0.1117\n",
            "\n",
            "Layer 11:\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1112\n",
            "  - Source 0:4 relevance score for this layer: 0.1112\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1111\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 12:\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1111\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 13:\n",
            "  - Source 0:1 relevance score for this layer: 0.1128\n",
            "  - Source 0:5 relevance score for this layer: 0.1126\n",
            "  - Source 0:6 relevance score for this layer: 0.1071\n",
            "  - Source 0:8 relevance score for this layer: 0.1107\n",
            "  - Source 0:2 relevance score for this layer: 0.1129\n",
            "  - Source 0:4 relevance score for this layer: 0.1076\n",
            "  - Source 0:7 relevance score for this layer: 0.1128\n",
            "  - Source 0:0 relevance score for this layer: 0.1106\n",
            "  - Source 0:3 relevance score for this layer: 0.1128\n",
            "\n",
            "Layer 14:\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1112\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 15:\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1112\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 16:\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1112\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 17:\n",
            "  - Source 0:8 relevance score for this layer: 0.1082\n",
            "  - Source 0:0 relevance score for this layer: 0.1092\n",
            "  - Source 0:6 relevance score for this layer: 0.1086\n",
            "  - Source 0:5 relevance score for this layer: 0.1071\n",
            "  - Source 0:1 relevance score for this layer: 0.1092\n",
            "  - Source 0:7 relevance score for this layer: 0.1064\n",
            "  - Source 0:4 relevance score for this layer: 0.1092\n",
            "  - Source 0:2 relevance score for this layer: 0.1345\n",
            "  - Source 0:3 relevance score for this layer: 0.1076\n",
            "\n",
            "Layer 18:\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:8 relevance score for this layer: 0.1112\n",
            "  - Source 0:6 relevance score for this layer: 0.1112\n",
            "  - Source 0:1 relevance score for this layer: 0.1109\n",
            "  - Source 0:5 relevance score for this layer: 0.1112\n",
            "  - Source 0:7 relevance score for this layer: 0.1112\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1112\n",
            "  - Source 0:3 relevance score for this layer: 0.1109\n",
            "\n",
            "Layer 19:\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1111\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1112\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1112\n",
            "\n",
            "Layer 20:\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1112\n",
            "  - Source 0:3 relevance score for this layer: 0.1112\n",
            "  - Source 0:7 relevance score for this layer: 0.1112\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 21:\n",
            "  - Source 0:5 relevance score for this layer: 0.1112\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 22:\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 23:\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.2222\n",
            "  - Source 0:1 relevance score for this layer: 0.2222\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 24:\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:0 relevance score for this layer: 0.1112\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1112\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1112\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 25:\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:0 relevance score for this layer: 0.1112\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1112\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1112\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 26:\n",
            "  - Source 0:1 relevance score for this layer: 0.1114\n",
            "  - Source 0:0 relevance score for this layer: 0.1116\n",
            "  - Source 0:8 relevance score for this layer: 0.1114\n",
            "  - Source 0:5 relevance score for this layer: 0.1101\n",
            "  - Source 0:7 relevance score for this layer: 0.1104\n",
            "  - Source 0:6 relevance score for this layer: 0.1114\n",
            "  - Source 0:4 relevance score for this layer: 0.1114\n",
            "  - Source 0:2 relevance score for this layer: 0.1109\n",
            "  - Source 0:3 relevance score for this layer: 0.1113\n",
            "\n",
            "Layer 27:\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:1 relevance score for this layer: 0.1112\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 28:\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 29:\n",
            "  - Source 0:8 relevance score for this layer: 0.1147\n",
            "  - Source 0:5 relevance score for this layer: 0.1139\n",
            "  - Source 0:4 relevance score for this layer: 0.1061\n",
            "  - Source 0:6 relevance score for this layer: 0.1073\n",
            "  - Source 0:0 relevance score for this layer: 0.1147\n",
            "  - Source 0:7 relevance score for this layer: 0.1072\n",
            "  - Source 0:3 relevance score for this layer: 0.1086\n",
            "  - Source 0:2 relevance score for this layer: 0.1129\n",
            "  - Source 0:1 relevance score for this layer: 0.1147\n",
            "\n",
            "Layer 30:\n",
            "  - Source 0:0 relevance score for this layer: 0.1110\n",
            "  - Source 0:1 relevance score for this layer: 0.1110\n",
            "  - Source 0:5 relevance score for this layer: 0.1112\n",
            "  - Source 0:3 relevance score for this layer: 0.1112\n",
            "  - Source 0:4 relevance score for this layer: 0.1112\n",
            "  - Source 0:7 relevance score for this layer: 0.1112\n",
            "  - Source 0:6 relevance score for this layer: 0.1112\n",
            "  - Source 0:8 relevance score for this layer: 0.1110\n",
            "  - Source 0:2 relevance score for this layer: 0.1112\n",
            "\n",
            "Layer 31:\n",
            "  - Source 0:0 relevance score for this layer: 0.1110\n",
            "  - Source 0:1 relevance score for this layer: 0.1110\n",
            "  - Source 0:5 relevance score for this layer: 0.1112\n",
            "  - Source 0:3 relevance score for this layer: 0.1112\n",
            "  - Source 0:4 relevance score for this layer: 0.1112\n",
            "  - Source 0:7 relevance score for this layer: 0.1112\n",
            "  - Source 0:6 relevance score for this layer: 0.1112\n",
            "  - Source 0:8 relevance score for this layer: 0.1110\n",
            "  - Source 0:2 relevance score for this layer: 0.1112\n",
            "\n",
            "Layer 32:\n",
            "  - Source 0:0 relevance score for this layer: 0.1110\n",
            "  - Source 0:1 relevance score for this layer: 0.1110\n",
            "  - Source 0:5 relevance score for this layer: 0.1112\n",
            "  - Source 0:3 relevance score for this layer: 0.1112\n",
            "  - Source 0:4 relevance score for this layer: 0.1112\n",
            "  - Source 0:7 relevance score for this layer: 0.1112\n",
            "  - Source 0:6 relevance score for this layer: 0.1112\n",
            "  - Source 0:8 relevance score for this layer: 0.1110\n",
            "  - Source 0:2 relevance score for this layer: 0.1112\n",
            "\n",
            "Layer 33:\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1112\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1112\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 34:\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 35:\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 36:\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1111\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "\n",
            "Layer 37:\n",
            "  - Source 0:6 relevance score for this layer: 0.1111\n",
            "  - Source 0:0 relevance score for this layer: 0.1111\n",
            "  - Source 0:5 relevance score for this layer: 0.1111\n",
            "  - Source 0:4 relevance score for this layer: 0.1111\n",
            "  - Source 0:3 relevance score for this layer: 0.1111\n",
            "  - Source 0:7 relevance score for this layer: 0.1111\n",
            "  - Source 0:2 relevance score for this layer: 0.1111\n",
            "  - Source 0:8 relevance score for this layer: 0.1111\n",
            "  - Source 0:1 relevance score for this layer: 0.1111\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting most relevant layers based on both sentence and difference similarity"
      ],
      "metadata": {
        "id": "PpUxizpARd2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extractRelevantLayersBasedOnTotalSimilarity(df, dfName):\n",
        "    print(f\"Most relevant layers for {dfName}\")\n",
        "    total_relevance_scores = {layer: [] for layer in df['layer'].unique()}\n",
        "\n",
        "    # Iterate over each evaluation sample\n",
        "    for evaluationSample in range(evaluationSamples):  # Adjust range based on your dataset\n",
        "        evalSample = sentences[evaluationSample + 6]  # Adjust index based on your dataset\n",
        "        evalEmbedding = model.encode(evalSample)\n",
        "\n",
        "        # Iterate over each training sample\n",
        "        for trainingSample in range(trainSamples):  # Adjust based on your dataset\n",
        "            trainSample = sentences[trainingSample]\n",
        "            trainEmbedding = model.encode(trainSample)\n",
        "\n",
        "            # Compute cosine similarity between the evaluation and training sentence embeddings\n",
        "            sentenceCS = compute_cosine_similarity(evalEmbedding, trainEmbedding)\n",
        "\n",
        "            # Filter the DataFrame for the current evaluation sample and layer\n",
        "            eval_df = df[df.index == (evaluationSample)]  # Filter rows for the evaluation sample\n",
        "\n",
        "            # For each layer, calculate the relevance score\n",
        "            for layer in eval_df['layer'].unique():\n",
        "                layer_data = eval_df[eval_df['layer'] == layer]  # Data for the current layer\n",
        "\n",
        "                # Get the layer difference (you already have this as 'difference')\n",
        "                layer_difference = layer_data['difference'].values\n",
        "\n",
        "                # Reduce layer_difference to a scalar by taking the mean if it's an array\n",
        "                if len(layer_difference.shape) > 1:\n",
        "                    layer_difference = np.mean(layer_difference)\n",
        "\n",
        "                # Normalize the components\n",
        "                sentenceCS_normalized = (sentenceCS + 1) / 2  # Rescale to 0â1 range\n",
        "                layer_difference_normalized = (layer_difference - np.min(layer_difference)) / (np.max(layer_difference) - np.min(layer_difference))\n",
        "\n",
        "                # Calculate the relevance score: combine sentenceCS and layer difference\n",
        "                relevance_score = sentenceCS_normalized * (1 - layer_difference_normalized)  # Adjust this as needed\n",
        "\n",
        "                # Debug print relevance score\n",
        "                #print(f\"Relevance score for evalSample {evaluationSample}, layer {layer}: {relevance_score}\")\n",
        "\n",
        "                # Ensure that the relevance score is a scalar before appending\n",
        "                total_relevance_scores[layer].append(relevance_score)\n",
        "\n",
        "        # Calculate total relevance scores and sort by relevance\n",
        "    layer_relevance = {}\n",
        "    for layer, relevance_scores in total_relevance_scores.items():\n",
        "        # Flatten the list if there are multiple sub-lists for each layer\n",
        "        flattened_relevance_scores = np.concatenate([np.array(relevance_scores[i]) for i in range(len(relevance_scores))])\n",
        "        # Calculate the total relevance score by averaging the flattened list\n",
        "        total_relevance = np.mean(flattened_relevance_scores)\n",
        "        layer_relevance[layer] = total_relevance\n",
        "\n",
        "    # Sort layers by their total relevance score in descending order\n",
        "    sorted_layers = sorted(layer_relevance.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Print sorted relevance scores\n",
        "    for layer, relevance_score in sorted_layers:\n",
        "        print(f\"Layer {layer}, Type={hidden_sizes[layer][0]} - Total Relevance Score: {relevance_score}\")\n",
        "    print()\n",
        "\n",
        "# Example function calls with dataframes\n",
        "extractRelevantLayersBasedOnTotalSimilarity(evalDf, \"Evaluation\")\n",
        "#extractRelevantLayersBasedOnTotalSimilarity(generatedEvalDf, \"Generated Evaluation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5XiYwmqM9SO",
        "outputId": "cd46724e-cab9-46f2-8240-8305810869c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most relevant layers for Evaluation\n",
            "Layer 13, Type=GELU - Total Relevance Score: 0.6886349474362854\n",
            "Layer 4, Type=Linear - Total Relevance Score: 0.6617097377301974\n",
            "Layer 23, Type=Dropout - Total Relevance Score: 0.6525621726547421\n",
            "Layer 29, Type=GELU - Total Relevance Score: 0.5718572391069195\n",
            "Layer 7, Type=Dropout - Total Relevance Score: 0.5501056762765494\n",
            "Layer 6, Type=Linear - Total Relevance Score: 0.49088975551062075\n",
            "Layer 21, Type=Linear - Total Relevance Score: 0.4503052048080021\n",
            "Layer 30, Type=Linear - Total Relevance Score: 0.4152231611215884\n",
            "Layer 31, Type=Sequential - Total Relevance Score: 0.4152231611215884\n",
            "Layer 32, Type=FeedForward - Total Relevance Score: 0.4152231611215884\n",
            "Layer 27, Type=LayerNorm - Total Relevance Score: 0.4098545452544785\n",
            "Layer 14, Type=Linear - Total Relevance Score: 0.3274401048938315\n",
            "Layer 15, Type=Sequential - Total Relevance Score: 0.3274401048938315\n",
            "Layer 16, Type=FeedForward - Total Relevance Score: 0.3274401048938315\n",
            "Layer 5, Type=Linear - Total Relevance Score: 0.30730662495391275\n",
            "Layer 19, Type=LayerNorm - Total Relevance Score: 0.3043895953519636\n",
            "Layer 3, Type=LayerNorm - Total Relevance Score: 0.2709505198935718\n",
            "Layer 37, Type=Linear - Total Relevance Score: 0.2683399390363927\n",
            "Layer 11, Type=LayerNorm - Total Relevance Score: 0.2489430742302529\n",
            "Layer 36, Type=LayerNorm - Total Relevance Score: 0.2453083759626142\n",
            "Layer 2, Type=Dropout - Total Relevance Score: 0.244023400137274\n",
            "Layer 12, Type=Linear - Total Relevance Score: 0.24062288624561914\n",
            "Layer 28, Type=Linear - Total Relevance Score: 0.2296195946966839\n",
            "Layer 8, Type=Linear - Total Relevance Score: 0.22068888102501624\n",
            "Layer 9, Type=MultiHeadAttention - Total Relevance Score: 0.22068888102501624\n",
            "Layer 34, Type=TransformerBlock - Total Relevance Score: 0.20912384124860492\n",
            "Layer 35, Type=Sequential - Total Relevance Score: 0.20912384124860492\n",
            "Layer 22, Type=Linear - Total Relevance Score: 0.20867371744480886\n",
            "Layer 18, Type=TransformerBlock - Total Relevance Score: 0.1818020233165001\n",
            "Layer 20, Type=Linear - Total Relevance Score: 0.17879835138304606\n",
            "Layer 17, Type=Dropout - Total Relevance Score: 0.16768976681363335\n",
            "Layer 10, Type=Dropout - Total Relevance Score: 0.15390954282932076\n",
            "Layer 24, Type=Linear - Total Relevance Score: 0.14316329567766592\n",
            "Layer 25, Type=MultiHeadAttention - Total Relevance Score: 0.14316329567766592\n",
            "Layer 26, Type=Dropout - Total Relevance Score: 0.14109629000791052\n",
            "Layer 33, Type=Dropout - Total Relevance Score: 0.13455767995556303\n",
            "Layer 0, Type=Embedding - Total Relevance Score: 0.12452411535037437\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Relevant layers based on direct similarity between sentences and activation"
      ],
      "metadata": {
        "id": "0QG_R29gCP8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extractLayersBestMatchingSentenceSimilarities(df, dfName):\n",
        "    print(f\"Most relevant layers for {dfName}\")\n",
        "    total_relevance_scores = {layer: [] for layer in df['layer'].unique()}\n",
        "\n",
        "    # Iterate over each evaluation sample\n",
        "    for evaluationSample in range(evaluationSamples):  # Adjust range based on your dataset\n",
        "        evalSample = sentences[evaluationSample + 6]  # Adjust index based on your dataset\n",
        "        evalEmbedding = model.encode(evalSample)\n",
        "\n",
        "        # Iterate over each training sample\n",
        "        for trainingSample in range(trainSamples):  # Adjust based on your dataset\n",
        "            trainSample = sentences[trainingSample]\n",
        "            trainEmbedding = model.encode(trainSample)\n",
        "\n",
        "            # Compute cosine similarity between the evaluation and training sentence embeddings\n",
        "            sentenceCS = compute_cosine_similarity(evalEmbedding, trainEmbedding)\n",
        "\n",
        "            # Filter the DataFrame for the current evaluation sample and layer\n",
        "            eval_df = df[df.index == (evaluationSample)]  # Filter rows for the evaluation sample\n",
        "\n",
        "            # For each layer, calculate the relevance score\n",
        "            for layer in eval_df['layer'].unique():\n",
        "                layer_data = eval_df[eval_df['layer'] == layer]  # Data for the current layer\n",
        "\n",
        "                # Get the layer difference (you already have this as 'difference')\n",
        "                layer_difference = layer_data['difference'].values\n",
        "\n",
        "                # Reduce layer_difference to a scalar by taking the mean if it's an array\n",
        "                if len(layer_difference.shape) > 1:\n",
        "                    layer_difference = np.mean(layer_difference)\n",
        "\n",
        "                # Min-Max Normalization for both sentenceCS and layer_difference\n",
        "                sentenceCS_min_max = (sentenceCS + 1) / 2  # Rescale to 0â1 range\n",
        "                layer_difference_min_max = (layer_difference - np.min(layer_difference)) / (np.max(layer_difference) - np.min(layer_difference))\n",
        "\n",
        "                # Z-Score Normalization for both sentenceCS and layer_difference with checks\n",
        "                if np.std(sentenceCS) != 0:  # Check if std is not zero\n",
        "                    sentenceCS_z_score = (sentenceCS - np.mean(sentenceCS)) / np.std(sentenceCS)\n",
        "                else:\n",
        "                    sentenceCS_z_score = 0  # If std is zero, set to 0 or use a fallback\n",
        "\n",
        "                if np.std(layer_difference) != 0:  # Check if std is not zero\n",
        "                    layer_difference_z_score = (layer_difference - np.mean(layer_difference)) / np.std(layer_difference)\n",
        "                else:\n",
        "                    layer_difference_z_score = 0  # If std is zero, set to 0 or use a fallback\n",
        "\n",
        "                # Calculate the relevance score using Min-Max normalization\n",
        "                relevance_score_min_max = abs(layer_difference_min_max - sentenceCS_min_max)\n",
        "\n",
        "                # Calculate the relevance score using Z-Score normalization\n",
        "                relevance_score_z_score = abs(layer_difference_z_score - sentenceCS_z_score)\n",
        "\n",
        "                # Combine both relevance scores by averaging them\n",
        "                combined_relevance_score = (relevance_score_min_max + relevance_score_z_score) / 2\n",
        "\n",
        "                # Ensure that the relevance score is a scalar before appending\n",
        "                total_relevance_scores[layer].append(combined_relevance_score)\n",
        "\n",
        "    # Calculate total relevance scores and sort by relevance\n",
        "    layer_relevance = {}\n",
        "\n",
        "    # Calculate total relevance scores using median, mean, and sum (normalized by hidden_sizes[layer][1])\n",
        "    for layer, relevance_scores in total_relevance_scores.items():\n",
        "        # Flatten the list if there are multiple sub-lists for each layer\n",
        "        flattened_relevance_scores = np.concatenate([np.array(relevance_scores[i]) for i in range(len(relevance_scores))])\n",
        "\n",
        "        # Calculate total relevance using median, mean, and sum\n",
        "        total_relevance_median = np.median(flattened_relevance_scores)\n",
        "        total_relevance_mean = np.mean(flattened_relevance_scores)\n",
        "\n",
        "        # Normalize the sum by dividing by hidden_sizes[layer][1]\n",
        "        total_relevance_sum = np.sum(flattened_relevance_scores) / hidden_sizes[layer][1]\n",
        "\n",
        "        # Store all three relevance scores in a dictionary for each layer\n",
        "        layer_relevance[layer] = {\n",
        "            'median': total_relevance_median,\n",
        "            'mean': total_relevance_mean,\n",
        "            'sum': total_relevance_sum\n",
        "        }\n",
        "\n",
        "    # Sort layers by the chosen relevance metric (e.g., 'median', 'mean', or 'sum')\n",
        "    sorted_layers = sorted(layer_relevance.items(),\n",
        "                           key=lambda x: (x[1]['median'], x[1]['mean'], x[1]['sum']))\n",
        "\n",
        "    # Print sorted relevance scores\n",
        "    for layer, relevance_scores in sorted_layers:\n",
        "        print(f\"Layer {layer}, Type={hidden_sizes[layer][0]} - \"\n",
        "              f\"Total Relevance Scores (Median: {relevance_scores['median']}, \"\n",
        "              f\"Mean: {relevance_scores['mean']}, Sum (normalized): {relevance_scores['sum']})\")\n",
        "\n",
        "# Example function calls with dataframes\n",
        "extractLayersBestMatchingSentenceSimilarities(evalDf, \"Evaluation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE9yYuNAc8k3",
        "outputId": "64caff81-1208-44db-d694-45902e718f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most relevant layers for Evaluation\n",
            "Layer 8, Type=Linear - Total Relevance Scores (Median: 0.0698969884843425, Mean: 0.21248351518467962, Sum (normalized): 190.76482255187057)\n",
            "Layer 9, Type=MultiHeadAttention - Total Relevance Scores (Median: 0.0698969884843425, Mean: 0.21248351518467962, Sum (normalized): 190.76482255187057)\n",
            "Layer 18, Type=TransformerBlock - Total Relevance Scores (Median: 0.08334401587582568, Mean: 0.22994817996214828, Sum (normalized): 206.23477390355174)\n",
            "Layer 11, Type=LayerNorm - Total Relevance Scores (Median: 0.09164316676011014, Mean: 0.2460239701416154, Sum (normalized): 220.90902318965883)\n",
            "Layer 34, Type=TransformerBlock - Total Relevance Scores (Median: 0.09337356207031636, Mean: 0.2565708885324674, Sum (normalized): 230.3124616592227)\n",
            "Layer 35, Type=Sequential - Total Relevance Scores (Median: 0.09337356207031636, Mean: 0.2565708885324674, Sum (normalized): 230.3124616592227)\n",
            "Layer 20, Type=Linear - Total Relevance Scores (Median: 0.10043148762108761, Mean: 0.2644982436193645, Sum (normalized): 237.53182112535902)\n",
            "Layer 36, Type=LayerNorm - Total Relevance Scores (Median: 0.10478703077689977, Mean: 0.25973373208958866, Sum (normalized): 233.45598341333732)\n",
            "Layer 12, Type=Linear - Total Relevance Scores (Median: 0.11511329958137437, Mean: 0.24421678309978598, Sum (normalized): 878.6080360738393)\n",
            "Layer 22, Type=Linear - Total Relevance Scores (Median: 0.12928372476137429, Mean: 0.260858674404821, Sum (normalized): 234.16141944620264)\n",
            "Layer 3, Type=LayerNorm - Total Relevance Scores (Median: 0.13033773176283509, Mean: 0.28173212723563357, Sum (normalized): 252.9683041932559)\n",
            "Layer 19, Type=LayerNorm - Total Relevance Scores (Median: 0.1399835241592094, Mean: 0.2802244419598796, Sum (normalized): 251.61819684314193)\n",
            "Layer 24, Type=Linear - Total Relevance Scores (Median: 0.15977012047271597, Mean: 0.2712663260277938, Sum (normalized): 243.6098763819914)\n",
            "Layer 25, Type=MultiHeadAttention - Total Relevance Scores (Median: 0.15977012047271597, Mean: 0.2712663260277938, Sum (normalized): 243.6098763819914)\n",
            "Layer 10, Type=Dropout - Total Relevance Scores (Median: 0.16097595390121955, Mean: 0.2649054612729876, Sum (normalized): 213.04883751285615)\n",
            "Layer 33, Type=Dropout - Total Relevance Scores (Median: 0.16118405091200722, Mean: 0.27718117777656315, Sum (normalized): 218.6771817901297)\n",
            "Layer 17, Type=Dropout - Total Relevance Scores (Median: 0.1613092240895524, Mean: 0.2897950071758636, Sum (normalized): 235.9640752439512)\n",
            "Layer 2, Type=Dropout - Total Relevance Scores (Median: 0.16344454099617706, Mean: 0.2828393242512808, Sum (normalized): 224.94565006859673)\n",
            "Layer 28, Type=Linear - Total Relevance Scores (Median: 0.17685175116092355, Mean: 0.24257888133000927, Sum (normalized): 872.747014326756)\n",
            "Layer 26, Type=Dropout - Total Relevance Scores (Median: 0.18230479465840782, Mean: 0.2992646039996777, Sum (normalized): 241.6717544278647)\n",
            "Layer 0, Type=Embedding - Total Relevance Scores (Median: 0.1868614447470591, Mean: 0.30222382314847657, Sum (normalized): 241.2281296745002)\n",
            "Layer 14, Type=Linear - Total Relevance Scores (Median: 0.2122375026550648, Mean: 0.3498369934499803, Sum (normalized): 78.51972883585984)\n",
            "Layer 15, Type=Sequential - Total Relevance Scores (Median: 0.2122375026550648, Mean: 0.3498369934499803, Sum (normalized): 314.07891534343935)\n",
            "Layer 16, Type=FeedForward - Total Relevance Scores (Median: 0.2122375026550648, Mean: 0.3498369934499803, Sum (normalized): 314.07891534343935)\n",
            "Layer 5, Type=Linear - Total Relevance Scores (Median: 0.2143311497737664, Mean: 0.34899995294810415, Sum (normalized): 313.41831712019194)\n",
            "Layer 27, Type=LayerNorm - Total Relevance Scores (Median: 0.21892030794523434, Mean: 0.3687983804965811, Sum (normalized): 331.10219186509465)\n",
            "Layer 29, Type=GELU - Total Relevance Scores (Median: 0.2562606197529625, Mean: 0.2726686400765512, Sum (normalized): 221.2947439579614)\n",
            "Layer 6, Type=Linear - Total Relevance Scores (Median: 0.3018533939040563, Mean: 0.4452733880151767, Sum (normalized): 399.87637462769186)\n",
            "Layer 30, Type=Linear - Total Relevance Scores (Median: 0.3284382251507776, Mean: 0.42752068982556474, Sum (normalized): 96.02515494128896)\n",
            "Layer 31, Type=Sequential - Total Relevance Scores (Median: 0.3284382251507776, Mean: 0.42752068982556474, Sum (normalized): 384.10061976515584)\n",
            "Layer 32, Type=FeedForward - Total Relevance Scores (Median: 0.3284382251507776, Mean: 0.42752068982556474, Sum (normalized): 384.10061976515584)\n",
            "Layer 13, Type=GELU - Total Relevance Scores (Median: 0.37215780443096624, Mean: 0.4228286561615827, Sum (normalized): 323.05375614480823)\n",
            "Layer 37, Type=Linear - Total Relevance Scores (Median: 0.46092681563026805, Mean: 0.4719262136407787, Sum (normalized): 424.71668981167994)\n",
            "Layer 21, Type=Linear - Total Relevance Scores (Median: 0.4797399552646718, Mean: 0.5310507156647749, Sum (normalized): 476.7701412120603)\n",
            "Layer 4, Type=Linear - Total Relevance Scores (Median: 0.4987556331207482, Mean: 0.572621083493043, Sum (normalized): 514.091454516215)\n",
            "Layer 23, Type=Dropout - Total Relevance Scores (Median: 0.5441006309777362, Mean: 0.6549538665360337, Sum (normalized): 0.7675240623469145)\n",
            "Layer 7, Type=Dropout - Total Relevance Scores (Median: 0.6642341336163513, Mean: 0.7407590551609102, Sum (normalized): 0.38581200789630743)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr, kendalltau, pearsonr\n",
        "\n",
        "def compute_cosine_similarity(image1, image2):\n",
        "    \"\"\"Compute cosine similarity between two images.\"\"\"\n",
        "    vec1 = image1.flatten().reshape(1, -1)\n",
        "    vec2 = image2.flatten().reshape(1, -1)\n",
        "    return cosine_similarity(vec1, vec2)[0][0]\n",
        "\n",
        "def computeSimilarity(sample, train_sample):\n",
        "    # Compute similarities\n",
        "    cosine_similarity = compute_cosine_similarity(sample, train_sample)\n",
        "    euclidean_distance = np.linalg.norm(sample - train_sample)  # Euclidean\n",
        "    manhattan_distance = np.sum(np.abs(sample - train_sample))  # Manhattan\n",
        "    jaccard_similarity = (\n",
        "        np.sum(np.minimum(sample, train_sample)) / np.sum(np.maximum(sample, train_sample))\n",
        "        if np.sum(np.maximum(sample, train_sample)) > 0 else None\n",
        "    )\n",
        "    hamming_distance = np.mean(sample != train_sample)  # Hamming\n",
        "    try:\n",
        "        pearson_correlation, _ = pearsonr(sample.flatten(), train_sample.flatten())  # Pearson\n",
        "    except ValueError:\n",
        "        pearson_correlation = None\n",
        "\n",
        "    return cosine_similarity, euclidean_distance, manhattan_distance, jaccard_similarity, hamming_distance, pearson_correlation\n",
        "\n",
        "def blendActivations(evaluationToCheck, name, eval_df):\n",
        "    _, mostUsed = getMostUsedFromDataFrame(eval_df, evaluationToCheck, 5, info=False)\n",
        "    totalSources = sum(count for _, count in mostUsed)\n",
        "\n",
        "    # Slice to get the first two dimensions from hidden_sizes\n",
        "    neurons_per_layer = np.array([layer[1] for layer in hidden_sizes])\n",
        "\n",
        "    # Create a single zero-initialized array of shape (38, max_neurons)\n",
        "    relevantParts = np.zeros((len(neurons_per_layer), int(neurons_per_layer.max())))\n",
        "    blendedActivations = np.zeros_like(relevantParts)\n",
        "\n",
        "    #print(relevantParts.shape)\n",
        "    evalActivations = np.zeros_like(relevantParts)\n",
        "    evalEntries = eval_df[eval_df.index == evaluationToCheck]\n",
        "    for entry in evalEntries[['layer', 'neuron', 'eval_neuron_value']].drop_duplicates().itertuples():\n",
        "        evalActivations[int(entry.layer)][int(entry.neuron)] = float(entry.eval_neuron_value)\n",
        "\n",
        "    for source, count in mostUsed:\n",
        "        sourceName = \"0:\" + str(source)\n",
        "        sourceEntries = evalEntries[evalEntries['source'] == sourceName]\n",
        "        for entry in sourceEntries[['layer', 'neuron', 'neuron_value', 'eval_neuron_value']].drop_duplicates().itertuples():\n",
        "            #print(entry)\n",
        "            # Access 'neuron_value' properly\n",
        "            blendedActivations[int(entry.layer), int(entry.neuron)] += float(entry.neuron_value) * (count / totalSources)\n",
        "\n",
        "    # Convert blendedActivations to dense array before similarity calculations\n",
        "    evaluationActivations = np.asarray(evalActivations.flatten().reshape(1, -1), dtype=np.float64)\n",
        "    blendedActivations_dense = blendedActivations.flatten().reshape(1, -1)\n",
        "\n",
        "    cosine_similarity, euclidean_distance, manhattan_distance, jaccard_similarity, hamming_distance, pearson_correlation = computeSimilarity(evaluationActivations, blendedActivations_dense)\n",
        "\n",
        "    kendall_tau, _ = kendalltau(evaluationActivations, blendedActivations_dense)\n",
        "    spearman_rho, _ = spearmanr(evaluationActivations, blendedActivations_dense)\n",
        "\n",
        "    # --- Print Results ---\n",
        "    print(f\"\\n--- Blended Activation Similarity Scores for {name}-Sample{evaluationToCheck} ---\")\n",
        "    print(f\"Kendall's Tau: {kendall_tau:.2f}\")\n",
        "    print(f\"Spearman's Rho: {spearman_rho:.2f}\")\n",
        "    print(f\"Cosine Similarity: {cosine_similarity:.4f}\")\n",
        "    print(f\"Euclidean Distance: {euclidean_distance:.4f}\")\n",
        "    print(f\"Manhattan Distance: {manhattan_distance:.4f}\")\n",
        "    print(f\"Jaccard Similarity: {jaccard_similarity:.4f}\" if jaccard_similarity is not None else \"Jaccard Similarity: N/A\")\n",
        "    print(f\"Hamming Distance: {hamming_distance:.4f}\")\n",
        "    print(f\"Pearson Correlation: {pearson_correlation:.4f}\" if pearson_correlation is not None else \"Pearson Correlation: N/A\")\n",
        "\n",
        "for evaluation in range(evaluationSamples):\n",
        "    blendActivations(evaluation, \"Evaluation\", evalDf)\n",
        "    blendActivations(evaluation, \"GeneratedEvaluation\", generatedEvalDf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-ByX9dFbBUC",
        "outputId": "7eb4fcfb-11da-4f96-927a-3536c8690b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample0 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.7910\n",
            "Euclidean Distance: 1479.7354\n",
            "Manhattan Distance: 165754.6559\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.7877\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample0 ---\n",
            "Kendall's Tau: 0.84\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9540\n",
            "Euclidean Distance: 725.0244\n",
            "Manhattan Distance: 153259.4231\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0439\n",
            "Pearson Correlation: 0.9529\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample1 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.7910\n",
            "Euclidean Distance: 1479.7354\n",
            "Manhattan Distance: 165754.6559\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.7877\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample1 ---\n",
            "Kendall's Tau: 0.84\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9546\n",
            "Euclidean Distance: 712.6566\n",
            "Manhattan Distance: 141406.3282\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0440\n",
            "Pearson Correlation: 0.9536\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample2 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.7910\n",
            "Euclidean Distance: 1479.7354\n",
            "Manhattan Distance: 165754.6559\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.7877\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample2 ---\n",
            "Kendall's Tau: 0.83\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9431\n",
            "Euclidean Distance: 813.2907\n",
            "Manhattan Distance: 155241.6484\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0439\n",
            "Pearson Correlation: 0.9418\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample3 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.7910\n",
            "Euclidean Distance: 1479.7354\n",
            "Manhattan Distance: 165754.6559\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.7877\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample3 ---\n",
            "Kendall's Tau: 0.85\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9567\n",
            "Euclidean Distance: 869.2505\n",
            "Manhattan Distance: 203203.7191\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0437\n",
            "Pearson Correlation: 0.9558\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample4 ---\n",
            "Kendall's Tau: 0.78\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.8160\n",
            "Euclidean Distance: 1390.2890\n",
            "Manhattan Distance: 140217.2572\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.8132\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample4 ---\n",
            "Kendall's Tau: 0.83\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9431\n",
            "Euclidean Distance: 813.2900\n",
            "Manhattan Distance: 155241.6921\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0440\n",
            "Pearson Correlation: 0.9418\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample5 ---\n",
            "Kendall's Tau: 0.79\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.8392\n",
            "Euclidean Distance: 1315.9538\n",
            "Manhattan Distance: 138323.4797\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.8368\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample5 ---\n",
            "Kendall's Tau: 0.83\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9485\n",
            "Euclidean Distance: 697.1876\n",
            "Manhattan Distance: 132810.5159\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0439\n",
            "Pearson Correlation: 0.9474\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample6 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.7910\n",
            "Euclidean Distance: 1479.7354\n",
            "Manhattan Distance: 165754.6559\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.7877\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample6 ---\n",
            "Kendall's Tau: 0.81\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9394\n",
            "Euclidean Distance: 704.0982\n",
            "Manhattan Distance: 120993.1613\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0441\n",
            "Pearson Correlation: 0.9380\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample7 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.7911\n",
            "Euclidean Distance: 1479.6356\n",
            "Manhattan Distance: 165797.3494\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.7878\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample7 ---\n",
            "Kendall's Tau: 0.80\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9247\n",
            "Euclidean Distance: 954.5330\n",
            "Manhattan Distance: 191906.5261\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0440\n",
            "Pearson Correlation: 0.9230\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample8 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.7910\n",
            "Euclidean Distance: 1479.7354\n",
            "Manhattan Distance: 165754.6559\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.7877\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample8 ---\n",
            "Kendall's Tau: 0.85\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9597\n",
            "Euclidean Distance: 635.6266\n",
            "Manhattan Distance: 116431.3177\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0439\n",
            "Pearson Correlation: 0.9588\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample9 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.7910\n",
            "Euclidean Distance: 1479.7354\n",
            "Manhattan Distance: 165754.6559\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.7877\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample9 ---\n",
            "Kendall's Tau: 0.84\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9569\n",
            "Euclidean Distance: 795.7583\n",
            "Manhattan Distance: 175291.0707\n",
            "Jaccard Similarity: N/A\n",
            "Hamming Distance: 0.0439\n",
            "Pearson Correlation: 0.9560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr, kendalltau, pearsonr\n",
        "\n",
        "def compute_cosine_similarity(image1, image2):\n",
        "    \"\"\"Compute cosine similarity between two images.\"\"\"\n",
        "    vec1 = image1.flatten().reshape(1, -1)\n",
        "    vec2 = image2.flatten().reshape(1, -1)\n",
        "    return cosine_similarity(vec1, vec2)[0][0]\n",
        "\n",
        "def computeSimilarity(sample, train_sample, normalized=False):\n",
        "    def normalize_vector(vec):\n",
        "        norm = np.linalg.norm(vec)  # Calculate the L2 norm (Euclidean norm) of the vector\n",
        "        if norm == 0:  # To prevent division by zero if the vector is all zeros\n",
        "            return vec\n",
        "        return vec / norm  # Divide the vector by its norm to normalize it\n",
        "\n",
        "    if (normalized):\n",
        "        # Normalize sample and train_sample\n",
        "        sample = normalize_vector(sample)\n",
        "        train_sample = normalize_vector(train_sample)\n",
        "\n",
        "    # Compute similarities\n",
        "    cosine_similarity = compute_cosine_similarity(sample, train_sample)\n",
        "    euclidean_distance = np.linalg.norm(sample - train_sample)  # Euclidean\n",
        "    manhattan_distance = np.sum(np.abs(sample - train_sample))  # Manhattan\n",
        "\n",
        "    shift = abs(min(np.min(sample), np.min(train_sample))) + 1\n",
        "    sample_shifted = sample + shift\n",
        "    train_sample_shifted = train_sample + shift\n",
        "    jaccard_similarity = (\n",
        "        np.sum(np.minimum(sample_shifted, train_sample_shifted)) /\n",
        "        np.sum(np.maximum(sample_shifted, train_sample_shifted))\n",
        "        if np.sum(np.maximum(sample_shifted, train_sample_shifted)) > 0 else None\n",
        "    )\n",
        "    hamming_distance = np.mean(sample != train_sample)  # Hamming\n",
        "    try:\n",
        "        pearson_correlation, _ = pearsonr(sample.flatten(), train_sample.flatten())  # Pearson\n",
        "    except ValueError:\n",
        "        pearson_correlation = None\n",
        "\n",
        "    return cosine_similarity, euclidean_distance, manhattan_distance, jaccard_similarity, hamming_distance, pearson_correlation\n",
        "\n",
        "def blendActivations(evaluationToCheck, name, closestSources, eval_df, debug=True, difference=False):\n",
        "    _, mostUsed = getMostUsedFromDataFrame(evalDf, evaluationToCheck, closestSources, info=False)\n",
        "    totalSources = sum(count for _, count in mostUsed)\n",
        "\n",
        "    min_count = min([count for _, count in mostUsed])  # Find the maximum count value\n",
        "\n",
        "    if(difference):\n",
        "        # Adjust each count by subtracting the minimum count\n",
        "        for idx, (source, count) in enumerate(mostUsed):\n",
        "            mostUsed[idx] = (source, count - min_count)\n",
        "\n",
        "        totalSources = sum(count for _, count in mostUsed)\n",
        "\n",
        "    # Slice to get the first two dimensions from hidden_sizes\n",
        "    neurons_per_layer = np.array([layer[1] for layer in hidden_sizes])\n",
        "\n",
        "    # Create a single zero-initialized array of shape (38, max_neurons)\n",
        "    relevantParts = np.zeros((len(neurons_per_layer), int(neurons_per_layer.max())))\n",
        "    blendedActivations = np.zeros_like(relevantParts)\n",
        "\n",
        "    #print(relevantParts.shape)\n",
        "    evalActivations = np.zeros_like(relevantParts)\n",
        "    evalEntries = eval_df[eval_df.index == evaluationToCheck]\n",
        "    for entry in evalEntries[['layer', 'neuron', 'eval_neuron_value']].drop_duplicates().itertuples():\n",
        "        evalActivations[int(entry.layer)][int(entry.neuron)] = float(entry.eval_neuron_value)\n",
        "\n",
        "    for source, count in mostUsed:\n",
        "        sourceName = \"0:\" + str(source)\n",
        "        sourceEntries = evalEntries[evalEntries['source'] == sourceName]\n",
        "        for entry in sourceEntries[['layer', 'neuron', 'neuron_value', 'eval_neuron_value']].drop_duplicates().itertuples():\n",
        "            #print(entry)\n",
        "            # Access 'neuron_value' properly\n",
        "            blendedActivations[int(entry.layer), int(entry.neuron)] += float(entry.neuron_value) * (count / totalSources)\n",
        "\n",
        "    # Convert blendedActivations to dense array before similarity calculations\n",
        "    evaluationActivations = np.asarray(evalActivations.flatten().reshape(1, -1), dtype=np.float64)\n",
        "    blendedActivations_dense = blendedActivations.flatten().reshape(1, -1)\n",
        "\n",
        "    cosine_similarity, euclidean_distance, manhattan_distance, jaccard_similarity, hamming_distance, pearson_correlation = computeSimilarity(evaluationActivations, blendedActivations_dense)\n",
        "\n",
        "    kendall_tau, _ = kendalltau(evaluationActivations, blendedActivations_dense)\n",
        "    spearman_rho, _ = spearmanr(evaluationActivations, blendedActivations_dense)\n",
        "\n",
        "    if(debug):\n",
        "        # --- Print Results ---\n",
        "        print(f\"\\n--- Blended Activation Similarity Scores for {name}-Sample{evaluationToCheck} ---\")\n",
        "        print(f\"Kendall's Tau: {kendall_tau:.2f}\")\n",
        "        print(f\"Spearman's Rho: {spearman_rho:.2f}\")\n",
        "        print(f\"Cosine Similarity: {cosine_similarity:.4f}\")\n",
        "        print(f\"Euclidean Distance: {euclidean_distance:.4f}\")\n",
        "        print(f\"Manhattan Distance: {manhattan_distance:.4f}\")\n",
        "        print(f\"Jaccard Similarity: {jaccard_similarity:.4f}\" if jaccard_similarity is not None else \"Jaccard Similarity: N/A\")\n",
        "        print(f\"Hamming Distance: {hamming_distance:.4f}\")\n",
        "        print(f\"Pearson Correlation: {pearson_correlation:.4f}\" if pearson_correlation is not None else \"Pearson Correlation: N/A\")\n",
        "\n",
        "    # Store best layers\n",
        "    layer_metrics = {\n",
        "        \"cosine_similarity\": cosine_similarity,\n",
        "        \"kendall_tau\": kendall_tau,\n",
        "        \"pearson_correlation\": pearson_correlation,\n",
        "        \"jaccard_similarity\": jaccard_similarity,\n",
        "        \"euclidean_distance\": euclidean_distance,\n",
        "        \"manhattan_distance\": manhattan_distance,\n",
        "        \"hamming_distance\": hamming_distance,\n",
        "    }\n",
        "\n",
        "    return blendedActivations, evalActivations, layer_metrics\n",
        "\n",
        "for evaluation in range(evaluationSamples):\n",
        "    blendActivations(evaluation, \"Evaluation\", 5, evalDf)\n",
        "    blendActivations(evaluation, \"GeneratedEvaluation\", 5, generatedEvalDf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1f921FK1x39",
        "outputId": "f200d97b-d3bc-4b56-91d2-e48cec3b8c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample0 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.7910\n",
            "Euclidean Distance: 1479.7354\n",
            "Manhattan Distance: 165754.6559\n",
            "Jaccard Similarity: 0.9987\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.7877\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample0 ---\n",
            "Kendall's Tau: 0.84\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9495\n",
            "Euclidean Distance: 817.8663\n",
            "Manhattan Distance: 186968.1171\n",
            "Jaccard Similarity: 0.9974\n",
            "Hamming Distance: 0.0439\n",
            "Pearson Correlation: 0.9484\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample1 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.7910\n",
            "Euclidean Distance: 1479.7354\n",
            "Manhattan Distance: 165754.6559\n",
            "Jaccard Similarity: 0.9987\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.7877\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample1 ---\n",
            "Kendall's Tau: 0.82\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9420\n",
            "Euclidean Distance: 844.3086\n",
            "Manhattan Distance: 181553.3983\n",
            "Jaccard Similarity: 0.9974\n",
            "Hamming Distance: 0.0440\n",
            "Pearson Correlation: 0.9407\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample2 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.7910\n",
            "Euclidean Distance: 1479.7354\n",
            "Manhattan Distance: 165754.6559\n",
            "Jaccard Similarity: 0.9987\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.7877\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample2 ---\n",
            "Kendall's Tau: 0.81\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9282\n",
            "Euclidean Distance: 946.6248\n",
            "Manhattan Distance: 196537.5650\n",
            "Jaccard Similarity: 0.9975\n",
            "Hamming Distance: 0.0439\n",
            "Pearson Correlation: 0.9266\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample3 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.7910\n",
            "Euclidean Distance: 1479.7354\n",
            "Manhattan Distance: 165754.6559\n",
            "Jaccard Similarity: 0.9987\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.7877\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample3 ---\n",
            "Kendall's Tau: 0.82\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9427\n",
            "Euclidean Distance: 1032.1448\n",
            "Manhattan Distance: 246842.7378\n",
            "Jaccard Similarity: 0.9965\n",
            "Hamming Distance: 0.0437\n",
            "Pearson Correlation: 0.9414\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample4 ---\n",
            "Kendall's Tau: 0.78\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.8160\n",
            "Euclidean Distance: 1390.2890\n",
            "Manhattan Distance: 140217.2572\n",
            "Jaccard Similarity: 0.9989\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.8132\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample4 ---\n",
            "Kendall's Tau: 0.82\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9324\n",
            "Euclidean Distance: 877.9461\n",
            "Manhattan Distance: 172353.5758\n",
            "Jaccard Similarity: 0.9978\n",
            "Hamming Distance: 0.0440\n",
            "Pearson Correlation: 0.9309\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample5 ---\n",
            "Kendall's Tau: 0.79\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.8392\n",
            "Euclidean Distance: 1315.9538\n",
            "Manhattan Distance: 138323.4797\n",
            "Jaccard Similarity: 0.9989\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.8368\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample5 ---\n",
            "Kendall's Tau: 0.82\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9205\n",
            "Euclidean Distance: 842.0614\n",
            "Manhattan Distance: 153619.9407\n",
            "Jaccard Similarity: 0.9982\n",
            "Hamming Distance: 0.0439\n",
            "Pearson Correlation: 0.9188\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample6 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.7910\n",
            "Euclidean Distance: 1479.7354\n",
            "Manhattan Distance: 165754.6559\n",
            "Jaccard Similarity: 0.9987\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.7877\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample6 ---\n",
            "Kendall's Tau: 0.81\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9380\n",
            "Euclidean Distance: 725.4300\n",
            "Manhattan Distance: 137826.7037\n",
            "Jaccard Similarity: 0.9980\n",
            "Hamming Distance: 0.0441\n",
            "Pearson Correlation: 0.9366\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample7 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.7911\n",
            "Euclidean Distance: 1479.6356\n",
            "Manhattan Distance: 165797.3494\n",
            "Jaccard Similarity: 0.9987\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.7878\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample7 ---\n",
            "Kendall's Tau: 0.81\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9267\n",
            "Euclidean Distance: 953.6388\n",
            "Manhattan Distance: 197157.4824\n",
            "Jaccard Similarity: 0.9975\n",
            "Hamming Distance: 0.0440\n",
            "Pearson Correlation: 0.9251\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample8 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.7910\n",
            "Euclidean Distance: 1479.7354\n",
            "Manhattan Distance: 165754.6559\n",
            "Jaccard Similarity: 0.9987\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.7877\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample8 ---\n",
            "Kendall's Tau: 0.83\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9471\n",
            "Euclidean Distance: 763.8033\n",
            "Manhattan Distance: 158461.4190\n",
            "Jaccard Similarity: 0.9978\n",
            "Hamming Distance: 0.0439\n",
            "Pearson Correlation: 0.9459\n",
            "\n",
            "--- Blended Activation Similarity Scores for Evaluation-Sample9 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.7910\n",
            "Euclidean Distance: 1479.7354\n",
            "Manhattan Distance: 165754.6559\n",
            "Jaccard Similarity: 0.9987\n",
            "Hamming Distance: 0.0443\n",
            "Pearson Correlation: 0.7877\n",
            "\n",
            "--- Blended Activation Similarity Scores for GeneratedEvaluation-Sample9 ---\n",
            "Kendall's Tau: 0.84\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9531\n",
            "Euclidean Distance: 871.1615\n",
            "Manhattan Distance: 197857.6535\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0439\n",
            "Pearson Correlation: 0.9521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def findBestLayerByBlendedActivations(evaluationToCheck, name, closestSources, eval_df):\n",
        "    blendedActivations, evalActivations, layer_metrics = blendActivations(evaluationToCheck, name, closestSources, eval_df, debug=False)\n",
        "\n",
        "    # --- Compute Metrics ---\n",
        "    best_layers = {metric: {\"value\": None, \"layer\": None} for metric in [\n",
        "        \"cosine_similarity\", \"kendall_tau\", \"pearson_correlation\", \"jaccard_similarity\",\n",
        "        \"euclidean_distance\", \"manhattan_distance\", \"hamming_distance\"\n",
        "    ]}\n",
        "\n",
        "    for layer_idx in range(len(blendedActivations)):\n",
        "        for metric, optimize_fn in {\n",
        "            \"cosine_similarity\": max,\n",
        "            \"kendall_tau\": max,\n",
        "            \"pearson_correlation\": max,\n",
        "            \"jaccard_similarity\": max,\n",
        "            \"euclidean_distance\": min,\n",
        "            \"manhattan_distance\": min,\n",
        "            \"hamming_distance\": min,\n",
        "        }.items():\n",
        "            if best_layers[metric][\"value\"] is None or optimize_fn(layer_metrics[metric], best_layers[metric][\"value\"]) == layer_metrics[metric]:\n",
        "                best_layers[metric][\"value\"] = layer_metrics[metric]\n",
        "                best_layers[metric][\"layer\"] = layer_idx\n",
        "\n",
        "    # Return the blended and evaluation activations for further analysis\n",
        "    return {\n",
        "        \"blended_activations\": blendedActivations,\n",
        "        \"evaluation_activations\": evalActivations,\n",
        "        \"best_layers\": best_layers\n",
        "    }\n",
        "\n",
        "def aggregateLayerMetricsByAllMeans(name, closestSources, eval_df):\n",
        "    # Metrics to compute and their optimization direction\n",
        "    metrics_to_optimize = {\n",
        "        \"cosine_similarity\": max,\n",
        "        \"kendall_tau\": max,\n",
        "        \"pearson_correlation\": max,\n",
        "        \"jaccard_similarity\": max,\n",
        "        \"euclidean_distance\": min,\n",
        "        \"manhattan_distance\": min,\n",
        "        \"hamming_distance\": min,\n",
        "    }\n",
        "\n",
        "    # Store metrics for all layers\n",
        "    layer_metrics_aggregated = {layer: {metric: [] for metric in metrics_to_optimize} for layer in range(len(hidden_sizes))}\n",
        "\n",
        "    for evaluation in range(evaluationSamples):\n",
        "        # Run analysis for the current evaluation sample\n",
        "        blendedActivations = findBestLayerByBlendedActivations(\n",
        "            evaluation, name, closestSources, eval_df\n",
        "        )\n",
        "\n",
        "        # Aggregate metrics layer-wise\n",
        "        for layer_idx in range(len(hidden_sizes)):\n",
        "            evaluation_layer = blendedActivations[\"evaluation_activations\"][layer_idx].flatten()\n",
        "            blended_layer = blendedActivations[\"blended_activations\"][layer_idx].flatten()\n",
        "\n",
        "            cosine_similarity, euclidean_distance, manhattan_distance, jaccard_similarity, hamming_distance, pearson_correlation = computeSimilarity(\n",
        "                evaluation_layer.reshape(1, -1), blended_layer.reshape(1, -1)\n",
        "            )\n",
        "\n",
        "            kendall_tau, _ = kendalltau(evaluation_layer, blended_layer)\n",
        "            spearman_rho, _ = spearmanr(evaluation_layer, blended_layer)\n",
        "\n",
        "            # Add layer metrics to aggregated results\n",
        "            layer_metrics_aggregated[layer_idx][\"cosine_similarity\"].append(cosine_similarity)\n",
        "            layer_metrics_aggregated[layer_idx][\"kendall_tau\"].append(kendall_tau)\n",
        "            layer_metrics_aggregated[layer_idx][\"pearson_correlation\"].append(pearson_correlation)\n",
        "            layer_metrics_aggregated[layer_idx][\"jaccard_similarity\"].append(jaccard_similarity)\n",
        "            layer_metrics_aggregated[layer_idx][\"euclidean_distance\"].append(euclidean_distance)\n",
        "            layer_metrics_aggregated[layer_idx][\"manhattan_distance\"].append(manhattan_distance)\n",
        "            layer_metrics_aggregated[layer_idx][\"hamming_distance\"].append(hamming_distance)\n",
        "\n",
        "    # Use np.mean to calculate the mean\n",
        "    sorted_layers = sorted(\n",
        "        layer_metrics_aggregated.items(),\n",
        "        key=lambda x: (\n",
        "            -np.mean(x[1].get(\"pearson_correlation\", [0])),  # Mean of the list, default to 0 if empty\n",
        "            -np.mean(x[1].get(\"cosine_similarity\", [0])),    # Mean of the list, default to 0 if empty\n",
        "            -np.mean(x[1].get(\"kendall_tau\", [0])),          # Mean of the list, default to 0 if empty\n",
        "            -np.mean(x[1].get(\"jaccard_similarity\", [0])),    # Mean of the list, default to 0 if empty\n",
        "            np.mean(x[1].get(\"manhattan_distance\", [0])),    # Mean for ascending order\n",
        "            np.mean(x[1].get(\"euclidean_distance\", [0])),    # Mean for ascending order\n",
        "            np.mean(x[1].get(\"hamming_distance\", [0])),      # Mean for ascending order\n",
        "        ),\n",
        "        reverse=False  # Use the above tuple ordering directly\n",
        "    )\n",
        "\n",
        "    mostRelevantLayers = []\n",
        "    # Print sorted relevance scores\n",
        "    print(f\"\\n--- Sorted Layers by Custom Order of Metrics for {name} ---\")\n",
        "    for layer, metrics in sorted_layers:\n",
        "        if(np.mean(metrics[\"pearson_correlation\"]) >= 0.85 and np.mean(metrics[\"cosine_similarity\"]) >= 0.85):\n",
        "            mostRelevantLayers.append(layer)\n",
        "        print(\n",
        "            f\"Layer {layer}, Type={hidden_sizes[layer][0]}\\n\" +\n",
        "            \"\\n\".join(\n",
        "                f\"    {metric.capitalize()}: {np.mean(metrics.get(metric, 0))}\"\n",
        "                for metric in [\"pearson_correlation\", \"cosine_similarity\", \"jaccard_similarity\",\"kendall_tau\", \"manhattan_distance\", \"euclidean_distance\", \"hamming_distance\"]\n",
        "                if metric in metrics\n",
        "            )\n",
        "        )\n",
        "\n",
        "    parameters = 0\n",
        "    print(\"Most Relevant Layers: \")\n",
        "    for layer in mostRelevantLayers:\n",
        "        if(hidden_sizes[layer][0] != \"Sequential\"):\n",
        "            # Find the metrics for the current layer in sorted_layers\n",
        "            _, metrics = next((l, m) for l, m in sorted_layers if l == layer)\n",
        "            metricsSummary = \", \".join(f\"{metric.capitalize()}: {np.mean(metrics.get(metric, 0))}\" for metric in [\"pearson_correlation\", \"cosine_similarity\", \"jaccard_similarity\", \"kendall_tau\", \"manhattan_distance\", \"euclidean_distance\", \"hamming_distance\"] if metric in metrics)\n",
        "            print(f\"Layer {layer}, Type={hidden_sizes[layer][0]}, Size={hidden_sizes[layer][2]}\", metricsSummary)\n",
        "            parameters += hidden_sizes[layer][2]\n",
        "    print(\"Total Parameters: \", parameters, \"/\", np.sum(np.array([layer[1] for layer in hidden_sizes])))\n",
        "\n",
        "#    return summarized_metrics, sorted_layers\n",
        "\n",
        "test = aggregateLayerMetricsByAllMeans(\"Evaluation\", 5, evalDf)\n",
        "test = aggregateLayerMetricsByAllMeans(\"GeneratedEvaluation\", 5, generatedEvalDf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjioMyNO2OwC",
        "outputId": "d2143332-0c80-4c64-9f77-3a3d157f2452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sorted Layers by Custom Order of Metrics for Evaluation ---\n",
            "Layer 0, Type=Embedding\n",
            "    Pearson_correlation: 0.06673792982433127\n",
            "    Cosine_similarity: 0.06669346209916778\n",
            "    Jaccard_similarity: 0.9968832419127169\n",
            "    Kendall_tau: 0.016061861012238367\n",
            "    Manhattan_distance: 526.5391238363135\n",
            "    Euclidean_distance: 23.88650723877866\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 1, Type=Embedding\n",
            "    Pearson_correlation: nan\n",
            "    Cosine_similarity: 0.0\n",
            "    Jaccard_similarity: 1.0\n",
            "    Kendall_tau: nan\n",
            "    Manhattan_distance: 0.0\n",
            "    Euclidean_distance: 0.0\n",
            "    Hamming_distance: 0.0\n",
            "Layer 30, Type=Linear\n",
            "    Pearson_correlation: 0.9835901479209754\n",
            "    Cosine_similarity: 0.9835901096558164\n",
            "    Jaccard_similarity: 0.997205970326912\n",
            "    Kendall_tau: 0.883199744561094\n",
            "    Manhattan_distance: 1792.330246389165\n",
            "    Euclidean_distance: 80.01409019319888\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 31, Type=Sequential\n",
            "    Pearson_correlation: 0.9835901479209754\n",
            "    Cosine_similarity: 0.9835901096558164\n",
            "    Jaccard_similarity: 0.997205970326912\n",
            "    Kendall_tau: 0.883199744561094\n",
            "    Manhattan_distance: 1792.330246389165\n",
            "    Euclidean_distance: 80.01409019319888\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 32, Type=FeedForward\n",
            "    Pearson_correlation: 0.9835901479209754\n",
            "    Cosine_similarity: 0.9835901096558164\n",
            "    Jaccard_similarity: 0.997205970326912\n",
            "    Kendall_tau: 0.883199744561094\n",
            "    Manhattan_distance: 1792.330246389165\n",
            "    Euclidean_distance: 80.01409019319888\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 21, Type=Linear\n",
            "    Pearson_correlation: 0.9610619158787157\n",
            "    Cosine_similarity: 0.9610637546734369\n",
            "    Jaccard_similarity: 0.9979027344421937\n",
            "    Kendall_tau: 0.9005916174410251\n",
            "    Manhattan_distance: 1673.908955998369\n",
            "    Euclidean_distance: 72.26675963015376\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 4, Type=Linear\n",
            "    Pearson_correlation: 0.9457544290274947\n",
            "    Cosine_similarity: 0.9457521160120017\n",
            "    Jaccard_similarity: 0.9988401383164739\n",
            "    Kendall_tau: 0.7661395433318491\n",
            "    Manhattan_distance: 916.688691512144\n",
            "    Euclidean_distance: 42.733784537456\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 20, Type=Linear\n",
            "    Pearson_correlation: 0.921715733336715\n",
            "    Cosine_similarity: 0.9217181044900571\n",
            "    Jaccard_similarity: 0.995662750033954\n",
            "    Kendall_tau: 0.8856036026422368\n",
            "    Manhattan_distance: 3808.5027054741804\n",
            "    Euclidean_distance: 162.16809005086276\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 28, Type=Linear\n",
            "    Pearson_correlation: 0.9057695980876262\n",
            "    Cosine_similarity: 0.9101175077257769\n",
            "    Jaccard_similarity: 0.9957712368250954\n",
            "    Kendall_tau: 0.9182932968756397\n",
            "    Manhattan_distance: 3099.716568843337\n",
            "    Euclidean_distance: 78.51037989515014\n",
            "    Hamming_distance: 0.061105915593847614\n",
            "Layer 37, Type=Linear\n",
            "    Pearson_correlation: 0.8552026280506331\n",
            "    Cosine_similarity: 0.9993136843103677\n",
            "    Jaccard_similarity: 0.8145377300431796\n",
            "    Kendall_tau: 0.5867290298350193\n",
            "    Manhattan_distance: 27059.314188172924\n",
            "    Euclidean_distance: 133.75408694595117\n",
            "    Hamming_distance: 0.9999801022743101\n",
            "Layer 5, Type=Linear\n",
            "    Pearson_correlation: 0.793677971945033\n",
            "    Cosine_similarity: 0.7936802654371329\n",
            "    Jaccard_similarity: 0.9978416078523253\n",
            "    Kendall_tau: 0.623281513139855\n",
            "    Manhattan_distance: 409.32572017450593\n",
            "    Euclidean_distance: 18.503930315641007\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 14, Type=Linear\n",
            "    Pearson_correlation: 0.7250482254502328\n",
            "    Cosine_similarity: 0.7250480897552505\n",
            "    Jaccard_similarity: 0.9970552022502936\n",
            "    Kendall_tau: 0.5727638736942028\n",
            "    Manhattan_distance: 8272.247841518765\n",
            "    Euclidean_distance: 375.09516803334355\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 15, Type=Sequential\n",
            "    Pearson_correlation: 0.7250482254502328\n",
            "    Cosine_similarity: 0.7250480897552505\n",
            "    Jaccard_similarity: 0.9970552022502936\n",
            "    Kendall_tau: 0.5727638736942028\n",
            "    Manhattan_distance: 8272.247841518765\n",
            "    Euclidean_distance: 375.09516803334355\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 16, Type=FeedForward\n",
            "    Pearson_correlation: 0.7250482254502328\n",
            "    Cosine_similarity: 0.7250480897552505\n",
            "    Jaccard_similarity: 0.9970552022502936\n",
            "    Kendall_tau: 0.5727638736942028\n",
            "    Manhattan_distance: 8272.247841518765\n",
            "    Euclidean_distance: 375.09516803334355\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 12, Type=Linear\n",
            "    Pearson_correlation: 0.6542074295381655\n",
            "    Cosine_similarity: 0.6619112770672757\n",
            "    Jaccard_similarity: 0.9882848762794738\n",
            "    Kendall_tau: 0.5501385927439065\n",
            "    Manhattan_distance: 6602.424721128553\n",
            "    Euclidean_distance: 153.61766061653708\n",
            "    Hamming_distance: 0.061105915593847614\n",
            "Layer 7, Type=Dropout\n",
            "    Pearson_correlation: 0.5999632964443651\n",
            "    Cosine_similarity: 0.5999744855980536\n",
            "    Jaccard_similarity: 0.9999373216265728\n",
            "    Kendall_tau: 0.6435184693461732\n",
            "    Manhattan_distance: 3.150235941258803\n",
            "    Euclidean_distance: 1.8239036976242748\n",
            "    Hamming_distance: 5.9693177069860916e-05\n",
            "Layer 19, Type=LayerNorm\n",
            "    Pearson_correlation: 0.5827293328104896\n",
            "    Cosine_similarity: 0.5827299037401221\n",
            "    Jaccard_similarity: 0.9976264491319441\n",
            "    Kendall_tau: 0.4131068716364861\n",
            "    Manhattan_distance: 362.1287197514243\n",
            "    Euclidean_distance: 16.465068894029425\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 6, Type=Linear\n",
            "    Pearson_correlation: 0.5645769252299044\n",
            "    Cosine_similarity: 0.5645765156470854\n",
            "    Jaccard_similarity: 0.9962998145261759\n",
            "    Kendall_tau: 0.42742617356298823\n",
            "    Manhattan_distance: 1223.702434772234\n",
            "    Euclidean_distance: 55.23238908225856\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 3, Type=LayerNorm\n",
            "    Pearson_correlation: 0.5641155528579342\n",
            "    Cosine_similarity: 0.564116146291603\n",
            "    Jaccard_similarity: 0.9977220363318693\n",
            "    Kendall_tau: 0.41079458382309325\n",
            "    Manhattan_distance: 353.93511848290643\n",
            "    Euclidean_distance: 16.12674736921284\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 27, Type=LayerNorm\n",
            "    Pearson_correlation: 0.5354678803949369\n",
            "    Cosine_similarity: 0.5354677929606684\n",
            "    Jaccard_similarity: 0.9977016442945572\n",
            "    Kendall_tau: 0.35825864972126975\n",
            "    Manhattan_distance: 380.4332222719767\n",
            "    Euclidean_distance: 17.10740273432844\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 18, Type=TransformerBlock\n",
            "    Pearson_correlation: 0.5352853932284913\n",
            "    Cosine_similarity: 0.535282188858487\n",
            "    Jaccard_similarity: 0.9965003234297587\n",
            "    Kendall_tau: 0.4018259364444098\n",
            "    Manhattan_distance: 12017.165307085417\n",
            "    Euclidean_distance: 557.8373417804157\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 34, Type=TransformerBlock\n",
            "    Pearson_correlation: 0.4922887554139339\n",
            "    Cosine_similarity: 0.4922840350925107\n",
            "    Jaccard_similarity: 0.9961817401533752\n",
            "    Kendall_tau: 0.3448732766886301\n",
            "    Manhattan_distance: 12681.698348627124\n",
            "    Euclidean_distance: 577.2862457471481\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 35, Type=Sequential\n",
            "    Pearson_correlation: 0.4922887554139339\n",
            "    Cosine_similarity: 0.4922840350925107\n",
            "    Jaccard_similarity: 0.9961817401533752\n",
            "    Kendall_tau: 0.3448732766886301\n",
            "    Manhattan_distance: 12681.698348627124\n",
            "    Euclidean_distance: 577.2862457471481\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 11, Type=LayerNorm\n",
            "    Pearson_correlation: 0.451764621389452\n",
            "    Cosine_similarity: 0.4517651561456829\n",
            "    Jaccard_similarity: 0.9975864886227106\n",
            "    Kendall_tau: 0.2988142214994013\n",
            "    Manhattan_distance: 432.7677198628754\n",
            "    Euclidean_distance: 19.905065744477888\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 36, Type=LayerNorm\n",
            "    Pearson_correlation: 0.4317223886047456\n",
            "    Cosine_similarity: 0.4317221316088653\n",
            "    Jaccard_similarity: 0.9976172468745199\n",
            "    Kendall_tau: 0.28699216334987554\n",
            "    Manhattan_distance: 477.77486192481354\n",
            "    Euclidean_distance: 21.613637522028135\n",
            "    Hamming_distance: 0.015281453329884395\n",
            "Layer 8, Type=Linear\n",
            "    Pearson_correlation: 0.37622630794353756\n",
            "    Cosine_similarity: 0.3762283392146795\n",
            "    Jaccard_similarity: 0.9961768114025709\n",
            "    Kendall_tau: 0.26920825310892016\n",
            "    Manhattan_distance: 4565.657581761356\n",
            "    Euclidean_distance: 207.77717632525037\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 9, Type=MultiHeadAttention\n",
            "    Pearson_correlation: 0.37622630794353756\n",
            "    Cosine_similarity: 0.3762283392146795\n",
            "    Jaccard_similarity: 0.9961768114025709\n",
            "    Kendall_tau: 0.26920825310892016\n",
            "    Manhattan_distance: 4565.657581761356\n",
            "    Euclidean_distance: 207.77717632525037\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 13, Type=GELU\n",
            "    Pearson_correlation: 0.17169124921283535\n",
            "    Cosine_similarity: 0.17949046921211406\n",
            "    Jaccard_similarity: 0.9632188429684951\n",
            "    Kendall_tau: -0.17428355356037736\n",
            "    Manhattan_distance: 2248.4507241004844\n",
            "    Euclidean_distance: 82.38137823495148\n",
            "    Hamming_distance: 0.052709075352687186\n",
            "Layer 17, Type=Dropout\n",
            "    Pearson_correlation: 0.15965465752865488\n",
            "    Cosine_similarity: 0.15965360320564886\n",
            "    Jaccard_similarity: 0.9962330917850469\n",
            "    Kendall_tau: 0.1442558002326825\n",
            "    Manhattan_distance: 11741.604542735842\n",
            "    Euclidean_distance: 566.4275242349588\n",
            "    Hamming_distance: 0.013828919354517777\n",
            "Layer 33, Type=Dropout\n",
            "    Pearson_correlation: 0.04710764114565541\n",
            "    Cosine_similarity: 0.04711001593753596\n",
            "    Jaccard_similarity: 0.9961804580952525\n",
            "    Kendall_tau: -0.011623651764801268\n",
            "    Manhattan_distance: 2702.430875089106\n",
            "    Euclidean_distance: 128.99984923759743\n",
            "    Hamming_distance: 0.013411067115028753\n",
            "Layer 22, Type=Linear\n",
            "    Pearson_correlation: 0.031363702644779744\n",
            "    Cosine_similarity: 0.03131820221212093\n",
            "    Jaccard_similarity: 0.9969429123422586\n",
            "    Kendall_tau: 0.01907360994187296\n",
            "    Manhattan_distance: 901.9363632423492\n",
            "    Euclidean_distance: 40.87976204696181\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 2, Type=Dropout\n",
            "    Pearson_correlation: 0.017465319322079266\n",
            "    Cosine_similarity: 0.017426326524403866\n",
            "    Jaccard_similarity: 0.9966941183784641\n",
            "    Kendall_tau: -0.005451515796085344\n",
            "    Manhattan_distance: 704.6990485240995\n",
            "    Euclidean_distance: 34.27207157200968\n",
            "    Hamming_distance: 0.013530453469168474\n",
            "Layer 23, Type=Dropout\n",
            "    Pearson_correlation: 0.0007366420651610325\n",
            "    Cosine_similarity: 0.0007717081928285989\n",
            "    Jaccard_similarity: 0.9999389920319087\n",
            "    Kendall_tau: 0.9865622580986002\n",
            "    Manhattan_distance: 3.06626472105989\n",
            "    Euclidean_distance: 1.7122119886995548\n",
            "    Hamming_distance: 7.959090275981455e-05\n",
            "Layer 29, Type=GELU\n",
            "    Pearson_correlation: -0.005059135983690022\n",
            "    Cosine_similarity: -0.005034906715245627\n",
            "    Jaccard_similarity: 0.9940513679993686\n",
            "    Kendall_tau: 0.7513760784081\n",
            "    Manhattan_distance: 351.1347272539015\n",
            "    Euclidean_distance: 29.541335136551062\n",
            "    Hamming_distance: 0.056907495473267414\n",
            "Layer 26, Type=Dropout\n",
            "    Pearson_correlation: -0.01824079971808692\n",
            "    Cosine_similarity: -0.01824620504647891\n",
            "    Jaccard_similarity: 0.9961467259601597\n",
            "    Kendall_tau: 0.00620042245561752\n",
            "    Manhattan_distance: 4029.624507014344\n",
            "    Euclidean_distance: 191.21587483449704\n",
            "    Hamming_distance: 0.013769226177447918\n",
            "Layer 10, Type=Dropout\n",
            "    Pearson_correlation: -0.02354101715610114\n",
            "    Cosine_similarity: -0.02354011998912737\n",
            "    Jaccard_similarity: 0.9959204935766157\n",
            "    Kendall_tau: -0.014735809641528777\n",
            "    Manhattan_distance: 5224.792030635297\n",
            "    Euclidean_distance: 249.0934683780034\n",
            "    Hamming_distance: 0.013791113675706868\n",
            "Layer 24, Type=Linear\n",
            "    Pearson_correlation: -0.0993520402810319\n",
            "    Cosine_similarity: -0.0993567378954161\n",
            "    Jaccard_similarity: 0.9955407682147331\n",
            "    Kendall_tau: -0.03556748352026664\n",
            "    Manhattan_distance: 4259.202130621818\n",
            "    Euclidean_distance: 193.0005484965919\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 25, Type=MultiHeadAttention\n",
            "    Pearson_correlation: -0.0993520402810319\n",
            "    Cosine_similarity: -0.0993567378954161\n",
            "    Jaccard_similarity: 0.9955407682147331\n",
            "    Kendall_tau: -0.03556748352026664\n",
            "    Manhattan_distance: 4259.202130621818\n",
            "    Euclidean_distance: 193.0005484965919\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Most Relevant Layers: \n",
            "Layer 30, Type=Linear, Size=768 Pearson_correlation: 0.9835901479209754, Cosine_similarity: 0.9835901096558164, Jaccard_similarity: 0.997205970326912, Kendall_tau: 0.883199744561094, Manhattan_distance: 1792.330246389165, Euclidean_distance: 80.01409019319888, Hamming_distance: 0.015261555604194444\n",
            "Layer 32, Type=FeedForward, Size=3072 Pearson_correlation: 0.9835901479209754, Cosine_similarity: 0.9835901096558164, Jaccard_similarity: 0.997205970326912, Kendall_tau: 0.883199744561094, Manhattan_distance: 1792.330246389165, Euclidean_distance: 80.01409019319888, Hamming_distance: 0.015261555604194444\n",
            "Layer 21, Type=Linear, Size=768 Pearson_correlation: 0.9610619158787157, Cosine_similarity: 0.9610637546734369, Jaccard_similarity: 0.9979027344421937, Kendall_tau: 0.9005916174410251, Manhattan_distance: 1673.908955998369, Euclidean_distance: 72.26675963015376, Hamming_distance: 0.015261555604194444\n",
            "Layer 4, Type=Linear, Size=768 Pearson_correlation: 0.9457544290274947, Cosine_similarity: 0.9457521160120017, Jaccard_similarity: 0.9988401383164739, Kendall_tau: 0.7661395433318491, Manhattan_distance: 916.688691512144, Euclidean_distance: 42.733784537456, Hamming_distance: 0.015261555604194444\n",
            "Layer 20, Type=Linear, Size=768 Pearson_correlation: 0.921715733336715, Cosine_similarity: 0.9217181044900571, Jaccard_similarity: 0.995662750033954, Kendall_tau: 0.8856036026422368, Manhattan_distance: 3808.5027054741804, Euclidean_distance: 162.16809005086276, Hamming_distance: 0.015261555604194444\n",
            "Layer 28, Type=Linear, Size=3072 Pearson_correlation: 0.9057695980876262, Cosine_similarity: 0.9101175077257769, Jaccard_similarity: 0.9957712368250954, Kendall_tau: 0.9182932968756397, Manhattan_distance: 3099.716568843337, Euclidean_distance: 78.51037989515014, Hamming_distance: 0.061105915593847614\n",
            "Layer 37, Type=Linear, Size=768 Pearson_correlation: 0.8552026280506331, Cosine_similarity: 0.9993136843103677, Jaccard_similarity: 0.8145377300431796, Kendall_tau: 0.5867290298350193, Manhattan_distance: 27059.314188172924, Euclidean_distance: 133.75408694595117, Hamming_distance: 0.9999801022743101\n",
            "Total Parameters:  9984 / 87889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sorted Layers by Custom Order of Metrics for GeneratedEvaluation ---\n",
            "Layer 0, Type=Embedding\n",
            "    Pearson_correlation: -0.005834112629211812\n",
            "    Cosine_similarity: -0.005846338768775965\n",
            "    Jaccard_similarity: 0.9966860230441312\n",
            "    Kendall_tau: 0.017588534068189006\n",
            "    Manhattan_distance: 553.0183856561443\n",
            "    Euclidean_distance: 25.01737628093611\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 1, Type=Embedding\n",
            "    Pearson_correlation: nan\n",
            "    Cosine_similarity: 0.0\n",
            "    Jaccard_similarity: 1.0\n",
            "    Kendall_tau: nan\n",
            "    Manhattan_distance: 0.0\n",
            "    Euclidean_distance: 0.0\n",
            "    Hamming_distance: 0.0\n",
            "Layer 28, Type=Linear\n",
            "    Pearson_correlation: 0.973571716578931\n",
            "    Cosine_similarity: 0.9749610186001\n",
            "    Jaccard_similarity: 0.995422867355608\n",
            "    Kendall_tau: 0.9745890960916215\n",
            "    Manhattan_distance: 3092.3953343374496\n",
            "    Euclidean_distance: 70.27570624184327\n",
            "    Hamming_distance: 0.061105915593847614\n",
            "Layer 30, Type=Linear\n",
            "    Pearson_correlation: 0.9733009888322087\n",
            "    Cosine_similarity: 0.9733009405929393\n",
            "    Jaccard_similarity: 0.9985379459978538\n",
            "    Kendall_tau: 0.8677993531048289\n",
            "    Manhattan_distance: 497.6902859859764\n",
            "    Euclidean_distance: 22.61177273679529\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 31, Type=Sequential\n",
            "    Pearson_correlation: 0.9733009888322087\n",
            "    Cosine_similarity: 0.9733009405929393\n",
            "    Jaccard_similarity: 0.9985379459978538\n",
            "    Kendall_tau: 0.8677993531048289\n",
            "    Manhattan_distance: 497.6902859859764\n",
            "    Euclidean_distance: 22.61177273679529\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 32, Type=FeedForward\n",
            "    Pearson_correlation: 0.9733009888322087\n",
            "    Cosine_similarity: 0.9733009405929393\n",
            "    Jaccard_similarity: 0.9985379459978538\n",
            "    Kendall_tau: 0.8677993531048289\n",
            "    Manhattan_distance: 497.6902859859764\n",
            "    Euclidean_distance: 22.61177273679529\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 21, Type=Linear\n",
            "    Pearson_correlation: 0.97225277335245\n",
            "    Cosine_similarity: 0.9722535305162386\n",
            "    Jaccard_similarity: 0.9978502060728157\n",
            "    Kendall_tau: 0.934504467945714\n",
            "    Manhattan_distance: 1660.5944556194493\n",
            "    Euclidean_distance: 70.58984422094608\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 37, Type=Linear\n",
            "    Pearson_correlation: 0.9452866586375673\n",
            "    Cosine_similarity: 0.9997257879801793\n",
            "    Jaccard_similarity: 0.551848382498382\n",
            "    Kendall_tau: 0.7688129623323168\n",
            "    Manhattan_distance: 113034.38150265573\n",
            "    Euclidean_distance: 506.6640076950167\n",
            "    Hamming_distance: 0.999982092046879\n",
            "Layer 4, Type=Linear\n",
            "    Pearson_correlation: 0.9284260835936262\n",
            "    Cosine_similarity: 0.928425696559332\n",
            "    Jaccard_similarity: 0.9987513664104387\n",
            "    Kendall_tau: 0.7539967563598475\n",
            "    Manhattan_distance: 1009.3112136069847\n",
            "    Euclidean_distance: 46.14095333939464\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 20, Type=Linear\n",
            "    Pearson_correlation: 0.9273135051111424\n",
            "    Cosine_similarity: 0.9273183653579056\n",
            "    Jaccard_similarity: 0.9969828500721365\n",
            "    Kendall_tau: 0.8708270572817118\n",
            "    Manhattan_distance: 1802.564881779553\n",
            "    Euclidean_distance: 78.26401479013494\n",
            "    Hamming_distance: 0.015267524921901426\n",
            "Layer 12, Type=Linear\n",
            "    Pearson_correlation: 0.9177209403729252\n",
            "    Cosine_similarity: 0.920328586178964\n",
            "    Jaccard_similarity: 0.9925463232250855\n",
            "    Kendall_tau: 0.8005817822920314\n",
            "    Manhattan_distance: 5222.520141459057\n",
            "    Euclidean_distance: 116.8868664475848\n",
            "    Hamming_distance: 0.061105915593847614\n",
            "Layer 14, Type=Linear\n",
            "    Pearson_correlation: 0.8900907344524318\n",
            "    Cosine_similarity: 0.8900906916253468\n",
            "    Jaccard_similarity: 0.9980811554054929\n",
            "    Kendall_tau: 0.7468154239953803\n",
            "    Manhattan_distance: 3127.713773268829\n",
            "    Euclidean_distance: 142.33740417007704\n",
            "    Hamming_distance: 0.015265535149332432\n",
            "Layer 15, Type=Sequential\n",
            "    Pearson_correlation: 0.8900907344524318\n",
            "    Cosine_similarity: 0.8900906916253468\n",
            "    Jaccard_similarity: 0.9980811554054929\n",
            "    Kendall_tau: 0.7468154239953803\n",
            "    Manhattan_distance: 3127.713773268829\n",
            "    Euclidean_distance: 142.33740417007704\n",
            "    Hamming_distance: 0.015265535149332432\n",
            "Layer 16, Type=FeedForward\n",
            "    Pearson_correlation: 0.8900907344524318\n",
            "    Cosine_similarity: 0.8900906916253468\n",
            "    Jaccard_similarity: 0.9980811554054929\n",
            "    Kendall_tau: 0.7468154239953803\n",
            "    Manhattan_distance: 3127.713773268829\n",
            "    Euclidean_distance: 142.33740417007704\n",
            "    Hamming_distance: 0.015265535149332432\n",
            "Layer 6, Type=Linear\n",
            "    Pearson_correlation: 0.8383177350291036\n",
            "    Cosine_similarity: 0.8383162788450319\n",
            "    Jaccard_similarity: 0.9976832580523347\n",
            "    Kendall_tau: 0.6728994176376035\n",
            "    Manhattan_distance: 762.0313312309441\n",
            "    Euclidean_distance: 34.753055234143844\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 19, Type=LayerNorm\n",
            "    Pearson_correlation: 0.8174704728563172\n",
            "    Cosine_similarity: 0.817470468879504\n",
            "    Jaccard_similarity: 0.9982767007615214\n",
            "    Kendall_tau: 0.6324573517124631\n",
            "    Manhattan_distance: 258.32055802309304\n",
            "    Euclidean_distance: 11.662457754029065\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 27, Type=LayerNorm\n",
            "    Pearson_correlation: 0.7851201763580036\n",
            "    Cosine_similarity: 0.7851201445720729\n",
            "    Jaccard_similarity: 0.9981491234615211\n",
            "    Kendall_tau: 0.5807949920415789\n",
            "    Manhattan_distance: 283.49440821285873\n",
            "    Euclidean_distance: 12.823036362633152\n",
            "    Hamming_distance: 0.015265535149332432\n",
            "Layer 8, Type=Linear\n",
            "    Pearson_correlation: 0.7760066611005637\n",
            "    Cosine_similarity: 0.7760064044061997\n",
            "    Jaccard_similarity: 0.9975239550569126\n",
            "    Kendall_tau: 0.5950380507704247\n",
            "    Manhattan_distance: 2057.492022460815\n",
            "    Euclidean_distance: 92.44831005930948\n",
            "    Hamming_distance: 0.015265535149332432\n",
            "Layer 9, Type=MultiHeadAttention\n",
            "    Pearson_correlation: 0.7760066611005637\n",
            "    Cosine_similarity: 0.7760064044061997\n",
            "    Jaccard_similarity: 0.9975239550569126\n",
            "    Kendall_tau: 0.5950380507704247\n",
            "    Manhattan_distance: 2057.492022460815\n",
            "    Euclidean_distance: 92.44831005930948\n",
            "    Hamming_distance: 0.015265535149332432\n",
            "Layer 34, Type=TransformerBlock\n",
            "    Pearson_correlation: 0.7612150214210146\n",
            "    Cosine_similarity: 0.7612121098389599\n",
            "    Jaccard_similarity: 0.9972311822942705\n",
            "    Kendall_tau: 0.5990072949810774\n",
            "    Manhattan_distance: 5430.913512317809\n",
            "    Euclidean_distance: 245.8796345783037\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 35, Type=Sequential\n",
            "    Pearson_correlation: 0.7612150214210146\n",
            "    Cosine_similarity: 0.7612121098389599\n",
            "    Jaccard_similarity: 0.9972311822942705\n",
            "    Kendall_tau: 0.5990072949810774\n",
            "    Manhattan_distance: 5430.913512317809\n",
            "    Euclidean_distance: 245.8796345783037\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 11, Type=LayerNorm\n",
            "    Pearson_correlation: 0.7582115030117567\n",
            "    Cosine_similarity: 0.7582116144212029\n",
            "    Jaccard_similarity: 0.9980656111253164\n",
            "    Kendall_tau: 0.5698787966979232\n",
            "    Manhattan_distance: 324.70512083585066\n",
            "    Euclidean_distance: 14.724933325893536\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 36, Type=LayerNorm\n",
            "    Pearson_correlation: 0.7349489007334098\n",
            "    Cosine_similarity: 0.734948811452476\n",
            "    Jaccard_similarity: 0.9978675423672249\n",
            "    Kendall_tau: 0.5765211721213864\n",
            "    Manhattan_distance: 349.1280365111721\n",
            "    Euclidean_distance: 15.837778656583163\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 18, Type=TransformerBlock\n",
            "    Pearson_correlation: 0.7291930331904052\n",
            "    Cosine_similarity: 0.7291909683859095\n",
            "    Jaccard_similarity: 0.9973368564109528\n",
            "    Kendall_tau: 0.5248979866711554\n",
            "    Manhattan_distance: 5257.08837483035\n",
            "    Euclidean_distance: 242.535298528883\n",
            "    Hamming_distance: 0.01526951469447042\n",
            "Layer 22, Type=Linear\n",
            "    Pearson_correlation: 0.5972447099414675\n",
            "    Cosine_similarity: 0.5972315973670816\n",
            "    Jaccard_similarity: 0.9974956201240832\n",
            "    Kendall_tau: 0.40356105392789476\n",
            "    Manhattan_distance: 541.7435645515372\n",
            "    Euclidean_distance: 24.58232250344313\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 3, Type=LayerNorm\n",
            "    Pearson_correlation: 0.5749976111749941\n",
            "    Cosine_similarity: 0.5749981674874626\n",
            "    Jaccard_similarity: 0.9976202007396914\n",
            "    Kendall_tau: 0.4144687799192666\n",
            "    Manhattan_distance: 354.31926956084874\n",
            "    Euclidean_distance: 16.073233207816333\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 5, Type=Linear\n",
            "    Pearson_correlation: 0.5065972017123019\n",
            "    Cosine_similarity: 0.5065975774637392\n",
            "    Jaccard_similarity: 0.9971798298319291\n",
            "    Kendall_tau: 0.34731982550440493\n",
            "    Manhattan_distance: 857.6619382900568\n",
            "    Euclidean_distance: 39.32716068015316\n",
            "    Hamming_distance: 0.015267524921901426\n",
            "Layer 24, Type=Linear\n",
            "    Pearson_correlation: 0.4701295759068225\n",
            "    Cosine_similarity: 0.4701257439608845\n",
            "    Jaccard_similarity: 0.99650842534709\n",
            "    Kendall_tau: 0.306478324566302\n",
            "    Manhattan_distance: 2401.700445066444\n",
            "    Euclidean_distance: 108.70866892917873\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 25, Type=MultiHeadAttention\n",
            "    Pearson_correlation: 0.4701295759068225\n",
            "    Cosine_similarity: 0.4701257439608845\n",
            "    Jaccard_similarity: 0.99650842534709\n",
            "    Kendall_tau: 0.306478324566302\n",
            "    Manhattan_distance: 2401.700445066444\n",
            "    Euclidean_distance: 108.70866892917873\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 7, Type=Dropout\n",
            "    Pearson_correlation: 0.3636925408303868\n",
            "    Cosine_similarity: 0.3637170854034324\n",
            "    Jaccard_similarity: 0.9999613883641197\n",
            "    Kendall_tau: 0.8858634488614004\n",
            "    Manhattan_distance: 1.9405900996968377\n",
            "    Euclidean_distance: 0.9466845597886717\n",
            "    Hamming_distance: 0.00016515112322661516\n",
            "Layer 13, Type=GELU\n",
            "    Pearson_correlation: 0.16197588472601288\n",
            "    Cosine_similarity: 0.16795766727860859\n",
            "    Jaccard_similarity: 0.9748042741498975\n",
            "    Kendall_tau: -0.2006952818203617\n",
            "    Manhattan_distance: 1521.0243120338662\n",
            "    Euclidean_distance: 55.374471593385465\n",
            "    Hamming_distance: 0.045492170244940996\n",
            "Layer 10, Type=Dropout\n",
            "    Pearson_correlation: 0.03200364612296609\n",
            "    Cosine_similarity: 0.0320064583647026\n",
            "    Jaccard_similarity: 0.9956593087415067\n",
            "    Kendall_tau: 0.03083688959257269\n",
            "    Manhattan_distance: 3549.849239751079\n",
            "    Euclidean_distance: 166.19638435446058\n",
            "    Hamming_distance: 0.014071671607935215\n",
            "Layer 17, Type=Dropout\n",
            "    Pearson_correlation: 0.013039717822348737\n",
            "    Cosine_similarity: 0.013038904611483201\n",
            "    Jaccard_similarity: 0.9950721965487851\n",
            "    Kendall_tau: 0.0012558035618050287\n",
            "    Manhattan_distance: 6889.443359172515\n",
            "    Euclidean_distance: 320.06878604583045\n",
            "    Hamming_distance: 0.014051773882245261\n",
            "Layer 2, Type=Dropout\n",
            "    Pearson_correlation: 0.005332185614932555\n",
            "    Cosine_similarity: 0.005359180226146123\n",
            "    Jaccard_similarity: 0.9965559634676732\n",
            "    Kendall_tau: -0.010948827826658727\n",
            "    Manhattan_distance: 708.5936257776152\n",
            "    Euclidean_distance: 33.268003314159785\n",
            "    Hamming_distance: 0.014107487514177128\n",
            "Layer 23, Type=Dropout\n",
            "    Pearson_correlation: nan\n",
            "    Cosine_similarity: 0.4046874713001379\n",
            "    Jaccard_similarity: 0.9999798914010729\n",
            "    Kendall_tau: nan\n",
            "    Manhattan_distance: 1.0106314337841618\n",
            "    Euclidean_distance: 0.7562127549816721\n",
            "    Hamming_distance: 4.5764769086893364e-05\n",
            "Layer 26, Type=Dropout\n",
            "    Pearson_correlation: -2.475885857658759e-05\n",
            "    Cosine_similarity: -2.780445633638615e-05\n",
            "    Jaccard_similarity: 0.995806196091259\n",
            "    Kendall_tau: -0.005547474015253013\n",
            "    Manhattan_distance: 2852.1359454028257\n",
            "    Euclidean_distance: 134.22994012692158\n",
            "    Hamming_distance: 0.013978152297192431\n",
            "Layer 29, Type=GELU\n",
            "    Pearson_correlation: -0.006016705532660322\n",
            "    Cosine_similarity: -0.006031639699680539\n",
            "    Jaccard_similarity: 0.9971350439976711\n",
            "    Kendall_tau: 0.7976096480679189\n",
            "    Manhattan_distance: 168.7187126822887\n",
            "    Euclidean_distance: 16.04253834293691\n",
            "    Hamming_distance: 0.04949957219889767\n",
            "Layer 33, Type=Dropout\n",
            "    Pearson_correlation: -0.009669008158794193\n",
            "    Cosine_similarity: -0.009668618053008888\n",
            "    Jaccard_similarity: 0.9965060061792258\n",
            "    Kendall_tau: -0.03517323050791124\n",
            "    Manhattan_distance: 1205.4402526239771\n",
            "    Euclidean_distance: 56.83740076426625\n",
            "    Hamming_distance: 0.01391050002984659\n",
            "Most Relevant Layers: \n",
            "Layer 28, Type=Linear, Size=3072 Pearson_correlation: 0.973571716578931, Cosine_similarity: 0.9749610186001, Jaccard_similarity: 0.995422867355608, Kendall_tau: 0.9745890960916215, Manhattan_distance: 3092.3953343374496, Euclidean_distance: 70.27570624184327, Hamming_distance: 0.061105915593847614\n",
            "Layer 30, Type=Linear, Size=768 Pearson_correlation: 0.9733009888322087, Cosine_similarity: 0.9733009405929393, Jaccard_similarity: 0.9985379459978538, Kendall_tau: 0.8677993531048289, Manhattan_distance: 497.6902859859764, Euclidean_distance: 22.61177273679529, Hamming_distance: 0.015263545376763438\n",
            "Layer 32, Type=FeedForward, Size=3072 Pearson_correlation: 0.9733009888322087, Cosine_similarity: 0.9733009405929393, Jaccard_similarity: 0.9985379459978538, Kendall_tau: 0.8677993531048289, Manhattan_distance: 497.6902859859764, Euclidean_distance: 22.61177273679529, Hamming_distance: 0.015263545376763438\n",
            "Layer 21, Type=Linear, Size=768 Pearson_correlation: 0.97225277335245, Cosine_similarity: 0.9722535305162386, Jaccard_similarity: 0.9978502060728157, Kendall_tau: 0.934504467945714, Manhattan_distance: 1660.5944556194493, Euclidean_distance: 70.58984422094608, Hamming_distance: 0.015261555604194444\n",
            "Layer 37, Type=Linear, Size=768 Pearson_correlation: 0.9452866586375673, Cosine_similarity: 0.9997257879801793, Jaccard_similarity: 0.551848382498382, Kendall_tau: 0.7688129623323168, Manhattan_distance: 113034.38150265573, Euclidean_distance: 506.6640076950167, Hamming_distance: 0.999982092046879\n",
            "Layer 4, Type=Linear, Size=768 Pearson_correlation: 0.9284260835936262, Cosine_similarity: 0.928425696559332, Jaccard_similarity: 0.9987513664104387, Kendall_tau: 0.7539967563598475, Manhattan_distance: 1009.3112136069847, Euclidean_distance: 46.14095333939464, Hamming_distance: 0.015263545376763438\n",
            "Layer 20, Type=Linear, Size=768 Pearson_correlation: 0.9273135051111424, Cosine_similarity: 0.9273183653579056, Jaccard_similarity: 0.9969828500721365, Kendall_tau: 0.8708270572817118, Manhattan_distance: 1802.564881779553, Euclidean_distance: 78.26401479013494, Hamming_distance: 0.015267524921901426\n",
            "Layer 12, Type=Linear, Size=3072 Pearson_correlation: 0.9177209403729252, Cosine_similarity: 0.920328586178964, Jaccard_similarity: 0.9925463232250855, Kendall_tau: 0.8005817822920314, Manhattan_distance: 5222.520141459057, Euclidean_distance: 116.8868664475848, Hamming_distance: 0.061105915593847614\n",
            "Layer 14, Type=Linear, Size=768 Pearson_correlation: 0.8900907344524318, Cosine_similarity: 0.8900906916253468, Jaccard_similarity: 0.9980811554054929, Kendall_tau: 0.7468154239953803, Manhattan_distance: 3127.713773268829, Euclidean_distance: 142.33740417007704, Hamming_distance: 0.015265535149332432\n",
            "Layer 16, Type=FeedForward, Size=3072 Pearson_correlation: 0.8900907344524318, Cosine_similarity: 0.8900906916253468, Jaccard_similarity: 0.9980811554054929, Kendall_tau: 0.7468154239953803, Manhattan_distance: 3127.713773268829, Euclidean_distance: 142.33740417007704, Hamming_distance: 0.015265535149332432\n",
            "Total Parameters:  16896 / 87889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Trainsamples vs. Testsamples"
      ],
      "metadata": {
        "id": "GLYaTy7vhC0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed0TrainGeneratedEvaluationSentences = ['physical athleticism or physical dexterity, with major competitions admitting only sports meeting this definition.', 'classification as sports.', 'with others being done by hundreds.', 'this definition.', 'producing a champion.', ', producing a \\\\\"tie\\\\\" or \\\\\"tie\\\\\" or \\\\\"tie\\\\\", producing a \\\\\"draw\\\\\", in a single winner.', 'with others being done by hundreds.', 'this definition.', 'ensure one winner., with major competitions admitting only sports meeting this definition.', 'hundreds.']\n",
        "seed0TestGeneratedEvaluationSentences = ['admitting only sports meeting this definition.', 'competitions admitting only sports meeting this definition.', 'by playoffs.', 'with others being done by hundreds.', 'others provide tie-breaking methods to ensure one winner.', 'competitions admitting only sports meeting this definition.', '., producing a single person with others being done by arranging games in a single person with others being done by hundreds., producing a single person with others provide enjoyment to spectators.', '. ensure one winner.', 'some cases, with different participant numbers, producing a tournament format, with different participant numbers, with others being done by arranging games in some cases, with others being done by arranging games in a tournament format in a single person with different participant numbers, with others provide tie-breaking methods to spectators... some cases, producing a single person with others provide.. provide.', 'classification as sports.']"
      ],
      "metadata": {
        "id": "ehLUMJjOz-IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDf = pq.read_table('./Data/identifiedClosestEvalSourcesTrainingSeed0.parquet').to_pandas(safe=False)\n",
        "testDf = pq.read_table('./Data/identifiedClosestEvalSourcesTestSeed0.parquet').to_pandas(safe=False)\n",
        "\n",
        "evaluationSamples = len(trainDf.index.unique())\n",
        "trainSamples = len(trainDf['source'].unique())"
      ],
      "metadata": {
        "id": "7avUGugLhBn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for evaluation in range(evaluationSamples):\n",
        "    blendActivations(evaluation, \"Training\", 5, trainDf)\n",
        "    blendActivations(evaluation, \"Test\", 5, testDf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHFdumw_iXmL",
        "outputId": "3dcf5be3-1821-42fa-9459-e85e06a8c67c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample0 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9776\n",
            "Euclidean Distance: 225.1966\n",
            "Manhattan Distance: 49999.2426\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9771\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample0 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9777\n",
            "Euclidean Distance: 225.0700\n",
            "Manhattan Distance: 49970.2538\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9771\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample1 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9776\n",
            "Euclidean Distance: 225.1966\n",
            "Manhattan Distance: 49999.2426\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9771\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample1 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9776\n",
            "Euclidean Distance: 225.1966\n",
            "Manhattan Distance: 49999.2426\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9771\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample2 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9776\n",
            "Euclidean Distance: 225.1966\n",
            "Manhattan Distance: 49999.2426\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9771\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample2 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9776\n",
            "Euclidean Distance: 225.2382\n",
            "Manhattan Distance: 50017.6120\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9770\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample3 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9776\n",
            "Euclidean Distance: 225.1966\n",
            "Manhattan Distance: 49999.2426\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9771\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample3 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9776\n",
            "Euclidean Distance: 225.3488\n",
            "Manhattan Distance: 50033.3346\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9770\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample4 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9792\n",
            "Euclidean Distance: 209.9139\n",
            "Manhattan Distance: 45573.0762\n",
            "Jaccard Similarity: 0.9974\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9787\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample4 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9792\n",
            "Euclidean Distance: 209.9139\n",
            "Manhattan Distance: 45573.0762\n",
            "Jaccard Similarity: 0.9974\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9787\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample5 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9792\n",
            "Euclidean Distance: 204.2743\n",
            "Manhattan Distance: 43571.4822\n",
            "Jaccard Similarity: 0.9975\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9787\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample5 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9792\n",
            "Euclidean Distance: 204.2743\n",
            "Manhattan Distance: 43571.4822\n",
            "Jaccard Similarity: 0.9975\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9787\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample6 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9776\n",
            "Euclidean Distance: 225.1966\n",
            "Manhattan Distance: 49999.2426\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9771\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample6 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9776\n",
            "Euclidean Distance: 225.1966\n",
            "Manhattan Distance: 49999.2426\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9771\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample7 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9776\n",
            "Euclidean Distance: 225.2256\n",
            "Manhattan Distance: 50007.5064\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9771\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample7 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9776\n",
            "Euclidean Distance: 225.2256\n",
            "Manhattan Distance: 50007.5064\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9771\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample8 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9777\n",
            "Euclidean Distance: 225.0333\n",
            "Manhattan Distance: 49950.4468\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9771\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample8 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9775\n",
            "Euclidean Distance: 225.6317\n",
            "Manhattan Distance: 50104.8325\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9769\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample9 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9776\n",
            "Euclidean Distance: 225.1966\n",
            "Manhattan Distance: 49999.2426\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9771\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample9 ---\n",
            "Kendall's Tau: 0.74\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9776\n",
            "Euclidean Distance: 225.1966\n",
            "Manhattan Distance: 49999.2426\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0446\n",
            "Pearson Correlation: 0.9771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = aggregateLayerMetricsByAllMeans(\"Train\", 5, trainDf)\n",
        "test = aggregateLayerMetricsByAllMeans(\"Test\", 5, testDf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvwCUIephIlM",
        "outputId": "909d291d-503c-499e-96e2-58b4ecc07449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sorted Layers by Custom Order of Metrics for Train ---\n",
            "Layer 0, Type=Embedding\n",
            "    Pearson_correlation: -0.013321016473944037\n",
            "    Cosine_similarity: -0.013323179467305546\n",
            "    Jaccard_similarity: 0.9967612986280171\n",
            "    Kendall_tau: 0.024281272868548958\n",
            "    Manhattan_distance: 719.1806125663627\n",
            "    Euclidean_distance: 32.09258568130836\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 1, Type=Embedding\n",
            "    Pearson_correlation: nan\n",
            "    Cosine_similarity: 0.0\n",
            "    Jaccard_similarity: 1.0\n",
            "    Kendall_tau: nan\n",
            "    Manhattan_distance: 0.0\n",
            "    Euclidean_distance: 0.0\n",
            "    Hamming_distance: 0.0\n",
            "Layer 30, Type=Linear\n",
            "    Pearson_correlation: 0.9029472215466419\n",
            "    Cosine_similarity: 0.9029472010687988\n",
            "    Jaccard_similarity: 0.9983499040684969\n",
            "    Kendall_tau: 0.7538914161396283\n",
            "    Manhattan_distance: 292.3890816339978\n",
            "    Euclidean_distance: 13.079640701274709\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 31, Type=Sequential\n",
            "    Pearson_correlation: 0.9029472215466419\n",
            "    Cosine_similarity: 0.9029472010687988\n",
            "    Jaccard_similarity: 0.9983499040684969\n",
            "    Kendall_tau: 0.7538914161396283\n",
            "    Manhattan_distance: 292.3890816339978\n",
            "    Euclidean_distance: 13.079640701274709\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 32, Type=FeedForward\n",
            "    Pearson_correlation: 0.9029472215466419\n",
            "    Cosine_similarity: 0.9029472010687988\n",
            "    Jaccard_similarity: 0.9983499040684969\n",
            "    Kendall_tau: 0.7538914161396283\n",
            "    Manhattan_distance: 292.3890816339978\n",
            "    Euclidean_distance: 13.079640701274709\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 14, Type=Linear\n",
            "    Pearson_correlation: 0.7923230994502765\n",
            "    Cosine_similarity: 0.792323019190109\n",
            "    Jaccard_similarity: 0.9982194151161103\n",
            "    Kendall_tau: 0.6050296409986974\n",
            "    Manhattan_distance: 250.71921699486217\n",
            "    Euclidean_distance: 11.211210625614157\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 15, Type=Sequential\n",
            "    Pearson_correlation: 0.7923230994502765\n",
            "    Cosine_similarity: 0.792323019190109\n",
            "    Jaccard_similarity: 0.9982194151161103\n",
            "    Kendall_tau: 0.6050296409986974\n",
            "    Manhattan_distance: 250.71921699486217\n",
            "    Euclidean_distance: 11.211210625614157\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 16, Type=FeedForward\n",
            "    Pearson_correlation: 0.7923230994502765\n",
            "    Cosine_similarity: 0.792323019190109\n",
            "    Jaccard_similarity: 0.9982194151161103\n",
            "    Kendall_tau: 0.6050296409986974\n",
            "    Manhattan_distance: 250.71921699486217\n",
            "    Euclidean_distance: 11.211210625614157\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 20, Type=Linear\n",
            "    Pearson_correlation: 0.6987632314825445\n",
            "    Cosine_similarity: 0.6987576920115544\n",
            "    Jaccard_similarity: 0.9978808446373654\n",
            "    Kendall_tau: 0.5193092311813405\n",
            "    Manhattan_distance: 365.9577918417296\n",
            "    Euclidean_distance: 16.61393783784422\n",
            "    Hamming_distance: 0.015279463557315399\n",
            "Layer 37, Type=Linear\n",
            "    Pearson_correlation: 0.6877892709754738\n",
            "    Cosine_similarity: 0.9940403123493609\n",
            "    Jaccard_similarity: 0.8503538938164545\n",
            "    Kendall_tau: 0.4172135570832265\n",
            "    Manhattan_distance: 28364.556591325236\n",
            "    Euclidean_distance: 150.6815813555257\n",
            "    Hamming_distance: 0.9999801022743101\n",
            "Layer 23, Type=Dropout\n",
            "    Pearson_correlation: 0.6840533933068264\n",
            "    Cosine_similarity: 0.6840924864759612\n",
            "    Jaccard_similarity: 0.9999834442456306\n",
            "    Kendall_tau: 0.9998358094593653\n",
            "    Manhattan_distance: 0.8320625221789177\n",
            "    Euclidean_distance: 0.24700739536461302\n",
            "    Hamming_distance: 0.00039795451379907274\n",
            "Layer 29, Type=GELU\n",
            "    Pearson_correlation: 0.661397627786056\n",
            "    Cosine_similarity: 0.6673330051765811\n",
            "    Jaccard_similarity: 0.9852010868699992\n",
            "    Kendall_tau: 0.38449949574825937\n",
            "    Manhattan_distance: 888.0887955121809\n",
            "    Euclidean_distance: 22.012938902781627\n",
            "    Hamming_distance: 0.061105915593847614\n",
            "Layer 34, Type=TransformerBlock\n",
            "    Pearson_correlation: 0.6580339571746302\n",
            "    Cosine_similarity: 0.6580338443296303\n",
            "    Jaccard_similarity: 0.9970513268785239\n",
            "    Kendall_tau: 0.48882872679499395\n",
            "    Manhattan_distance: 1389.6503983858627\n",
            "    Euclidean_distance: 61.84192071008302\n",
            "    Hamming_distance: 0.015281453329884395\n",
            "Layer 35, Type=Sequential\n",
            "    Pearson_correlation: 0.6580339571746302\n",
            "    Cosine_similarity: 0.6580338443296303\n",
            "    Jaccard_similarity: 0.9970513268785239\n",
            "    Kendall_tau: 0.48882872679499395\n",
            "    Manhattan_distance: 1389.6503983858627\n",
            "    Euclidean_distance: 61.84192071008302\n",
            "    Hamming_distance: 0.015281453329884395\n",
            "Layer 36, Type=LayerNorm\n",
            "    Pearson_correlation: 0.6554435396459217\n",
            "    Cosine_similarity: 0.655443597610773\n",
            "    Jaccard_similarity: 0.9974920450500375\n",
            "    Kendall_tau: 0.4982547874992311\n",
            "    Manhattan_distance: 475.1032335634921\n",
            "    Euclidean_distance: 21.20547973423435\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 4, Type=Linear\n",
            "    Pearson_correlation: 0.6097491367847162\n",
            "    Cosine_similarity: 0.609758065826236\n",
            "    Jaccard_similarity: 0.9975712256983593\n",
            "    Kendall_tau: 0.425262618158336\n",
            "    Manhattan_distance: 428.10282021315436\n",
            "    Euclidean_distance: 19.34336871318082\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 28, Type=Linear\n",
            "    Pearson_correlation: 0.6035891376218947\n",
            "    Cosine_similarity: 0.605981893583224\n",
            "    Jaccard_similarity: 0.9905448362445999\n",
            "    Kendall_tau: 0.3878479348957266\n",
            "    Manhattan_distance: 1375.4376053613964\n",
            "    Euclidean_distance: 31.078075407516714\n",
            "    Hamming_distance: 0.061105915593847614\n",
            "Layer 5, Type=Linear\n",
            "    Pearson_correlation: 0.5537605676757786\n",
            "    Cosine_similarity: 0.5537562725427174\n",
            "    Jaccard_similarity: 0.9976889251733683\n",
            "    Kendall_tau: 0.3867889001296671\n",
            "    Manhattan_distance: 312.53830393000027\n",
            "    Euclidean_distance: 14.004532009821798\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 3, Type=LayerNorm\n",
            "    Pearson_correlation: 0.5517970712651064\n",
            "    Cosine_similarity: 0.551797261348703\n",
            "    Jaccard_similarity: 0.9974599403860959\n",
            "    Kendall_tau: 0.3432515557345064\n",
            "    Manhattan_distance: 529.8929055348206\n",
            "    Euclidean_distance: 23.65179304006697\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 21, Type=Linear\n",
            "    Pearson_correlation: 0.5508315702102411\n",
            "    Cosine_similarity: 0.5508428972930833\n",
            "    Jaccard_similarity: 0.9977303872704576\n",
            "    Kendall_tau: 0.362749163840954\n",
            "    Manhattan_distance: 304.83967285291544\n",
            "    Euclidean_distance: 13.75933349559763\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 13, Type=GELU\n",
            "    Pearson_correlation: 0.5435150992852587\n",
            "    Cosine_similarity: 0.5468136065222604\n",
            "    Jaccard_similarity: 0.9877425617053476\n",
            "    Kendall_tau: 0.3394886045359072\n",
            "    Manhattan_distance: 729.9658177609635\n",
            "    Euclidean_distance: 19.653900683710948\n",
            "    Hamming_distance: 0.06112581331953758\n",
            "Layer 12, Type=Linear\n",
            "    Pearson_correlation: 0.5388668360942444\n",
            "    Cosine_similarity: 0.5389698662989957\n",
            "    Jaccard_similarity: 0.9905192798769431\n",
            "    Kendall_tau: 0.38657758995121255\n",
            "    Manhattan_distance: 1383.6581424574258\n",
            "    Euclidean_distance: 31.469934217237743\n",
            "    Hamming_distance: 0.061105915593847614\n",
            "Layer 19, Type=LayerNorm\n",
            "    Pearson_correlation: 0.514333825668707\n",
            "    Cosine_similarity: 0.5143340475688902\n",
            "    Jaccard_similarity: 0.9973408297405367\n",
            "    Kendall_tau: 0.34937996226190277\n",
            "    Manhattan_distance: 530.5327714132591\n",
            "    Euclidean_distance: 24.099886424072718\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 8, Type=Linear\n",
            "    Pearson_correlation: 0.5125921911345324\n",
            "    Cosine_similarity: 0.5125919926591245\n",
            "    Jaccard_similarity: 0.997056658573183\n",
            "    Kendall_tau: 0.3494147363769424\n",
            "    Manhattan_distance: 467.37266170450823\n",
            "    Euclidean_distance: 20.896259220180134\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 9, Type=MultiHeadAttention\n",
            "    Pearson_correlation: 0.5125921911345324\n",
            "    Cosine_similarity: 0.5125919926591245\n",
            "    Jaccard_similarity: 0.997056658573183\n",
            "    Kendall_tau: 0.3494147363769424\n",
            "    Manhattan_distance: 467.37266170450823\n",
            "    Euclidean_distance: 20.896259220180134\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 6, Type=Linear\n",
            "    Pearson_correlation: 0.502761303329402\n",
            "    Cosine_similarity: 0.5027605594947266\n",
            "    Jaccard_similarity: 0.9976477106729671\n",
            "    Kendall_tau: 0.3014583751065216\n",
            "    Manhattan_distance: 438.53044452180256\n",
            "    Euclidean_distance: 19.777462565001024\n",
            "    Hamming_distance: 0.015281453329884395\n",
            "Layer 27, Type=LayerNorm\n",
            "    Pearson_correlation: 0.4928031887525677\n",
            "    Cosine_similarity: 0.4928033714301372\n",
            "    Jaccard_similarity: 0.9972203182635591\n",
            "    Kendall_tau: 0.3520949260665768\n",
            "    Manhattan_distance: 547.2715089242763\n",
            "    Euclidean_distance: 24.56037856672764\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 11, Type=LayerNorm\n",
            "    Pearson_correlation: 0.49033640190431205\n",
            "    Cosine_similarity: 0.49033661271493356\n",
            "    Jaccard_similarity: 0.997064804528434\n",
            "    Kendall_tau: 0.2712672770322931\n",
            "    Manhattan_distance: 544.0900390833282\n",
            "    Euclidean_distance: 24.860190094743\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 7, Type=Dropout\n",
            "    Pearson_correlation: 0.4775504480304648\n",
            "    Cosine_similarity: 0.4775945761321269\n",
            "    Jaccard_similarity: 0.9999544503908531\n",
            "    Kendall_tau: 0.9998750161702807\n",
            "    Manhattan_distance: 2.28931344936666\n",
            "    Euclidean_distance: 1.1983061343507972\n",
            "    Hamming_distance: 0.00039795451379907274\n",
            "Layer 24, Type=Linear\n",
            "    Pearson_correlation: 0.4686358152354718\n",
            "    Cosine_similarity: 0.4686593053447844\n",
            "    Jaccard_similarity: 0.9969669352297423\n",
            "    Kendall_tau: 0.30661902091808635\n",
            "    Manhattan_distance: 440.76297735383935\n",
            "    Euclidean_distance: 19.58437293469806\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 25, Type=MultiHeadAttention\n",
            "    Pearson_correlation: 0.4686358152354718\n",
            "    Cosine_similarity: 0.4686593053447844\n",
            "    Jaccard_similarity: 0.9969669352297423\n",
            "    Kendall_tau: 0.30661902091808635\n",
            "    Manhattan_distance: 440.76297735383935\n",
            "    Euclidean_distance: 19.58437293469806\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 22, Type=Linear\n",
            "    Pearson_correlation: 0.45042287541958903\n",
            "    Cosine_similarity: 0.45042027036173715\n",
            "    Jaccard_similarity: 0.997391630200539\n",
            "    Kendall_tau: 0.3294332140795378\n",
            "    Manhattan_distance: 422.5258616809284\n",
            "    Euclidean_distance: 19.207068339149146\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 18, Type=TransformerBlock\n",
            "    Pearson_correlation: 0.33469896685069866\n",
            "    Cosine_similarity: 0.3347030754672546\n",
            "    Jaccard_similarity: 0.9967454845222026\n",
            "    Kendall_tau: 0.22743242382568782\n",
            "    Manhattan_distance: 1112.5274003083973\n",
            "    Euclidean_distance: 51.621587789974605\n",
            "    Hamming_distance: 0.015221760152814534\n",
            "Layer 33, Type=Dropout\n",
            "    Pearson_correlation: 0.06863978789300927\n",
            "    Cosine_similarity: 0.06863971649108204\n",
            "    Jaccard_similarity: 0.9960836582646685\n",
            "    Kendall_tau: 0.05639871097905939\n",
            "    Manhattan_distance: 704.4417054121211\n",
            "    Euclidean_distance: 32.81572143213879\n",
            "    Hamming_distance: 0.013749328451757964\n",
            "Layer 17, Type=Dropout\n",
            "    Pearson_correlation: 0.035138366285390285\n",
            "    Cosine_similarity: 0.035144080882048324\n",
            "    Jaccard_similarity: 0.9971135491623244\n",
            "    Kendall_tau: 0.04027005139996461\n",
            "    Manhattan_distance: 435.72606077448097\n",
            "    Euclidean_distance: 20.954747081484072\n",
            "    Hamming_distance: 0.013411067115028753\n",
            "Layer 10, Type=Dropout\n",
            "    Pearson_correlation: 0.023249157774898232\n",
            "    Cosine_similarity: 0.02325067079182176\n",
            "    Jaccard_similarity: 0.9965982163033292\n",
            "    Kendall_tau: 0.05088407535413466\n",
            "    Manhattan_distance: 581.3003551597146\n",
            "    Euclidean_distance: 27.394375528064632\n",
            "    Hamming_distance: 0.013629942097618242\n",
            "Layer 2, Type=Dropout\n",
            "    Pearson_correlation: -0.00699718695421635\n",
            "    Cosine_similarity: -0.006968051479579282\n",
            "    Jaccard_similarity: 0.9967652762075885\n",
            "    Kendall_tau: 0.024957361538425458\n",
            "    Manhattan_distance: 953.4681381703497\n",
            "    Euclidean_distance: 46.10893101467906\n",
            "    Hamming_distance: 0.013769226177447918\n",
            "Layer 26, Type=Dropout\n",
            "    Pearson_correlation: -0.01021604119017752\n",
            "    Cosine_similarity: -0.010135965602711816\n",
            "    Jaccard_similarity: 0.9965148857803836\n",
            "    Kendall_tau: -0.025622060019226324\n",
            "    Manhattan_distance: 534.041783998071\n",
            "    Euclidean_distance: 25.35670045595111\n",
            "    Hamming_distance: 0.013769226177447918\n",
            "Most Relevant Layers: \n",
            "Layer 30, Type=Linear, Size=768 Pearson_correlation: 0.9029472215466419, Cosine_similarity: 0.9029472010687988, Jaccard_similarity: 0.9983499040684969, Kendall_tau: 0.7538914161396283, Manhattan_distance: 292.3890816339978, Euclidean_distance: 13.079640701274709, Hamming_distance: 0.015261555604194444\n",
            "Layer 32, Type=FeedForward, Size=3072 Pearson_correlation: 0.9029472215466419, Cosine_similarity: 0.9029472010687988, Jaccard_similarity: 0.9983499040684969, Kendall_tau: 0.7538914161396283, Manhattan_distance: 292.3890816339978, Euclidean_distance: 13.079640701274709, Hamming_distance: 0.015261555604194444\n",
            "Total Parameters:  3840 / 87889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sorted Layers by Custom Order of Metrics for Test ---\n",
            "Layer 0, Type=Embedding\n",
            "    Pearson_correlation: -0.013321016473944037\n",
            "    Cosine_similarity: -0.013323179467305546\n",
            "    Jaccard_similarity: 0.9967612986280171\n",
            "    Kendall_tau: 0.024281272868548958\n",
            "    Manhattan_distance: 719.1806125663627\n",
            "    Euclidean_distance: 32.09258568130836\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 1, Type=Embedding\n",
            "    Pearson_correlation: nan\n",
            "    Cosine_similarity: 0.0\n",
            "    Jaccard_similarity: 1.0\n",
            "    Kendall_tau: nan\n",
            "    Manhattan_distance: 0.0\n",
            "    Euclidean_distance: 0.0\n",
            "    Hamming_distance: 0.0\n",
            "Layer 30, Type=Linear\n",
            "    Pearson_correlation: 0.9029472215466419\n",
            "    Cosine_similarity: 0.9029472010687988\n",
            "    Jaccard_similarity: 0.9983499040684969\n",
            "    Kendall_tau: 0.7538914161396283\n",
            "    Manhattan_distance: 292.3890816339978\n",
            "    Euclidean_distance: 13.079640701274709\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 31, Type=Sequential\n",
            "    Pearson_correlation: 0.9029472215466419\n",
            "    Cosine_similarity: 0.9029472010687988\n",
            "    Jaccard_similarity: 0.9983499040684969\n",
            "    Kendall_tau: 0.7538914161396283\n",
            "    Manhattan_distance: 292.3890816339978\n",
            "    Euclidean_distance: 13.079640701274709\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 32, Type=FeedForward\n",
            "    Pearson_correlation: 0.9029472215466419\n",
            "    Cosine_similarity: 0.9029472010687988\n",
            "    Jaccard_similarity: 0.9983499040684969\n",
            "    Kendall_tau: 0.7538914161396283\n",
            "    Manhattan_distance: 292.3890816339978\n",
            "    Euclidean_distance: 13.079640701274709\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 14, Type=Linear\n",
            "    Pearson_correlation: 0.7923230994502765\n",
            "    Cosine_similarity: 0.792323019190109\n",
            "    Jaccard_similarity: 0.9982194151161103\n",
            "    Kendall_tau: 0.6050296409986974\n",
            "    Manhattan_distance: 250.71921699486217\n",
            "    Euclidean_distance: 11.211210625614157\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 16, Type=FeedForward\n",
            "    Pearson_correlation: 0.7900612061636163\n",
            "    Cosine_similarity: 0.7900609924103719\n",
            "    Jaccard_similarity: 0.9982187920831495\n",
            "    Kendall_tau: 0.5982805661451795\n",
            "    Manhattan_distance: 252.55616396204908\n",
            "    Euclidean_distance: 11.290444820268723\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 15, Type=Sequential\n",
            "    Pearson_correlation: 0.7852966456193478\n",
            "    Cosine_similarity: 0.7852963565052361\n",
            "    Jaccard_similarity: 0.9981979196745334\n",
            "    Kendall_tau: 0.5943652287501258\n",
            "    Manhattan_distance: 250.82563905629846\n",
            "    Euclidean_distance: 11.205826900355573\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 20, Type=Linear\n",
            "    Pearson_correlation: 0.6971404276814239\n",
            "    Cosine_similarity: 0.6971328331601232\n",
            "    Jaccard_similarity: 0.9978825247726112\n",
            "    Kendall_tau: 0.5159978779220346\n",
            "    Manhattan_distance: 370.8373689573424\n",
            "    Euclidean_distance: 16.84694809571258\n",
            "    Hamming_distance: 0.015281453329884395\n",
            "Layer 37, Type=Linear\n",
            "    Pearson_correlation: 0.6877892709754738\n",
            "    Cosine_similarity: 0.9940403123493609\n",
            "    Jaccard_similarity: 0.8503538938164545\n",
            "    Kendall_tau: 0.4172135570832265\n",
            "    Manhattan_distance: 28364.556591325236\n",
            "    Euclidean_distance: 150.6815813555257\n",
            "    Hamming_distance: 0.9999801022743101\n",
            "Layer 23, Type=Dropout\n",
            "    Pearson_correlation: 0.6840533933068264\n",
            "    Cosine_similarity: 0.6840924864759612\n",
            "    Jaccard_similarity: 0.9999834442456306\n",
            "    Kendall_tau: 0.9998358094593653\n",
            "    Manhattan_distance: 0.8320625221789177\n",
            "    Euclidean_distance: 0.24700739536461302\n",
            "    Hamming_distance: 0.00039795451379907274\n",
            "Layer 29, Type=GELU\n",
            "    Pearson_correlation: 0.661397627786056\n",
            "    Cosine_similarity: 0.6673330051765811\n",
            "    Jaccard_similarity: 0.9852010868699992\n",
            "    Kendall_tau: 0.38449949574825937\n",
            "    Manhattan_distance: 888.0887955121809\n",
            "    Euclidean_distance: 22.012938902781627\n",
            "    Hamming_distance: 0.061105915593847614\n",
            "Layer 34, Type=TransformerBlock\n",
            "    Pearson_correlation: 0.6580339571746302\n",
            "    Cosine_similarity: 0.6580338443296303\n",
            "    Jaccard_similarity: 0.9970513268785239\n",
            "    Kendall_tau: 0.48882872679499395\n",
            "    Manhattan_distance: 1389.6503983858627\n",
            "    Euclidean_distance: 61.84192071008302\n",
            "    Hamming_distance: 0.015281453329884395\n",
            "Layer 35, Type=Sequential\n",
            "    Pearson_correlation: 0.6580339571746302\n",
            "    Cosine_similarity: 0.6580338443296303\n",
            "    Jaccard_similarity: 0.9970513268785239\n",
            "    Kendall_tau: 0.48882872679499395\n",
            "    Manhattan_distance: 1389.6503983858627\n",
            "    Euclidean_distance: 61.84192071008302\n",
            "    Hamming_distance: 0.015281453329884395\n",
            "Layer 36, Type=LayerNorm\n",
            "    Pearson_correlation: 0.6554435396459217\n",
            "    Cosine_similarity: 0.655443597610773\n",
            "    Jaccard_similarity: 0.9974920450500375\n",
            "    Kendall_tau: 0.4982547874992311\n",
            "    Manhattan_distance: 475.1032335634921\n",
            "    Euclidean_distance: 21.20547973423435\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 4, Type=Linear\n",
            "    Pearson_correlation: 0.6097491367847162\n",
            "    Cosine_similarity: 0.609758065826236\n",
            "    Jaccard_similarity: 0.9975712256983593\n",
            "    Kendall_tau: 0.425262618158336\n",
            "    Manhattan_distance: 428.10282021315436\n",
            "    Euclidean_distance: 19.34336871318082\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 28, Type=Linear\n",
            "    Pearson_correlation: 0.6035891376218947\n",
            "    Cosine_similarity: 0.605981893583224\n",
            "    Jaccard_similarity: 0.9905448362445999\n",
            "    Kendall_tau: 0.3878479348957266\n",
            "    Manhattan_distance: 1375.4376053613964\n",
            "    Euclidean_distance: 31.078075407516714\n",
            "    Hamming_distance: 0.061105915593847614\n",
            "Layer 5, Type=Linear\n",
            "    Pearson_correlation: 0.5537605676757786\n",
            "    Cosine_similarity: 0.5537562725427174\n",
            "    Jaccard_similarity: 0.9976889251733683\n",
            "    Kendall_tau: 0.3867889001296671\n",
            "    Manhattan_distance: 312.53830393000027\n",
            "    Euclidean_distance: 14.004532009821798\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 3, Type=LayerNorm\n",
            "    Pearson_correlation: 0.5517970712651064\n",
            "    Cosine_similarity: 0.551797261348703\n",
            "    Jaccard_similarity: 0.9974599403860959\n",
            "    Kendall_tau: 0.3432515557345064\n",
            "    Manhattan_distance: 529.8929055348206\n",
            "    Euclidean_distance: 23.65179304006697\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 21, Type=Linear\n",
            "    Pearson_correlation: 0.5508315702102411\n",
            "    Cosine_similarity: 0.5508428972930833\n",
            "    Jaccard_similarity: 0.9977303872704576\n",
            "    Kendall_tau: 0.362749163840954\n",
            "    Manhattan_distance: 304.83967285291544\n",
            "    Euclidean_distance: 13.75933349559763\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 13, Type=GELU\n",
            "    Pearson_correlation: 0.5435150992852587\n",
            "    Cosine_similarity: 0.5468136065222604\n",
            "    Jaccard_similarity: 0.9877425617053476\n",
            "    Kendall_tau: 0.3394886045359072\n",
            "    Manhattan_distance: 729.9658177609635\n",
            "    Euclidean_distance: 19.653900683710948\n",
            "    Hamming_distance: 0.06112581331953758\n",
            "Layer 12, Type=Linear\n",
            "    Pearson_correlation: 0.5388668360942444\n",
            "    Cosine_similarity: 0.5389698662989957\n",
            "    Jaccard_similarity: 0.9905192798769431\n",
            "    Kendall_tau: 0.38657758995121255\n",
            "    Manhattan_distance: 1383.6581424574258\n",
            "    Euclidean_distance: 31.469934217237743\n",
            "    Hamming_distance: 0.061105915593847614\n",
            "Layer 19, Type=LayerNorm\n",
            "    Pearson_correlation: 0.514333825668707\n",
            "    Cosine_similarity: 0.5143340475688902\n",
            "    Jaccard_similarity: 0.9973408297405367\n",
            "    Kendall_tau: 0.34937996226190277\n",
            "    Manhattan_distance: 530.5327714132591\n",
            "    Euclidean_distance: 24.099886424072718\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 8, Type=Linear\n",
            "    Pearson_correlation: 0.5125921911345324\n",
            "    Cosine_similarity: 0.5125919926591245\n",
            "    Jaccard_similarity: 0.997056658573183\n",
            "    Kendall_tau: 0.3494147363769424\n",
            "    Manhattan_distance: 467.37266170450823\n",
            "    Euclidean_distance: 20.896259220180134\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 9, Type=MultiHeadAttention\n",
            "    Pearson_correlation: 0.5125921911345324\n",
            "    Cosine_similarity: 0.5125919926591245\n",
            "    Jaccard_similarity: 0.997056658573183\n",
            "    Kendall_tau: 0.3494147363769424\n",
            "    Manhattan_distance: 467.37266170450823\n",
            "    Euclidean_distance: 20.896259220180134\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 6, Type=Linear\n",
            "    Pearson_correlation: 0.502761303329402\n",
            "    Cosine_similarity: 0.5027605594947266\n",
            "    Jaccard_similarity: 0.9976477106729671\n",
            "    Kendall_tau: 0.3014583751065216\n",
            "    Manhattan_distance: 438.53044452180256\n",
            "    Euclidean_distance: 19.777462565001024\n",
            "    Hamming_distance: 0.015281453329884395\n",
            "Layer 27, Type=LayerNorm\n",
            "    Pearson_correlation: 0.4928031887525677\n",
            "    Cosine_similarity: 0.4928033714301372\n",
            "    Jaccard_similarity: 0.9972203182635591\n",
            "    Kendall_tau: 0.3520949260665768\n",
            "    Manhattan_distance: 547.2715089242763\n",
            "    Euclidean_distance: 24.56037856672764\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 11, Type=LayerNorm\n",
            "    Pearson_correlation: 0.49033640190431205\n",
            "    Cosine_similarity: 0.49033661271493356\n",
            "    Jaccard_similarity: 0.997064804528434\n",
            "    Kendall_tau: 0.2712672770322931\n",
            "    Manhattan_distance: 544.0900390833282\n",
            "    Euclidean_distance: 24.860190094743\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 7, Type=Dropout\n",
            "    Pearson_correlation: 0.4775504480304648\n",
            "    Cosine_similarity: 0.4775945761321269\n",
            "    Jaccard_similarity: 0.9999544503908531\n",
            "    Kendall_tau: 0.9998750161702807\n",
            "    Manhattan_distance: 2.28931344936666\n",
            "    Euclidean_distance: 1.1983061343507972\n",
            "    Hamming_distance: 0.00039795451379907274\n",
            "Layer 25, Type=MultiHeadAttention\n",
            "    Pearson_correlation: 0.4686358152354718\n",
            "    Cosine_similarity: 0.4686593053447844\n",
            "    Jaccard_similarity: 0.9969669352297423\n",
            "    Kendall_tau: 0.30661902091808635\n",
            "    Manhattan_distance: 440.76297735383935\n",
            "    Euclidean_distance: 19.58437293469806\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 24, Type=Linear\n",
            "    Pearson_correlation: 0.46717072791639086\n",
            "    Cosine_similarity: 0.46719288875196946\n",
            "    Jaccard_similarity: 0.9970298085774518\n",
            "    Kendall_tau: 0.3029973002351658\n",
            "    Manhattan_distance: 444.0657557319313\n",
            "    Euclidean_distance: 19.755077308147666\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 22, Type=Linear\n",
            "    Pearson_correlation: 0.4281984815065214\n",
            "    Cosine_similarity: 0.4281959740179856\n",
            "    Jaccard_similarity: 0.9973266488273722\n",
            "    Kendall_tau: 0.3144077715174365\n",
            "    Manhattan_distance: 430.18597540223\n",
            "    Euclidean_distance: 19.5082701748812\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 18, Type=TransformerBlock\n",
            "    Pearson_correlation: 0.33469896685069866\n",
            "    Cosine_similarity: 0.3347030754672546\n",
            "    Jaccard_similarity: 0.9967454845222026\n",
            "    Kendall_tau: 0.22743242382568782\n",
            "    Manhattan_distance: 1112.5274003083973\n",
            "    Euclidean_distance: 51.621587789974605\n",
            "    Hamming_distance: 0.015221760152814534\n",
            "Layer 33, Type=Dropout\n",
            "    Pearson_correlation: 0.06863978789300927\n",
            "    Cosine_similarity: 0.06863971649108204\n",
            "    Jaccard_similarity: 0.9960836582646685\n",
            "    Kendall_tau: 0.05639871097905939\n",
            "    Manhattan_distance: 704.4417054121211\n",
            "    Euclidean_distance: 32.81572143213879\n",
            "    Hamming_distance: 0.013749328451757964\n",
            "Layer 17, Type=Dropout\n",
            "    Pearson_correlation: 0.035138366285390285\n",
            "    Cosine_similarity: 0.035144080882048324\n",
            "    Jaccard_similarity: 0.9971135491623244\n",
            "    Kendall_tau: 0.04027005139996461\n",
            "    Manhattan_distance: 435.72606077448097\n",
            "    Euclidean_distance: 20.954747081484072\n",
            "    Hamming_distance: 0.013411067115028753\n",
            "Layer 10, Type=Dropout\n",
            "    Pearson_correlation: 0.023249157774898232\n",
            "    Cosine_similarity: 0.02325067079182176\n",
            "    Jaccard_similarity: 0.9965982163033292\n",
            "    Kendall_tau: 0.05088407535413466\n",
            "    Manhattan_distance: 581.3003551597146\n",
            "    Euclidean_distance: 27.394375528064632\n",
            "    Hamming_distance: 0.013629942097618242\n",
            "Layer 2, Type=Dropout\n",
            "    Pearson_correlation: -0.00699718695421635\n",
            "    Cosine_similarity: -0.006968051479579282\n",
            "    Jaccard_similarity: 0.9967652762075885\n",
            "    Kendall_tau: 0.024957361538425458\n",
            "    Manhattan_distance: 953.4681381703497\n",
            "    Euclidean_distance: 46.10893101467906\n",
            "    Hamming_distance: 0.013769226177447918\n",
            "Layer 26, Type=Dropout\n",
            "    Pearson_correlation: -0.01021604119017752\n",
            "    Cosine_similarity: -0.010135965602711816\n",
            "    Jaccard_similarity: 0.9965148857803836\n",
            "    Kendall_tau: -0.025622060019226324\n",
            "    Manhattan_distance: 534.041783998071\n",
            "    Euclidean_distance: 25.35670045595111\n",
            "    Hamming_distance: 0.013769226177447918\n",
            "Most Relevant Layers: \n",
            "Layer 30, Type=Linear, Size=768 Pearson_correlation: 0.9029472215466419, Cosine_similarity: 0.9029472010687988, Jaccard_similarity: 0.9983499040684969, Kendall_tau: 0.7538914161396283, Manhattan_distance: 292.3890816339978, Euclidean_distance: 13.079640701274709, Hamming_distance: 0.015261555604194444\n",
            "Layer 32, Type=FeedForward, Size=3072 Pearson_correlation: 0.9029472215466419, Cosine_similarity: 0.9029472010687988, Jaccard_similarity: 0.9983499040684969, Kendall_tau: 0.7538914161396283, Manhattan_distance: 292.3890816339978, Euclidean_distance: 13.079640701274709, Hamming_distance: 0.015261555604194444\n",
            "Total Parameters:  3840 / 87889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainDf = pq.read_table('./Data/identifiedClosestGeneratedEvalSourcesTrainingSeed0.parquet').to_pandas(safe=False)\n",
        "testDf = pq.read_table('./Data/identifiedClosestGeneratedEvalSourcesTestSeed0.parquet').to_pandas(safe=False)\n",
        "\n",
        "evaluationSamples = len(trainDf.index.unique())\n",
        "trainSamples = len(trainDf['source'].unique())"
      ],
      "metadata": {
        "id": "MmP6Poz5lzQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for evaluation in range(evaluationSamples):\n",
        "    blendActivations(evaluation, \"Training\", 5, trainDf)\n",
        "    blendActivations(evaluation, \"Test\", 5, testDf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ca4yKvel7St",
        "outputId": "a45d9c3f-3e10-4b1b-9032-b19025e7b1d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample0 ---\n",
            "Kendall's Tau: 0.78\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9885\n",
            "Euclidean Distance: 329.3123\n",
            "Manhattan Distance: 81957.6245\n",
            "Jaccard Similarity: 0.9945\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9882\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample0 ---\n",
            "Kendall's Tau: 0.79\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9880\n",
            "Euclidean Distance: 244.6394\n",
            "Manhattan Distance: 59710.0419\n",
            "Jaccard Similarity: 0.9960\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9877\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample1 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9859\n",
            "Euclidean Distance: 212.9317\n",
            "Manhattan Distance: 50189.9565\n",
            "Jaccard Similarity: 0.9963\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9856\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample1 ---\n",
            "Kendall's Tau: 0.78\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9892\n",
            "Euclidean Distance: 375.4850\n",
            "Manhattan Distance: 93424.8464\n",
            "Jaccard Similarity: 0.9938\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9889\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample2 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9875\n",
            "Euclidean Distance: 282.5204\n",
            "Manhattan Distance: 69736.2672\n",
            "Jaccard Similarity: 0.9953\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9872\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample2 ---\n",
            "Kendall's Tau: 0.78\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9883\n",
            "Euclidean Distance: 266.7570\n",
            "Manhattan Distance: 65827.3943\n",
            "Jaccard Similarity: 0.9953\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9880\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample3 ---\n",
            "Kendall's Tau: 0.78\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9878\n",
            "Euclidean Distance: 267.6970\n",
            "Manhattan Distance: 65800.8206\n",
            "Jaccard Similarity: 0.9955\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9875\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample3 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9875\n",
            "Euclidean Distance: 282.5204\n",
            "Manhattan Distance: 69736.2672\n",
            "Jaccard Similarity: 0.9953\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9872\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample4 ---\n",
            "Kendall's Tau: 0.78\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9864\n",
            "Euclidean Distance: 168.8016\n",
            "Manhattan Distance: 37208.7581\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9860\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample4 ---\n",
            "Kendall's Tau: 0.79\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9900\n",
            "Euclidean Distance: 281.8489\n",
            "Manhattan Distance: 70053.5575\n",
            "Jaccard Similarity: 0.9953\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9897\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample5 ---\n",
            "Kendall's Tau: 0.75\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9853\n",
            "Euclidean Distance: 256.7214\n",
            "Manhattan Distance: 62982.0597\n",
            "Jaccard Similarity: 0.9961\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9849\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample5 ---\n",
            "Kendall's Tau: 0.78\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9899\n",
            "Euclidean Distance: 343.8277\n",
            "Manhattan Distance: 85650.9370\n",
            "Jaccard Similarity: 0.9943\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9896\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample6 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9875\n",
            "Euclidean Distance: 282.5204\n",
            "Manhattan Distance: 69736.2672\n",
            "Jaccard Similarity: 0.9953\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9872\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample6 ---\n",
            "Kendall's Tau: 0.78\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9868\n",
            "Euclidean Distance: 218.6823\n",
            "Manhattan Distance: 52142.9126\n",
            "Jaccard Similarity: 0.9964\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9865\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample7 ---\n",
            "Kendall's Tau: 0.78\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9878\n",
            "Euclidean Distance: 267.7191\n",
            "Manhattan Distance: 65806.7041\n",
            "Jaccard Similarity: 0.9955\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9875\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample7 ---\n",
            "Kendall's Tau: 0.78\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9865\n",
            "Euclidean Distance: 191.0834\n",
            "Manhattan Distance: 44080.5783\n",
            "Jaccard Similarity: 0.9970\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9862\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample8 ---\n",
            "Kendall's Tau: 0.78\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9884\n",
            "Euclidean Distance: 290.8944\n",
            "Manhattan Distance: 72056.7173\n",
            "Jaccard Similarity: 0.9950\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9881\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample8 ---\n",
            "Kendall's Tau: 0.76\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9840\n",
            "Euclidean Distance: 167.0534\n",
            "Manhattan Distance: 36177.0854\n",
            "Jaccard Similarity: 0.9973\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9836\n",
            "\n",
            "--- Blended Activation Similarity Scores for Training-Sample9 ---\n",
            "Kendall's Tau: 0.76\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9835\n",
            "Euclidean Distance: 166.3115\n",
            "Manhattan Distance: 35719.9467\n",
            "Jaccard Similarity: 0.9972\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9831\n",
            "\n",
            "--- Blended Activation Similarity Scores for Test-Sample9 ---\n",
            "Kendall's Tau: 0.77\n",
            "Spearman's Rho: nan\n",
            "Cosine Similarity: 0.9859\n",
            "Euclidean Distance: 212.9317\n",
            "Manhattan Distance: 50189.9565\n",
            "Jaccard Similarity: 0.9963\n",
            "Hamming Distance: 0.0447\n",
            "Pearson Correlation: 0.9856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = aggregateLayerMetricsByAllMeans(\"Train\", 5, trainDf)\n",
        "test = aggregateLayerMetricsByAllMeans(\"Test\", 5, testDf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-JfhrjNKwpS",
        "outputId": "ffcd1b74-8e41-48a0-fa43-e7c78b22536f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sorted Layers by Custom Order of Metrics for Train ---\n",
            "Layer 0, Type=Embedding\n",
            "    Pearson_correlation: -0.004689984193353045\n",
            "    Cosine_similarity: -0.004696406564585741\n",
            "    Jaccard_similarity: 0.9964562291038723\n",
            "    Kendall_tau: 0.003336207996995548\n",
            "    Manhattan_distance: 688.8366447921867\n",
            "    Euclidean_distance: 31.13431802663941\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 1, Type=Embedding\n",
            "    Pearson_correlation: nan\n",
            "    Cosine_similarity: 0.0\n",
            "    Jaccard_similarity: 1.0\n",
            "    Kendall_tau: nan\n",
            "    Manhattan_distance: 0.0\n",
            "    Euclidean_distance: 0.0\n",
            "    Hamming_distance: 0.0\n",
            "Layer 30, Type=Linear\n",
            "    Pearson_correlation: 0.9372632784359972\n",
            "    Cosine_similarity: 0.9372629978390595\n",
            "    Jaccard_similarity: 0.9986894000223476\n",
            "    Kendall_tau: 0.8027587578404332\n",
            "    Manhattan_distance: 229.96830722969145\n",
            "    Euclidean_distance: 10.397268456596214\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 31, Type=Sequential\n",
            "    Pearson_correlation: 0.9372632784359972\n",
            "    Cosine_similarity: 0.9372629978390595\n",
            "    Jaccard_similarity: 0.9986894000223476\n",
            "    Kendall_tau: 0.8027587578404332\n",
            "    Manhattan_distance: 229.96830722969145\n",
            "    Euclidean_distance: 10.397268456596214\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 32, Type=FeedForward\n",
            "    Pearson_correlation: 0.9372632784359972\n",
            "    Cosine_similarity: 0.9372629978390595\n",
            "    Jaccard_similarity: 0.9986894000223476\n",
            "    Kendall_tau: 0.8027587578404332\n",
            "    Manhattan_distance: 229.96830722969145\n",
            "    Euclidean_distance: 10.397268456596214\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 14, Type=Linear\n",
            "    Pearson_correlation: 0.8519973554784366\n",
            "    Cosine_similarity: 0.8519971408590411\n",
            "    Jaccard_similarity: 0.9985845108342115\n",
            "    Kendall_tau: 0.67731219269593\n",
            "    Manhattan_distance: 192.37335156453923\n",
            "    Euclidean_distance: 8.700394283692876\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 15, Type=Sequential\n",
            "    Pearson_correlation: 0.8519973554784366\n",
            "    Cosine_similarity: 0.8519971408590411\n",
            "    Jaccard_similarity: 0.9985845108342115\n",
            "    Kendall_tau: 0.67731219269593\n",
            "    Manhattan_distance: 192.37335156453923\n",
            "    Euclidean_distance: 8.700394283692876\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 16, Type=FeedForward\n",
            "    Pearson_correlation: 0.8519973554784366\n",
            "    Cosine_similarity: 0.8519971408590411\n",
            "    Jaccard_similarity: 0.9985845108342115\n",
            "    Kendall_tau: 0.67731219269593\n",
            "    Manhattan_distance: 192.37335156453923\n",
            "    Euclidean_distance: 8.700394283692876\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 8, Type=Linear\n",
            "    Pearson_correlation: 0.7915093762262522\n",
            "    Cosine_similarity: 0.7915076252215648\n",
            "    Jaccard_similarity: 0.9983537149608113\n",
            "    Kendall_tau: 0.6181819129401779\n",
            "    Manhattan_distance: 226.29706375214155\n",
            "    Euclidean_distance: 10.235653221576808\n",
            "    Hamming_distance: 0.015267524921901426\n",
            "Layer 9, Type=MultiHeadAttention\n",
            "    Pearson_correlation: 0.7915093762262522\n",
            "    Cosine_similarity: 0.7915076252215648\n",
            "    Jaccard_similarity: 0.9983537149608113\n",
            "    Kendall_tau: 0.6181819129401779\n",
            "    Manhattan_distance: 226.29706375214155\n",
            "    Euclidean_distance: 10.235653221576808\n",
            "    Hamming_distance: 0.015267524921901426\n",
            "Layer 37, Type=Linear\n",
            "    Pearson_correlation: 0.7908744979693452\n",
            "    Cosine_similarity: 0.9962703196390261\n",
            "    Jaccard_similarity: 0.7828159677184585\n",
            "    Kendall_tau: 0.5236329704678585\n",
            "    Manhattan_distance: 44850.10115844238\n",
            "    Euclidean_distance: 215.3930030445246\n",
            "    Hamming_distance: 0.9999801022743101\n",
            "Layer 36, Type=LayerNorm\n",
            "    Pearson_correlation: 0.7675481630140565\n",
            "    Cosine_similarity: 0.7675481768178317\n",
            "    Jaccard_similarity: 0.9979375254719745\n",
            "    Kendall_tau: 0.5774785456323547\n",
            "    Manhattan_distance: 398.4859373036712\n",
            "    Euclidean_distance: 17.968267070063973\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 34, Type=TransformerBlock\n",
            "    Pearson_correlation: 0.7660603132639703\n",
            "    Cosine_similarity: 0.7660533804743961\n",
            "    Jaccard_similarity: 0.9975907067509541\n",
            "    Kendall_tau: 0.5756511854965819\n",
            "    Manhattan_distance: 895.350007902375\n",
            "    Euclidean_distance: 40.45647919344955\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 35, Type=Sequential\n",
            "    Pearson_correlation: 0.7660603132639703\n",
            "    Cosine_similarity: 0.7660533804743961\n",
            "    Jaccard_similarity: 0.9975907067509541\n",
            "    Kendall_tau: 0.5756511854965819\n",
            "    Manhattan_distance: 895.350007902375\n",
            "    Euclidean_distance: 40.45647919344955\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 20, Type=Linear\n",
            "    Pearson_correlation: 0.7606366005412684\n",
            "    Cosine_similarity: 0.7606405121439053\n",
            "    Jaccard_similarity: 0.9980464589745338\n",
            "    Kendall_tau: 0.5648655671661736\n",
            "    Manhattan_distance: 297.78964385871484\n",
            "    Euclidean_distance: 13.583419459684483\n",
            "    Hamming_distance: 0.015265535149332432\n",
            "Layer 29, Type=GELU\n",
            "    Pearson_correlation: 0.7542952256357627\n",
            "    Cosine_similarity: 0.7586757039540071\n",
            "    Jaccard_similarity: 0.9872982185895423\n",
            "    Kendall_tau: 0.4660142318664401\n",
            "    Manhattan_distance: 761.5152613396178\n",
            "    Euclidean_distance: 18.840016340675042\n",
            "    Hamming_distance: 0.061105915593847614\n",
            "Layer 24, Type=Linear\n",
            "    Pearson_correlation: 0.7323046916320723\n",
            "    Cosine_similarity: 0.7323160294637979\n",
            "    Jaccard_similarity: 0.9983535022007605\n",
            "    Kendall_tau: 0.5340410253934463\n",
            "    Manhattan_distance: 224.10822070790627\n",
            "    Euclidean_distance: 10.200369080477923\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 25, Type=MultiHeadAttention\n",
            "    Pearson_correlation: 0.7323046916320723\n",
            "    Cosine_similarity: 0.7323160294637979\n",
            "    Jaccard_similarity: 0.9983535022007605\n",
            "    Kendall_tau: 0.5340410253934463\n",
            "    Manhattan_distance: 224.10822070790627\n",
            "    Euclidean_distance: 10.200369080477923\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 28, Type=Linear\n",
            "    Pearson_correlation: 0.715749865924677\n",
            "    Cosine_similarity: 0.7175609897171925\n",
            "    Jaccard_similarity: 0.9920073873868722\n",
            "    Kendall_tau: 0.49363330300605607\n",
            "    Manhattan_distance: 1180.5859322843748\n",
            "    Euclidean_distance: 26.706415018517127\n",
            "    Hamming_distance: 0.0611118849115546\n",
            "Layer 4, Type=Linear\n",
            "    Pearson_correlation: 0.6985175412712176\n",
            "    Cosine_similarity: 0.6985201211588165\n",
            "    Jaccard_similarity: 0.9979279897671558\n",
            "    Kendall_tau: 0.47287059326255837\n",
            "    Manhattan_distance: 319.07578525325573\n",
            "    Euclidean_distance: 14.451004412167553\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 6, Type=Linear\n",
            "    Pearson_correlation: 0.6841568510402151\n",
            "    Cosine_similarity: 0.6841559811336319\n",
            "    Jaccard_similarity: 0.9978986991369002\n",
            "    Kendall_tau: 0.4769760929396913\n",
            "    Manhattan_distance: 310.76021232677664\n",
            "    Euclidean_distance: 14.091585263555345\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 22, Type=Linear\n",
            "    Pearson_correlation: 0.6511098304402431\n",
            "    Cosine_similarity: 0.6511096879371512\n",
            "    Jaccard_similarity: 0.9978011774352862\n",
            "    Kendall_tau: 0.46558434695765394\n",
            "    Manhattan_distance: 305.34139775276134\n",
            "    Euclidean_distance: 13.762555134869178\n",
            "    Hamming_distance: 0.015267524921901426\n",
            "Layer 13, Type=GELU\n",
            "    Pearson_correlation: 0.6417057809416576\n",
            "    Cosine_similarity: 0.6444997546027461\n",
            "    Jaccard_similarity: 0.9892122228291059\n",
            "    Kendall_tau: 0.40838225202316447\n",
            "    Manhattan_distance: 642.0299180422651\n",
            "    Euclidean_distance: 16.79084077226535\n",
            "    Hamming_distance: 0.061103925821278625\n",
            "Layer 27, Type=LayerNorm\n",
            "    Pearson_correlation: 0.624865623619115\n",
            "    Cosine_similarity: 0.624865675530555\n",
            "    Jaccard_similarity: 0.9974922320732507\n",
            "    Kendall_tau: 0.4156258486526846\n",
            "    Manhattan_distance: 480.9651736470216\n",
            "    Euclidean_distance: 21.703168125739335\n",
            "    Hamming_distance: 0.015265535149332432\n",
            "Layer 12, Type=Linear\n",
            "    Pearson_correlation: 0.6226813090615938\n",
            "    Cosine_similarity: 0.6227076483959891\n",
            "    Jaccard_similarity: 0.9918216547804939\n",
            "    Kendall_tau: 0.4352235527148579\n",
            "    Manhattan_distance: 1247.8691767195207\n",
            "    Euclidean_distance: 28.238863546702383\n",
            "    Hamming_distance: 0.061105915593847614\n",
            "Layer 21, Type=Linear\n",
            "    Pearson_correlation: 0.6153786223388364\n",
            "    Cosine_similarity: 0.6153824354760374\n",
            "    Jaccard_similarity: 0.997854704104421\n",
            "    Kendall_tau: 0.42270370129960605\n",
            "    Manhattan_distance: 288.9266443784691\n",
            "    Euclidean_distance: 13.064160163125205\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 19, Type=LayerNorm\n",
            "    Pearson_correlation: 0.6078385117033734\n",
            "    Cosine_similarity: 0.6078386002856349\n",
            "    Jaccard_similarity: 0.9975008253413421\n",
            "    Kendall_tau: 0.39523813100549643\n",
            "    Manhattan_distance: 487.7890920005831\n",
            "    Euclidean_distance: 22.02621479949734\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 5, Type=Linear\n",
            "    Pearson_correlation: 0.6063583536112699\n",
            "    Cosine_similarity: 0.6063537379618855\n",
            "    Jaccard_similarity: 0.997852418519459\n",
            "    Kendall_tau: 0.3866196481553787\n",
            "    Manhattan_distance: 302.99296192529613\n",
            "    Euclidean_distance: 13.59355782757731\n",
            "    Hamming_distance: 0.015267524921901426\n",
            "Layer 7, Type=Dropout\n",
            "    Pearson_correlation: 0.6037557158001955\n",
            "    Cosine_similarity: 0.6038846905489013\n",
            "    Jaccard_similarity: 0.999978736903125\n",
            "    Kendall_tau: 0.9224399702122342\n",
            "    Manhattan_distance: 1.068644468603737\n",
            "    Euclidean_distance: 0.30483501039672833\n",
            "    Hamming_distance: 0.0005770340450086555\n",
            "Layer 3, Type=LayerNorm\n",
            "    Pearson_correlation: 0.6009175537467242\n",
            "    Cosine_similarity: 0.6009177005655685\n",
            "    Jaccard_similarity: 0.9975341177189415\n",
            "    Kendall_tau: 0.4122427367076913\n",
            "    Manhattan_distance: 495.2868873387664\n",
            "    Euclidean_distance: 22.428561770040965\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 11, Type=LayerNorm\n",
            "    Pearson_correlation: 0.5729568694235464\n",
            "    Cosine_similarity: 0.5729570445985915\n",
            "    Jaccard_similarity: 0.9973879995487334\n",
            "    Kendall_tau: 0.3827963026086259\n",
            "    Manhattan_distance: 508.77376407952517\n",
            "    Euclidean_distance: 22.976961801437696\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 23, Type=Dropout\n",
            "    Pearson_correlation: 0.5023262335655443\n",
            "    Cosine_similarity: 0.5024559536325448\n",
            "    Jaccard_similarity: 0.9999800835232608\n",
            "    Kendall_tau: 0.9224822780719881\n",
            "    Manhattan_distance: 1.0009646721592937\n",
            "    Euclidean_distance: 0.32659019904235376\n",
            "    Hamming_distance: 0.0005770340450086555\n",
            "Layer 18, Type=TransformerBlock\n",
            "    Pearson_correlation: 0.40441861132522805\n",
            "    Cosine_similarity: 0.40442599614895425\n",
            "    Jaccard_similarity: 0.9967560601093217\n",
            "    Kendall_tau: 0.278487542004391\n",
            "    Manhattan_distance: 902.2836932564327\n",
            "    Euclidean_distance: 41.1638662434258\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 17, Type=Dropout\n",
            "    Pearson_correlation: 0.013816875478284734\n",
            "    Cosine_similarity: 0.01381956394136078\n",
            "    Jaccard_similarity: 0.9971328369241625\n",
            "    Kendall_tau: -0.045206285608461974\n",
            "    Manhattan_distance: 380.0431661044433\n",
            "    Euclidean_distance: 17.918891125557685\n",
            "    Hamming_distance: 0.013970193206916452\n",
            "Layer 26, Type=Dropout\n",
            "    Pearson_correlation: 0.013364337746915079\n",
            "    Cosine_similarity: 0.013422182010867156\n",
            "    Jaccard_similarity: 0.9970080406694871\n",
            "    Kendall_tau: 0.011107804686622693\n",
            "    Manhattan_distance: 354.481261757442\n",
            "    Euclidean_distance: 16.716430819096196\n",
            "    Hamming_distance: 0.014011978430865355\n",
            "Layer 2, Type=Dropout\n",
            "    Pearson_correlation: 0.0026211289098501344\n",
            "    Cosine_similarity: 0.0026662362562734155\n",
            "    Jaccard_similarity: 0.9966840090103795\n",
            "    Kendall_tau: 0.0025349081934745144\n",
            "    Manhattan_distance: 887.7447497261707\n",
            "    Euclidean_distance: 41.910287287987\n",
            "    Hamming_distance: 0.014051773882245258\n",
            "Layer 33, Type=Dropout\n",
            "    Pearson_correlation: -0.017424009814841497\n",
            "    Cosine_similarity: -0.0174237045664175\n",
            "    Jaccard_similarity: 0.9961890940848249\n",
            "    Kendall_tau: 0.0033740059644806627\n",
            "    Manhattan_distance: 660.3698959583019\n",
            "    Euclidean_distance: 30.5172925709347\n",
            "    Hamming_distance: 0.01412937501243608\n",
            "Layer 10, Type=Dropout\n",
            "    Pearson_correlation: -0.050299882253491066\n",
            "    Cosine_similarity: -0.05030343492668536\n",
            "    Jaccard_similarity: 0.9969957776777871\n",
            "    Kendall_tau: -0.020320673339765824\n",
            "    Manhattan_distance: 406.85865694745536\n",
            "    Euclidean_distance: 19.32735007503461\n",
            "    Hamming_distance: 0.013854786397914718\n",
            "Most Relevant Layers: \n",
            "Layer 30, Type=Linear, Size=768 Pearson_correlation: 0.9372632784359972, Cosine_similarity: 0.9372629978390595, Jaccard_similarity: 0.9986894000223476, Kendall_tau: 0.8027587578404332, Manhattan_distance: 229.96830722969145, Euclidean_distance: 10.397268456596214, Hamming_distance: 0.015263545376763438\n",
            "Layer 32, Type=FeedForward, Size=3072 Pearson_correlation: 0.9372632784359972, Cosine_similarity: 0.9372629978390595, Jaccard_similarity: 0.9986894000223476, Kendall_tau: 0.8027587578404332, Manhattan_distance: 229.96830722969145, Euclidean_distance: 10.397268456596214, Hamming_distance: 0.015263545376763438\n",
            "Layer 14, Type=Linear, Size=768 Pearson_correlation: 0.8519973554784366, Cosine_similarity: 0.8519971408590411, Jaccard_similarity: 0.9985845108342115, Kendall_tau: 0.67731219269593, Manhattan_distance: 192.37335156453923, Euclidean_distance: 8.700394283692876, Hamming_distance: 0.015263545376763438\n",
            "Layer 16, Type=FeedForward, Size=3072 Pearson_correlation: 0.8519973554784366, Cosine_similarity: 0.8519971408590411, Jaccard_similarity: 0.9985845108342115, Kendall_tau: 0.67731219269593, Manhattan_distance: 192.37335156453923, Euclidean_distance: 8.700394283692876, Hamming_distance: 0.015263545376763438\n",
            "Total Parameters:  7680 / 87889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sorted Layers by Custom Order of Metrics for Test ---\n",
            "Layer 0, Type=Embedding\n",
            "    Pearson_correlation: -0.005905855675768805\n",
            "    Cosine_similarity: -0.005912704143366351\n",
            "    Jaccard_similarity: 0.9964509635520533\n",
            "    Kendall_tau: -0.00586307306983533\n",
            "    Manhattan_distance: 693.4668223240162\n",
            "    Euclidean_distance: 31.114497691520018\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 1, Type=Embedding\n",
            "    Pearson_correlation: nan\n",
            "    Cosine_similarity: 0.0\n",
            "    Jaccard_similarity: 1.0\n",
            "    Kendall_tau: nan\n",
            "    Manhattan_distance: 0.0\n",
            "    Euclidean_distance: 0.0\n",
            "    Hamming_distance: 0.0\n",
            "Layer 30, Type=Linear\n",
            "    Pearson_correlation: 0.9508142093795439\n",
            "    Cosine_similarity: 0.9508139224434581\n",
            "    Jaccard_similarity: 0.9987528865011719\n",
            "    Kendall_tau: 0.8358010127281867\n",
            "    Manhattan_distance: 212.46560704695094\n",
            "    Euclidean_distance: 9.541328343282302\n",
            "    Hamming_distance: 0.015265535149332432\n",
            "Layer 31, Type=Sequential\n",
            "    Pearson_correlation: 0.9508142093795439\n",
            "    Cosine_similarity: 0.9508139224434581\n",
            "    Jaccard_similarity: 0.9987528865011719\n",
            "    Kendall_tau: 0.8358010127281867\n",
            "    Manhattan_distance: 212.46560704695094\n",
            "    Euclidean_distance: 9.541328343282302\n",
            "    Hamming_distance: 0.015265535149332432\n",
            "Layer 32, Type=FeedForward\n",
            "    Pearson_correlation: 0.9508142093795439\n",
            "    Cosine_similarity: 0.9508139224434581\n",
            "    Jaccard_similarity: 0.9987528865011719\n",
            "    Kendall_tau: 0.8358010127281867\n",
            "    Manhattan_distance: 212.46560704695094\n",
            "    Euclidean_distance: 9.541328343282302\n",
            "    Hamming_distance: 0.015265535149332432\n",
            "Layer 14, Type=Linear\n",
            "    Pearson_correlation: 0.8872421695543263\n",
            "    Cosine_similarity: 0.8872417152950348\n",
            "    Jaccard_similarity: 0.9987421658165732\n",
            "    Kendall_tau: 0.7104303183018373\n",
            "    Manhattan_distance: 164.20653617663461\n",
            "    Euclidean_distance: 7.421817153140038\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 15, Type=Sequential\n",
            "    Pearson_correlation: 0.8872421695543263\n",
            "    Cosine_similarity: 0.8872417152950348\n",
            "    Jaccard_similarity: 0.9987421658165732\n",
            "    Kendall_tau: 0.7104303183018373\n",
            "    Manhattan_distance: 164.20653617663461\n",
            "    Euclidean_distance: 7.421817153140038\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 16, Type=FeedForward\n",
            "    Pearson_correlation: 0.8872421695543263\n",
            "    Cosine_similarity: 0.8872417152950348\n",
            "    Jaccard_similarity: 0.9987421658165732\n",
            "    Kendall_tau: 0.7104303183018373\n",
            "    Manhattan_distance: 164.20653617663461\n",
            "    Euclidean_distance: 7.421817153140038\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 8, Type=Linear\n",
            "    Pearson_correlation: 0.8073548988897722\n",
            "    Cosine_similarity: 0.8073527563274709\n",
            "    Jaccard_similarity: 0.9983848457085491\n",
            "    Kendall_tau: 0.62913526160843\n",
            "    Manhattan_distance: 213.08078710128416\n",
            "    Euclidean_distance: 9.627413139414077\n",
            "    Hamming_distance: 0.015265535149332432\n",
            "Layer 9, Type=MultiHeadAttention\n",
            "    Pearson_correlation: 0.8073548988897722\n",
            "    Cosine_similarity: 0.8073527563274709\n",
            "    Jaccard_similarity: 0.9983848457085491\n",
            "    Kendall_tau: 0.62913526160843\n",
            "    Manhattan_distance: 213.08078710128416\n",
            "    Euclidean_distance: 9.627413139414077\n",
            "    Hamming_distance: 0.015265535149332432\n",
            "Layer 37, Type=Linear\n",
            "    Pearson_correlation: 0.8042724933604125\n",
            "    Cosine_similarity: 0.9964916566592761\n",
            "    Jaccard_similarity: 0.7754316787916308\n",
            "    Kendall_tau: 0.5399647969626044\n",
            "    Manhattan_distance: 46881.91830379064\n",
            "    Euclidean_distance: 223.87514991451462\n",
            "    Hamming_distance: 0.9999840818194482\n",
            "Layer 36, Type=LayerNorm\n",
            "    Pearson_correlation: 0.7843466857115459\n",
            "    Cosine_similarity: 0.784346669649268\n",
            "    Jaccard_similarity: 0.9979601343224157\n",
            "    Kendall_tau: 0.5989420305687233\n",
            "    Manhattan_distance: 386.62259654744537\n",
            "    Euclidean_distance: 17.439879841024133\n",
            "    Hamming_distance: 0.015267524921901426\n",
            "Layer 34, Type=TransformerBlock\n",
            "    Pearson_correlation: 0.7826563552822361\n",
            "    Cosine_similarity: 0.7826500681026871\n",
            "    Jaccard_similarity: 0.9976250968210432\n",
            "    Kendall_tau: 0.5937107893403276\n",
            "    Manhattan_distance: 861.1145748079032\n",
            "    Euclidean_distance: 38.85983433600793\n",
            "    Hamming_distance: 0.015265535149332432\n",
            "Layer 35, Type=Sequential\n",
            "    Pearson_correlation: 0.7826563552822361\n",
            "    Cosine_similarity: 0.7826500681026871\n",
            "    Jaccard_similarity: 0.9976250968210432\n",
            "    Kendall_tau: 0.5937107893403276\n",
            "    Manhattan_distance: 861.1145748079032\n",
            "    Euclidean_distance: 38.85983433600793\n",
            "    Hamming_distance: 0.015265535149332432\n",
            "Layer 20, Type=Linear\n",
            "    Pearson_correlation: 0.7727155992400986\n",
            "    Cosine_similarity: 0.7727174862248669\n",
            "    Jaccard_similarity: 0.9980725577607437\n",
            "    Kendall_tau: 0.5775794243512081\n",
            "    Manhattan_distance: 294.18341298923826\n",
            "    Euclidean_distance: 13.27340063217935\n",
            "    Hamming_distance: 0.015265535149332432\n",
            "Layer 29, Type=GELU\n",
            "    Pearson_correlation: 0.7680636639473302\n",
            "    Cosine_similarity: 0.7722048955447839\n",
            "    Jaccard_similarity: 0.9876049057286188\n",
            "    Kendall_tau: 0.4782568348732366\n",
            "    Manhattan_distance: 743.0339019282793\n",
            "    Euclidean_distance: 18.417742661610895\n",
            "    Hamming_distance: 0.061105915593847614\n",
            "Layer 24, Type=Linear\n",
            "    Pearson_correlation: 0.7458699265955588\n",
            "    Cosine_similarity: 0.7458831819262757\n",
            "    Jaccard_similarity: 0.9983711184750094\n",
            "    Kendall_tau: 0.5449804523752462\n",
            "    Manhattan_distance: 217.0932523866984\n",
            "    Euclidean_distance: 9.870019908297417\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 25, Type=MultiHeadAttention\n",
            "    Pearson_correlation: 0.7458699265955588\n",
            "    Cosine_similarity: 0.7458831819262757\n",
            "    Jaccard_similarity: 0.9983711184750094\n",
            "    Kendall_tau: 0.5449804523752462\n",
            "    Manhattan_distance: 217.0932523866984\n",
            "    Euclidean_distance: 9.870019908297417\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 28, Type=Linear\n",
            "    Pearson_correlation: 0.7334376235642643\n",
            "    Cosine_similarity: 0.7351414264051072\n",
            "    Jaccard_similarity: 0.9923185871707648\n",
            "    Kendall_tau: 0.5109374433111493\n",
            "    Manhattan_distance: 1153.570042028078\n",
            "    Euclidean_distance: 26.09450756725023\n",
            "    Hamming_distance: 0.061113874684123604\n",
            "Layer 4, Type=Linear\n",
            "    Pearson_correlation: 0.7144275890976541\n",
            "    Cosine_similarity: 0.7144292977786052\n",
            "    Jaccard_similarity: 0.9979916900275434\n",
            "    Kendall_tau: 0.4955132215749779\n",
            "    Manhattan_distance: 306.09718383468663\n",
            "    Euclidean_distance: 13.75168783883447\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 6, Type=Linear\n",
            "    Pearson_correlation: 0.6941121721617096\n",
            "    Cosine_similarity: 0.6941126749867126\n",
            "    Jaccard_similarity: 0.9979900611386275\n",
            "    Kendall_tau: 0.4842891243628439\n",
            "    Manhattan_distance: 303.65054754375853\n",
            "    Euclidean_distance: 13.722703677446807\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 22, Type=Linear\n",
            "    Pearson_correlation: 0.6661311427778311\n",
            "    Cosine_similarity: 0.6661297034121378\n",
            "    Jaccard_similarity: 0.9978690337058953\n",
            "    Kendall_tau: 0.4815921837444659\n",
            "    Manhattan_distance: 297.9677367754293\n",
            "    Euclidean_distance: 13.466216052378911\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 13, Type=GELU\n",
            "    Pearson_correlation: 0.6587640646140149\n",
            "    Cosine_similarity: 0.6614255237424527\n",
            "    Jaccard_similarity: 0.9894812979345453\n",
            "    Kendall_tau: 0.41605405261418243\n",
            "    Manhattan_distance: 625.8850345852218\n",
            "    Euclidean_distance: 16.24413032042049\n",
            "    Hamming_distance: 0.061099946276140626\n",
            "Layer 27, Type=LayerNorm\n",
            "    Pearson_correlation: 0.642387501069761\n",
            "    Cosine_similarity: 0.6423875433993637\n",
            "    Jaccard_similarity: 0.997629326887985\n",
            "    Kendall_tau: 0.4229928634841033\n",
            "    Manhattan_distance: 472.4097966985246\n",
            "    Euclidean_distance: 21.292306087601553\n",
            "    Hamming_distance: 0.015265535149332432\n",
            "Layer 12, Type=Linear\n",
            "    Pearson_correlation: 0.6361795678153084\n",
            "    Cosine_similarity: 0.6361833005168509\n",
            "    Jaccard_similarity: 0.9917850837317527\n",
            "    Kendall_tau: 0.44642902397344963\n",
            "    Manhattan_distance: 1215.069038112804\n",
            "    Euclidean_distance: 27.45389654408109\n",
            "    Hamming_distance: 0.061107905366416616\n",
            "Layer 21, Type=Linear\n",
            "    Pearson_correlation: 0.6296603596852712\n",
            "    Cosine_similarity: 0.6296638324885934\n",
            "    Jaccard_similarity: 0.9978291505568027\n",
            "    Kendall_tau: 0.42430877427361724\n",
            "    Manhattan_distance: 284.4899570613301\n",
            "    Euclidean_distance: 12.747396906528431\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 7, Type=Dropout\n",
            "    Pearson_correlation: 0.6251185702582\n",
            "    Cosine_similarity: 0.6252405092298066\n",
            "    Jaccard_similarity: 0.9999787151403412\n",
            "    Kendall_tau: 0.9224494010579555\n",
            "    Manhattan_distance: 1.0697382331838468\n",
            "    Euclidean_distance: 0.3045955195302492\n",
            "    Hamming_distance: 0.0005770340450086555\n",
            "Layer 5, Type=Linear\n",
            "    Pearson_correlation: 0.621079710711012\n",
            "    Cosine_similarity: 0.6210812636630296\n",
            "    Jaccard_similarity: 0.997867273165415\n",
            "    Kendall_tau: 0.41731543319236913\n",
            "    Manhattan_distance: 287.7630094338864\n",
            "    Euclidean_distance: 13.014267789923503\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 19, Type=LayerNorm\n",
            "    Pearson_correlation: 0.617773371146568\n",
            "    Cosine_similarity: 0.6177734774284676\n",
            "    Jaccard_similarity: 0.9975639602260795\n",
            "    Kendall_tau: 0.4107736492934557\n",
            "    Manhattan_distance: 483.6343688407918\n",
            "    Euclidean_distance: 21.765410191090414\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 3, Type=LayerNorm\n",
            "    Pearson_correlation: 0.6025217858933927\n",
            "    Cosine_similarity: 0.6025219004096635\n",
            "    Jaccard_similarity: 0.9975018691100814\n",
            "    Kendall_tau: 0.4047978678993613\n",
            "    Manhattan_distance: 498.2591075978057\n",
            "    Euclidean_distance: 22.406919130746495\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 11, Type=LayerNorm\n",
            "    Pearson_correlation: 0.5781985431794844\n",
            "    Cosine_similarity: 0.5781986828180056\n",
            "    Jaccard_similarity: 0.99751058104838\n",
            "    Kendall_tau: 0.3756791945054775\n",
            "    Manhattan_distance: 506.5588023198799\n",
            "    Euclidean_distance: 22.86527038775344\n",
            "    Hamming_distance: 0.015263545376763438\n",
            "Layer 23, Type=Dropout\n",
            "    Pearson_correlation: 0.4666717426315481\n",
            "    Cosine_similarity: 0.46680658049563883\n",
            "    Jaccard_similarity: 0.9999801643989061\n",
            "    Kendall_tau: 0.9224633143086773\n",
            "    Manhattan_distance: 0.9969000400909664\n",
            "    Euclidean_distance: 0.32733514541679526\n",
            "    Hamming_distance: 0.0005770340450086555\n",
            "Layer 18, Type=TransformerBlock\n",
            "    Pearson_correlation: 0.4116667272975234\n",
            "    Cosine_similarity: 0.41167289671278773\n",
            "    Jaccard_similarity: 0.9968437898767389\n",
            "    Kendall_tau: 0.2872579585524024\n",
            "    Manhattan_distance: 896.7462105307461\n",
            "    Euclidean_distance: 40.84421513298188\n",
            "    Hamming_distance: 0.015261555604194444\n",
            "Layer 17, Type=Dropout\n",
            "    Pearson_correlation: 0.025380045918257037\n",
            "    Cosine_similarity: 0.025382358093357087\n",
            "    Jaccard_similarity: 0.9971506419381825\n",
            "    Kendall_tau: -0.0259916448672809\n",
            "    Manhattan_distance: 367.7766716497082\n",
            "    Euclidean_distance: 17.44039889303071\n",
            "    Hamming_distance: 0.013972182979485446\n",
            "Layer 26, Type=Dropout\n",
            "    Pearson_correlation: 0.020574373379433714\n",
            "    Cosine_similarity: 0.02064673873045686\n",
            "    Jaccard_similarity: 0.9971055824845401\n",
            "    Kendall_tau: 0.032061231277992694\n",
            "    Manhattan_distance: 349.27157955088455\n",
            "    Euclidean_distance: 16.577187844299008\n",
            "    Hamming_distance: 0.014011978430865355\n",
            "Layer 2, Type=Dropout\n",
            "    Pearson_correlation: 0.017500858352128758\n",
            "    Cosine_similarity: 0.017545013009626947\n",
            "    Jaccard_similarity: 0.9966695336021928\n",
            "    Kendall_tau: 0.010877659762032966\n",
            "    Manhattan_distance: 876.750829188208\n",
            "    Euclidean_distance: 41.39836575821563\n",
            "    Hamming_distance: 0.014049784109676264\n",
            "Layer 33, Type=Dropout\n",
            "    Pearson_correlation: -0.022757183733860965\n",
            "    Cosine_similarity: -0.02275716216754583\n",
            "    Jaccard_similarity: 0.9961020983777322\n",
            "    Kendall_tau: 0.007955721618044035\n",
            "    Manhattan_distance: 661.9435382966692\n",
            "    Euclidean_distance: 30.543056469763364\n",
            "    Hamming_distance: 0.014131364785005074\n",
            "Layer 10, Type=Dropout\n",
            "    Pearson_correlation: -0.051101051878860826\n",
            "    Cosine_similarity: -0.0511030599359697\n",
            "    Jaccard_similarity: 0.9969905965943522\n",
            "    Kendall_tau: -0.024741514171908202\n",
            "    Manhattan_distance: 397.6289078546521\n",
            "    Euclidean_distance: 18.93099177829449\n",
            "    Hamming_distance: 0.013852796625345724\n",
            "Most Relevant Layers: \n",
            "Layer 30, Type=Linear, Size=768 Pearson_correlation: 0.9508142093795439, Cosine_similarity: 0.9508139224434581, Jaccard_similarity: 0.9987528865011719, Kendall_tau: 0.8358010127281867, Manhattan_distance: 212.46560704695094, Euclidean_distance: 9.541328343282302, Hamming_distance: 0.015265535149332432\n",
            "Layer 32, Type=FeedForward, Size=3072 Pearson_correlation: 0.9508142093795439, Cosine_similarity: 0.9508139224434581, Jaccard_similarity: 0.9987528865011719, Kendall_tau: 0.8358010127281867, Manhattan_distance: 212.46560704695094, Euclidean_distance: 9.541328343282302, Hamming_distance: 0.015265535149332432\n",
            "Layer 14, Type=Linear, Size=768 Pearson_correlation: 0.8872421695543263, Cosine_similarity: 0.8872417152950348, Jaccard_similarity: 0.9987421658165732, Kendall_tau: 0.7104303183018373, Manhattan_distance: 164.20653617663461, Euclidean_distance: 7.421817153140038, Hamming_distance: 0.015261555604194444\n",
            "Layer 16, Type=FeedForward, Size=3072 Pearson_correlation: 0.8872421695543263, Cosine_similarity: 0.8872417152950348, Jaccard_similarity: 0.9987421658165732, Kendall_tau: 0.7104303183018373, Manhattan_distance: 164.20653617663461, Euclidean_distance: 7.421817153140038, Hamming_distance: 0.015261555604194444\n",
            "Total Parameters:  7680 / 87889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test for best sentence split"
      ],
      "metadata": {
        "id": "HlPncAtvElwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q stanza\n",
        "import stanza\n",
        "# Suppress logging from stanza\n",
        "nlp = stanza.Pipeline('en', verbose=False)\n",
        "stanza.download('en', verbose=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUjK_ZxLEqMa",
        "outputId": "4407401e-fecc-48c9-d316-3f9f4b6a11f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "\n",
        "# Patterns for data cleaning\n",
        "patterns = [\n",
        "    (re.compile(r'\\('), ''),  # Remove open brackets\n",
        "    (re.compile(r'\\)'), ''),  # Remove close brackets\n",
        "    (re.compile(r\"=.*=\"), ''),  # Remove headings\n",
        "    (re.compile(r\"<unk>\"), ''),  # Remove unknown tokens\n",
        "    (re.compile(r\"-\"), ' '),  # Exchange hyphens for spaces\n",
        "    (re.compile(r\"[^\\w.' ]\"), ''),  # Remove non-alphanumeric, except specific symbols\n",
        "]\n",
        "\n",
        "def load_data(filepath):\n",
        "    try:\n",
        "        with open(filepath, 'r') as f:\n",
        "            return f.read()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file {filepath} was not found.\")\n",
        "        return \"\"\n",
        "    except IOError as e:\n",
        "        print(f\"Error reading the file {filepath}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def create_sources(data):\n",
        "    heading_pattern = r'(?<!\\= )= [^=]+ =(?!= )'\n",
        "\n",
        "    with open(data, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "        titles, sources = [], []\n",
        "        sourceStartLine = \"\"\n",
        "        titleLine = \"\"\n",
        "        source = []\n",
        "        lastSource = \"\"\n",
        "        count = 0\n",
        "        title = False\n",
        "\n",
        "        for line in lines:\n",
        "            #if ((\" = \" in titleLine and bool(re.search(heading_pattern, titleLine)))):\n",
        "            #   print(f\"FOUND!!!\\n.SourceStart.{sourceStartLine}.Title.{titleLine}.Line.{line}\")\n",
        "            if (sourceStartLine == \"\\\" \\n\" and (\" = \" in titleLine and bool(re.search(heading_pattern, titleLine))) and line == \" \\n\"):\n",
        "                title = titleLine.replace('= ', '').replace(' =', '').replace(' <unk> ', '').replace('\\n', '')\n",
        "                titles.append(title)\n",
        "                tempSource = (\"\").join(source)\n",
        "                for pattern, replacement in patterns:\n",
        "                    tempSource = pattern.sub(replacement, tempSource)\n",
        "                if(lastSource != \"\"):\n",
        "                    sources.append(lastSource)\n",
        "                lastSource = tempSource\n",
        "                source = []\n",
        "                count += 1\n",
        "\n",
        "            #title = (sourceStartLine == \"\\\" \\n\" and (\" = \" in titleLine and bool(re.search(heading_pattern, titleLine))) and line == \" \\n\")\n",
        "            source.append(line)\n",
        "            # if(count < 100):\n",
        "            #     print(sourceStartLine == \"\\\"\", \" = \" in titleLine, bool(re.search(heading_pattern, titleLine)), line == \"\")\n",
        "            #     print(count, title, sourceStartLine, titleLine, line)\n",
        "            sourceStartLine = titleLine\n",
        "            titleLine = line\n",
        "\n",
        "    sources = sources[1:]\n",
        "\n",
        "    print(titles)\n",
        "\n",
        "    # Print the adjusted titles\n",
        "    # for source, title in zip(sources, titles):\n",
        "    #     print(f\"Title: {title}\")\n",
        "    #     print(f\"SourceStart: {source[:50]}\")\n",
        "    #     print(f\"SourceEnd: {source[-50:]}\\n\")\n",
        "\n",
        "    return titles, sources\n",
        "\n",
        "def create_source_structure(data, name, titles, sources, sentenceChecker):\n",
        "    source_structure = []\n",
        "    found_sentences_count = 0\n",
        "\n",
        "    # Write sources to the output file\n",
        "    with open(f\"{name}_sources.txt\", 'w', encoding='utf-8') as f:\n",
        "        for source_number, (title, source) in enumerate(zip(titles, sources)):\n",
        "            print(f\"Currently at source number: {source_number}/{len(titles)} ({title})\")\n",
        "            f.write(f\"{title}:\\n\")  # Write the title\n",
        "            source_structure.append([])\n",
        "\n",
        "            # Split data into sentences and sequences\n",
        "            sentences, found_sentences, _ = split_data(source, sentenceChecker)\n",
        "            found_sentences_count += len(found_sentences)\n",
        "\n",
        "            #_, sequences = create_sequences(sentences)\n",
        "\n",
        "            for sentence_number, sentence in enumerate(sentences):\n",
        "                # Append the sequence to the source structure\n",
        "                source_structure[source_number].append(sentence)\n",
        "                # Write to file\n",
        "                f.write(f\"[{source_number}:{sentence_number}] {sentence}\\n\")\n",
        "\n",
        "            f.write('\\n')  # Add a newline after each title for better readability\n",
        "\n",
        "    return titles, sources, source_structure, found_sentences_count\n",
        "\n",
        "def split_data(data, sentenceChecker, num_sentences=-1):\n",
        "    found_sentences = sentenceChecker(data) if num_sentences == -1 else sentenceChecker(data)[:num_sentences]\n",
        "    if sentenceChecker == nlp:\n",
        "        found_sentences = [sentence.text for sentence in found_sentences.sentences]\n",
        "    filtered_sentences = [sentence for sentence in found_sentences if bool(re.search(r'\\.\\s*$', sentence)) and len(sentence) > 3] #Make sure the sentence ends with a point\n",
        "    words = sorted({word for sent in filtered_sentences for word in sent.split()})\n",
        "    words.insert(0, \"\")  # Add an empty string for padding\n",
        "    return filtered_sentences, found_sentences, words\n",
        "\n",
        "train_text = load_data(\"./train.txt\")\n",
        "#print(train_text[:10000])\n",
        "train_titles, train_sources = create_sources(\"./train.txt\")\n",
        "start_time = timeit.default_timer()\n",
        "titles, sources, nlp_train_source_structure, nlp_train_found = create_source_structure(train_text, \"train-nlp\", train_titles, train_sources, nlp)\n",
        "nlp_time = timeit.default_timer() - start_time\n",
        "print(f\"NLP Train Sentence Creation Time: {nlp_time:.2f} seconds\\n\")\n",
        "\n",
        "test_text = load_data(\"./test.txt\")\n",
        "test_titles, test_sources = create_sources(\"./test.txt\")\n",
        "start_time = timeit.default_timer()\n",
        "titles, sources, nlp_test_source_structure, nlp_test_found = create_source_structure(test_text, \"test-nlp\", test_titles, test_sources, nlp)\n",
        "nlp_time = timeit.default_timer() - start_time\n",
        "print(f\"NLP Test Sentence Creation Time: {nlp_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ-wxykSEs5H",
        "outputId": "2404240f-e2ea-4895-fd6a-5905b1d30045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' 2013 â 14 York City F.C. season ', ' Big Boy ( song ) ', ' The Remix ( Lady Gaga album ) ', \" New Year 's Eve ( Up All Night ) \", ' Geopyxis carbonaria ', ' Cyclone Graham ', ' M @-@ 108 ( Michigan highway ) ', ' Simon Bradstreet ', ' Ghost in the Shell : Stand Alone Complex - Solid State Society ', ' Tropical Storm Jose ( 2005 ) ', ' California State Route 243 ', ' Dave Sisler ', \" Gambia women 's national football team \", ' Katherine Pulaski ', ' Thom Darden ', ' Vitamin D ( Glee ) ', ' Fastra II ', \" Livin ' the Dream \", ' HMS Marlborough ( 1912 ) ', ' The Sixth Extinction ', ' Greens Ledge Light ', ' Hannah Primrose , Countess of Rosebery ', ' Mycena galericulata ', ' Chagas disease ', ' LiSA ( Japanese musician , born 1987 ) ', ' Sorraia ', ' Varanasi ', ' Missouri River ', ' Magadheera ', ' HMS Boreas () ', ' Cape lobster ', ' Kitsune ', ' Paul Thomas Anderson ', ' General aviation in the United Kingdom ', ' Gaboon viper ', ' Lloyd Mathews ', ' Jeremi WiÅniowiecki ', ' 1939 Pacific hurricane season ', \" St Peulan 's Church , Llanbeulan \", ' Tawny nurse shark ', ' Tropical Storm Abby ( 1964 ) ', ' McAllister Tower Apartments ', ' Odaenathus ', ' Forbidden Fruit ( J. Cole song ) ', ' Arihant @-@ class submarine ', ' Charles @-@ Valentin Alkan ', ' Jenova Chen ', ' Mitsuyo Maeda ', ' AIL Storm ', ' Gold Beach ', ' HMS Hostile () ', ' Stanley Green ', ' Hope Highway ', ' Jin â Song Wars ', ' Treaty of Ciudad JuÃ¡rez ', ' Max Mosley ', ' Superman : Escape from Krypton ', ' Nebraska Highway 88 ', ' Hurricane Abby ( 1960 ) ', ' Haifa ', ' Robbie Fowler ', ' Protein ', ' Tupolev Tu @-@ 12 ', ' Freakum Dress ', ' Plum cake ', ' Aerith Gainsborough ', ' Nina Simone ', \" Simon de Montfort 's Parliament \", ' Comair Flight 5191 ', ' Ode to a Nightingale ', ' The Crab with the Golden Claws ', ' President Evil ', ' Tessa NoÃ«l ', ' The Secret of Monkey Island ', ' Gerard ( archbishop of York ) ', ' Typhoon Imbudo ', ' Rio de Janeiro bid for the 2016 Summer Olympics ', ' Action of 13 September 1810 ', ' Amanita muscaria ', ' Civilian Public Service ', ' Ohio State Route 319 ', ' 1973 Atlantic hurricane season ', ' Mutinus elegans ', ' Winston Tunnel ', ' Leg before wicket ', ' Corn crake ', ' Chris Turner ( American football ) ', ' 7 Independent Company ( Rhodesia ) ', ' Bassline ( Chris Brown song ) ', \" Monty Can 't Buy Me Love \", \" Ha ' K 'in Xook \", ' Fern Hobbs ', ' Gregory Helms ', ' 2007 Hawaii Bowl ', ' Fernando Torres ', ' The Goat Puzzle ', ' The Moth ( Lost ) ', ' The Tramp Dentists ', \" O 'Brien @-@ class destroyer \", \" Don 't Take It, Babe , It Just Ain 't Your Story \", ' Far Away Places ( Mad Men ) ', ' Old Pine Church ', ' Toronto Magnetic and Meteorological Observatory ', ' Ten Commandments in Catholic theology ', ' Silver Bullet ( roller coaster ) ', ' Mexico City Metropolitan Cathedral ', ' Cell nucleus ', ' Lady in the Lake trial ', ' Languedoc @-@ Roussillon wine ', ' Youth on the Prow , and Pleasure at the Helm ', ' Tower Building of the Little Rock Arsenal ', \" God 's Choice \", ' West End Girls ', ' Italian cruiser Aretusa ', ' Henry HoÊ»olulu Pitman ', ' Cyclone Herbie ', ' HMS Comet () ', ' Berhtwald ', ' The General in His Labyrinth ', ' Mothers of the Disappeared ', ' Hurricane Tanya ( 1995 ) ', ' 1998 National League Wild Card tie @-@ breaker game ', ' Arbeideren ( Hamar ) ', ' Galveston , Texas ', ' Copia ( museum ) ', ' Hugh Foliot ', ' USS Breese ( DD @-@ 122 ) ', ' Lisa the Simpson ', ' Marauders ( Star Trek : Enterprise ) ', ' Charmbracelet ', ' Baltimore mayoral election , 1999 ', ' Oribi ', ' Domnall mac Murchada ', ' Snow ( visual novel ) ', ' World War I Memorial ( East Providence , Rhode Island ) ', ' Exploration of Jupiter ', ' Something Borrowed ( Torchwood ) ', ' Kalyanasundara ', ' Rachel Green ', ' New York State Route 38 ', ' Valkyria Chronicles III ', ' Oldham ', ' Norsk Spisevognselskap ', ' The Son Also Draws ', ' Trinsey v. Pennsylvania ', ' A Month in the Country ( film ) ', ' Hadji Ali ', ' Frank Slide ', ' Soviet cruiser Krasnyi Kavkaz ', ' Star ', ' Lost Horizons ( Lemon Jelly album ) ', ' Irresistible ( The X @-@ Files ) ', ' Lock Haven , Pennsylvania ', ' RÃ©union ibis ', ' M @-@ 114 ( Michigan highway ) ', ' Liu Kang ', ' Architecture of the Song dynasty ', ' Burning of women in England ', ' Loverboy ( Mariah Carey song ) ', ' Blackwyche ', ' LÃ¡grimas CÃ¡lidas ', ' Marshall Applewhite ', ' Ed Barrow ', ' August ( Fringe ) ', ' White Dog ( Gary novel ) ', ' Waterfall Gully , South Australia ', ' SMS ZrÃ­nyi ', ' Moment of Surrender ', ' Paranthodon ', ' 1940 Atlantic hurricane season ', ' Portuguese ironclad Vasco da Gama ', ' Biddenden Maids ', ' 1806 Great Coastal hurricane ', \" Don 't You Wanna Stay \", ' Devin Townsend ', ' England national rugby union team ', ' Jane Dudley , Duchess of Northumberland ', ' Spanish Hill ', ' In Bloom ', ' Mole cricket ', ' Wales national rugby union team ', ' USS Illinois ( BB @-@ 7 ) ', \" Christmas 1994 nor 'easter \", ' Yamaha NS @-@ 10 ', ' I Am Unicorn ', ' Hurricane Felicia ( 2009 ) ', ' Yo @-@ Yo ( Nicola Roberts song ) ', ' The Food Album ', ' Islais Creek ', ' Super Mario Land ', ' Ceratopsia ', ' Santa @-@ Fe ( Bob Dylan song ) ', ' Strand , London ', ' Live & Kicking ', ' Weather buoy ', ' Elgin Cathedral ', ' Hi , Infidelity ', ' Polka Party ! ', ' Van Morrison : Too Late to Stop Now ', ' True Blue ( Madonna song ) ', ' Dover Athletic F.C. ', ' June 1941 uprising in eastern Herzegovina ', ' Stay @-@ at @-@ home dad ', ' Stephanolepis cirrhifer ', ' M @-@ 47 ( Michigan highway ) ', ' G.I. Joe : Retaliation ', ' Perry the', ' 1986 Peach Bowl ', ' Midge ( Barbie ) ', ' Tina Fey ', ' Westminster Assembly ', ' Boise National Forest ', ' History of Bradford City', ' Tropical Storm Domoina ', ' Corpus Christi Bay ', ' Canning Dam ', ' Tropical Storm Brenda ( 1960 ) ', ' Giacomo Meyerbeer ', ' First Battle of Maryang San ', ' Curtis Woodhouse ', ' Mount Jackson ( Antarctica ) ', ' Stanley Price Weir ', \" Rosemary 's Baby ( 30 Rock ) \", ' 1981 European Cup Final ', \" Kir 'Shara \", ' Acute myeloid leukemia ', ' Perfect Dark ( 2010 video game ) ', ' Sentence spacing ', ' Edward Creutz ', ' Cambodian Campaign ', ' Music of Chrono Cross ', ' Rob Howard ', ' Gertrude Barrows Bennett ', ' Jifna ', ' S.R. 819 ', ' SMS Markgraf ', ' Polish culture during World War II ', ' Antimony ', ' Tropical Storm Olaf ( 1997 ) ', ' Hugh Walpole ', ' Richard Cresswell ', ' Who Am I ( Casting Crowns song ) ', ' Josce de Dinan ', ' Erving Goffman ', ' Key ( basketball ) ', ' 130th Engineer Brigade ( United States ) ', ' Stop ! ! Hibari @-@ kun ! ', ' Pattycake ( gorilla ) ', ' Sister Wives ', ' SM U @-@ 3 ( Austria @-@ Hungary ) ', \" St Caffo 's Church , Llangaffo \", ' Old Baltimore Pike ', ' Tiber Oil Field ', ' Battle of Hubbardton ', \" Jane 's Attack Squadron \", ' Hoyt Wilhelm ', ' Journey ( 2012 video game ) ', ' Astraeus hygrometricus ', ' Sinclair Sovereign ', ' Ceres ( dwarf planet ) ', ' French cruiser Sully ', ' Washington State Route 516 ', ' Sweet Love ( Chris Brown song ) ', ' First Ostend Raid ', ' The Boat Race 1900 ', ' U2 concert in Sarajevo ', ' 2 / 4th Machine Gun Battalion ( Australia ) ', ' Man Down ( song ) ', ' Tintin in the Congo ', ' Charles Eaton ( RAAF officer ) ', ' 2010 Alabama Crimson Tide football team ', ' Roger Federer ', ' K @-@ 22 ( Kansas highway ) ', ' Nicole Franklin ', ' West Hendford Cricket Ground , Yeovil ', ' Ãmar mac Arailt ', ' Henry of Grosmont , 1st Duke of Lancaster ', ' Arizona State Route 67 ', ' Rockstar 101 ', ' Love Me Like You ', ' Sclerodermatineae ', ' L.A.M.B. ', ' Clocks ( song ) ', ' Crush ( video game ) ', ' Temple of Eshmun ', ' Iguanodon ', ' Sandwich Day ', ' Croatian independence referendum , 1991 ', ' Truth in Numbers ? ', ' Pokiri ', ' Banai ( goddess ) ', ' Etymology of Wicca ', ' The Stolen Eagle ', ' Battle of Merville Gun Battery ', ' The Tempest ( album ) ', ' Eva PerÃ³n ', ' 1955 Atlantic hurricane season ', ' Route 261 ( Delaware â Pennsylvania ) ', ' Otra Nota ', ' Awakening ( Star Trek : Enterprise ) ', ' John Cullen ', ' Karamokho Alfa ', ' Michelle Rzepecki ', \" Boy @-@ Scoutz ' n the Hood \", ' Politics of Croatia ', ' Super Science Stories ', ' 2010 Haiti earthquake ', ' Cole Hamels ', ' Art Ross ', ' New York State Route 448 ', ' Subtropical Storm Alpha ( 1972 ) ', ' George N. Briggs ', ' Andrew Johnston ( singer ) ', \" The Actor 's Children \", ' Qedarite ', ' Elephanta Caves ', ' 1981 Peach Bowl ( January ) ', ' Rebbie Jackson ', ' The One I Love ( manga ) ', ' John of Brienne ', ' Hoysala literature ', ' James Robert Baker ', ' Art in Medieval Scotland ', ' Sang Pencerah ', ' Grade I listed buildings in Somerset ', ' Directed acyclic graph ', ' WASP @-@ 44 ', ' Odyssey Number Five ', ' The Convict ( 1910 film ) ', ' Tristan ( horse ) ', ' Ulysses ( poem ) ', ' Jack and Jill ( nursery rhyme ) ', ' Allah ', ' Romanian Land Forces ', ' Bart vs. Australia ', ' T30 Howitzer Motor Carriage ', ' Halo : Uprising ', ' You Only Live Twice ( film ) ', ' Laurence Olivier ', ' Track and field ', ' Yoko Shimomura ', ' Condom ', ' Wilhelm Busch ', ' The Family Jewels ( Marina and the Diamonds album ) ', ' Royal prerogative in the United Kingdom ', ' Patriarchal Cathedral of the Holy Ascension of God ', ' Roxas ( Kingdom Hearts ) ', ' Texas A & M Singing Cadets ', ' Battle of Binh', ' Harold Innis ', ' Type 94 Nambu pistol ', ' Inocybe praetervisa ', ' Saint Leonard Catholic Church ( Madison , Nebraska ) ', ' Project Chanology ', ' New York State Route 368 ', ' Mozambican War of Independence ', ' Territorial era of Minnesota ', ' James Nesbitt ', ' Skye ', ' Thomas Quiney ', ' The Fox , the Wolf and the Husbandman ', ' Bossy ( Lindsay Lohan song ) ', ' Underneath ( The X @-@ Files ) ', ' Arikamedu ', ' Shaoguan incident ', ' Humpty Dumpty ', ' Bob Dylan ', ' Kakapo ', ' Berg ( station ) ', ' Noisy miner ', ' Michael Jordan ', ' Crash Boom Bang ! ', ' Cougar ', ' Joe Nathan ', ' Blackburn Firecrest ', ' Zygoballus sexpunctatus ', ' Forward Intelligence Team ', ' Adams River ( British Columbia ) ', ' Bodyline ', ' World War Z ', ' Hellblazer ', ' History of artificial intelligence ', ' Plunketts Creek ( Loyalsock Creek ) ', ' USS Atlanta ( 1861 ) ', ' Voyage : Inspired by Jules Verne ', ' Coldrum Long Barrow ', ' Back to Tennessee ( song ) ', ' Murder of Tom ap Rhys Pryce ', ' Why Does It Hurt So Bad ', ' Johnny McNichol ', ' T. Arthur Cottam ', ' Trials and Tribble @-@ ations ', ' Mega Man & Bass ', ' 2011 â 12 Columbus Blue Jackets season ', ' The Magdalen Reading ', ' Caught Up ( Usher song ) ', ' Common starling ', ' Hurricane Lorenzo ( 2007 ) ', ' The Good Terrorist ', ' New Jersey Route 65 ', ' Mark Stockwell ', ' Dangerously in Love Tour ', ' Le souper de Beaucaire ', ' HMS Black Prince ( 1904 ) ', ' 1990 Pacific hurricane season ', ' The Archaeology of Ritual and Magic ', ' Chasing Vermeer ', ' Aston Villa F.C. ', ' Daydream ( Mariah Carey album ) ', ' Stuart McCall ', ' Florida State Road 878 ', ' Business School ( The Office ) ', ' Kedok Ketawa ', ' Welsh National Opera ', ' Isabella Beeton ', ' Middle Colonies ', ' Frederick Reines ', ' M @-@ 122 ( Michigan highway ) ', ' Imagine ( John Lennon song ) ', ' Black @-@ tailed jackrabbit ', ' Independence Day ( India ) ', ' Glorious First of June ', ' Residence of the United States Ambassador to the United Nations ', ' Hurricane Ingrid ', ' Moro River Campaign ', ' The Boat Race 1999 ', ' Rhode Island Route 4 ', ' Europium ', ' North @-@ Eastern Area Command ( RAAF ) ', ' Kaboom ( Parks and Recreation ) ', ' Survivor Series ( 1992 ) ', ' Illinois ( Sufjan Stevens album ) ', ' Dota 2 ', ' Partington ', ' Alice in Chains ', ' Landing at Anzac Cove ', ' M @-@ 5 ( Michigan highway ) ', ' Sholay ', ' Memory Almost Full ', ' Florida Atlantic University ', ' Ouw Peh Tjoa ', ' Saprang Kalayanamitr ', ' Road to the North Pole ', ' Corythosaurus ', ' Of Human Feelings ', ' Hurricane Uleki ', ' Richard Nixon presidential campaign , 1968 ', ' Maryland Route 194 ', ' Berkley Bedell ', ' Cadmium ', ' On the Pulse of Morning ', ' Zagreb Synagogue ', ' Joyful , Joyful ', ' The Importance of Being Earnest ', ' WASP @-@ 13b ', ' Hurricane Omar ( 2008 ) ', ' First @-@ move advantage in chess ', ' Cater 2 U ', ' Parliament Act 1911 ', ' Crown Fountain ', ' Hurricane Dot ( 1959 ) ', \" USS O 'Brien ( DD @-@ 51 ) \", ' Sovetsky Soyuz @-@ class battleship ', ' Transit of Venus ', ' Ernie Cooksey ', ' Mariana ( poem ) ', ' A4232 road ', ' SS El Sol ', ' No. 79 Wing RAAF ', ' Papal conclave , 1769 ', ' Christine Hakim ', ' DuMont Television Network ', ' Tales of Destiny 2 ', \" Carre 's Grammar School \", ' Lactarius indigo ', ' Somerset County Cricket Club in 2009 ', ' Mogadishu ', ' Diamond stingray ', ' Djedkare Isesi ', ' Khoo@-@', ' Hoover Dam ', ' Leslie Andrew ', ' Ãlfric of Abingdon ', ' Principe Amedeo @-@ class ironclad ', ' If I Never See Your Face Again ', ' Plain maskray ', ' Hydnellum peckii ', ' Gregorian Tower ', ' Rocky Mountain Horse ', ' Kyra ( Charmed ) ', ' Derfflinger @-@ class battlecruiser ', ' Church of Christ Pantocrator , Nesebar ', ' 2016 Spanish Grand Prix ', ' Harajuku Lovers Tour ', ' Portrait of Monsieur Bertin ', ' My Boo ( Usher and Alicia Keys song ) ', \" St Mary 's Church , Rhodogeidio \", ' Ancient Egyptian deities ', ' Central Area Command ( RAAF ) ', ' Wrapped in Red ', ' Crosby Garrett Helmet ', \" St Mary 's Church ,Alderley \", ' Not Quite Hollywood : The Wild , Untold Story of Ozploitation ! ', \" You 're Gonna Love Tomorrow \", ' Stefan Wever ', ' Adam Stansfield ', ' Zhou Tong ( archer ) ', ' The Clean Tech Revolution ', ' The Amps ', ' The Dreamscape ', ' Washington State Route 221 ', ' The Wave ( Miike Snow song ) ', ' Trees ( poem ) ', ' 2008 Bahrain Grand Prix ', ' Thunderbirds ( TV series ) ', ' Bath Assembly Rooms ', ' George Calvert , 1st Baron Baltimore ', ' Laborintus II ( 2012 recording ) ', ' Loose ( Nelly Furtado album ) ', ' Guitar Hero ', ' Banksia violacea ', ' Georgian scripts ', ' Fear of Flying ( The Simpsons ) ', ' Battle of Tellicherry ', ' Chapter 1 ( House of Cards ) ', ' Ontario Highway 36 ', ' Miss Meyers ', \" There 's Got to Be a Way \", ' Development of Fez ', ' Martin Keamy ', ' Verpa bohemica ', ' Ghost in the Shell ( 1995 film ) ', ' The Secret ( The Office ) ', ' Barbarian II : The Dungeon of Drax ', ' New York State Route 185 ', ' Amylostereum ', ' Tommy Lawton ', ' Transportation in Omaha ', ' Species ( film ) ', ' Until the Whole World Hears ', ' The Litigators ', ' Species of Allosaurus ', ' December 1964 South Vietnamese coup ', ' Norman Finkelstein ', ' Sarnia ', \" We 'll Always Have Paris ( Star Trek : The Next Generation ) \", ' The Feast of the Goat ', ' E. W. Hornung ', ' Johnson â Corey â Chaykovsky reaction ', ' Kaimanawa horse ', ' First Light ( Rebecca Stead novel ) ', ' Last Exit on Brooklyn ', ' Brandon Minor ', ' M @-@ 6 ( Michigan highway ) ', ' History of Braathens SAFE ( 1946 â 93 ) ', ' Crazy in Love ', \" Burns ' Heir \", ' Jacob deGrom ', ' Arnhem Oosterbeek War Cemetery ', ' Burn ', ' Grammy Award for Best Concept Music Video ', ' Mumia Abu @-@ Jamal ', ' SMS Erzherzog Ferdinand Max ', ' Utah State Route 61 ', ' Maggie Simpson ', ' Hurricane Flossy ( 1956 ) ', ' Jacqueline Fernandez ', ' God of War video game collections ', ' Flash Gordon Strange Adventure Magazine ', ' Protomycena ', ' South of Heaven ', ' 2nd Battalion 9th Marines ', ' Battle of Romani ', ' Fort Glanville Conservation Park ', ' New York State Route 164 ', ' Ace Attorney ', ' St Nazaire Raid ', ' Krak des Chevaliers ', ' Mortimer Wheeler ', ' TautiÅ¡ka giesmÄ ', ' Ode on Indolence ', \" Galentine 's Day \", ' M @-@ 81 ( Michigan highway ) ', ' Hibiscus ( restaurant ) ', ' Vistara ', ' ToninÃ¡ ', ' Invisible rail ', ' Xenon ', ' Gold dollar ', ' 766th Independent Infantry Regiment ( North Korea ) ', ' Ireland ']\n",
            "Currently at source number: 0/591 ( 2013 â 14 York City F.C. season )\n",
            "Currently at source number: 1/591 ( Big Boy ( song ) )\n",
            "Currently at source number: 2/591 ( The Remix ( Lady Gaga album ) )\n",
            "Currently at source number: 3/591 ( New Year 's Eve ( Up All Night ) )\n",
            "Currently at source number: 4/591 ( Geopyxis carbonaria )\n",
            "Currently at source number: 5/591 ( Cyclone Graham )\n",
            "Currently at source number: 6/591 ( M @-@ 108 ( Michigan highway ) )\n",
            "Currently at source number: 7/591 ( Simon Bradstreet )\n",
            "Currently at source number: 8/591 ( Ghost in the Shell : Stand Alone Complex - Solid State Society )\n",
            "Currently at source number: 9/591 ( Tropical Storm Jose ( 2005 ) )\n",
            "Currently at source number: 10/591 ( California State Route 243 )\n",
            "Currently at source number: 11/591 ( Dave Sisler )\n",
            "Currently at source number: 12/591 ( Gambia women 's national football team )\n",
            "Currently at source number: 13/591 ( Katherine Pulaski )\n",
            "Currently at source number: 14/591 ( Thom Darden )\n",
            "Currently at source number: 15/591 ( Vitamin D ( Glee ) )\n",
            "Currently at source number: 16/591 ( Fastra II )\n",
            "Currently at source number: 17/591 ( Livin ' the Dream )\n",
            "Currently at source number: 18/591 ( HMS Marlborough ( 1912 ) )\n",
            "Currently at source number: 19/591 ( The Sixth Extinction )\n",
            "Currently at source number: 20/591 ( Greens Ledge Light )\n",
            "Currently at source number: 21/591 ( Hannah Primrose , Countess of Rosebery )\n",
            "Currently at source number: 22/591 ( Mycena galericulata )\n",
            "Currently at source number: 23/591 ( Chagas disease )\n",
            "Currently at source number: 24/591 ( LiSA ( Japanese musician , born 1987 ) )\n",
            "Currently at source number: 25/591 ( Sorraia )\n",
            "Currently at source number: 26/591 ( Varanasi )\n",
            "Currently at source number: 27/591 ( Missouri River )\n",
            "Currently at source number: 28/591 ( Magadheera )\n",
            "Currently at source number: 29/591 ( HMS Boreas () )\n",
            "Currently at source number: 30/591 ( Cape lobster )\n",
            "Currently at source number: 31/591 ( Kitsune )\n",
            "Currently at source number: 32/591 ( Paul Thomas Anderson )\n",
            "Currently at source number: 33/591 ( General aviation in the United Kingdom )\n",
            "Currently at source number: 34/591 ( Gaboon viper )\n",
            "Currently at source number: 35/591 ( Lloyd Mathews )\n",
            "Currently at source number: 36/591 ( Jeremi WiÅniowiecki )\n",
            "Currently at source number: 37/591 ( 1939 Pacific hurricane season )\n",
            "Currently at source number: 38/591 ( St Peulan 's Church , Llanbeulan )\n",
            "Currently at source number: 39/591 ( Tawny nurse shark )\n",
            "Currently at source number: 40/591 ( Tropical Storm Abby ( 1964 ) )\n",
            "Currently at source number: 41/591 ( McAllister Tower Apartments )\n",
            "Currently at source number: 42/591 ( Odaenathus )\n",
            "Currently at source number: 43/591 ( Forbidden Fruit ( J. Cole song ) )\n",
            "Currently at source number: 44/591 ( Arihant @-@ class submarine )\n",
            "Currently at source number: 45/591 ( Charles @-@ Valentin Alkan )\n",
            "Currently at source number: 46/591 ( Jenova Chen )\n",
            "Currently at source number: 47/591 ( Mitsuyo Maeda )\n",
            "Currently at source number: 48/591 ( AIL Storm )\n",
            "Currently at source number: 49/591 ( Gold Beach )\n",
            "Currently at source number: 50/591 ( HMS Hostile () )\n",
            "Currently at source number: 51/591 ( Stanley Green )\n",
            "Currently at source number: 52/591 ( Hope Highway )\n",
            "Currently at source number: 53/591 ( Jin â Song Wars )\n",
            "Currently at source number: 54/591 ( Treaty of Ciudad JuÃ¡rez )\n",
            "Currently at source number: 55/591 ( Max Mosley )\n",
            "Currently at source number: 56/591 ( Superman : Escape from Krypton )\n",
            "Currently at source number: 57/591 ( Nebraska Highway 88 )\n",
            "Currently at source number: 58/591 ( Hurricane Abby ( 1960 ) )\n",
            "Currently at source number: 59/591 ( Haifa )\n",
            "Currently at source number: 60/591 ( Robbie Fowler )\n",
            "Currently at source number: 61/591 ( Protein )\n",
            "Currently at source number: 62/591 ( Tupolev Tu @-@ 12 )\n",
            "Currently at source number: 63/591 ( Freakum Dress )\n",
            "Currently at source number: 64/591 ( Plum cake )\n",
            "Currently at source number: 65/591 ( Aerith Gainsborough )\n",
            "Currently at source number: 66/591 ( Nina Simone )\n",
            "Currently at source number: 67/591 ( Simon de Montfort 's Parliament )\n",
            "Currently at source number: 68/591 ( Comair Flight 5191 )\n",
            "Currently at source number: 69/591 ( Ode to a Nightingale )\n",
            "Currently at source number: 70/591 ( The Crab with the Golden Claws )\n",
            "Currently at source number: 71/591 ( President Evil )\n",
            "Currently at source number: 72/591 ( Tessa NoÃ«l )\n",
            "Currently at source number: 73/591 ( The Secret of Monkey Island )\n",
            "Currently at source number: 74/591 ( Gerard ( archbishop of York ) )\n",
            "Currently at source number: 75/591 ( Typhoon Imbudo )\n",
            "Currently at source number: 76/591 ( Rio de Janeiro bid for the 2016 Summer Olympics )\n",
            "Currently at source number: 77/591 ( Action of 13 September 1810 )\n",
            "Currently at source number: 78/591 ( Amanita muscaria )\n",
            "Currently at source number: 79/591 ( Civilian Public Service )\n",
            "Currently at source number: 80/591 ( Ohio State Route 319 )\n",
            "Currently at source number: 81/591 ( 1973 Atlantic hurricane season )\n",
            "Currently at source number: 82/591 ( Mutinus elegans )\n",
            "Currently at source number: 83/591 ( Winston Tunnel )\n",
            "Currently at source number: 84/591 ( Leg before wicket )\n",
            "Currently at source number: 85/591 ( Corn crake )\n",
            "Currently at source number: 86/591 ( Chris Turner ( American football ) )\n",
            "Currently at source number: 87/591 ( 7 Independent Company ( Rhodesia ) )\n",
            "Currently at source number: 88/591 ( Bassline ( Chris Brown song ) )\n",
            "Currently at source number: 89/591 ( Monty Can 't Buy Me Love )\n",
            "Currently at source number: 90/591 ( Ha ' K 'in Xook )\n",
            "Currently at source number: 91/591 ( Fern Hobbs )\n",
            "Currently at source number: 92/591 ( Gregory Helms )\n",
            "Currently at source number: 93/591 ( 2007 Hawaii Bowl )\n",
            "Currently at source number: 94/591 ( Fernando Torres )\n",
            "Currently at source number: 95/591 ( The Goat Puzzle )\n",
            "Currently at source number: 96/591 ( The Moth ( Lost ) )\n",
            "Currently at source number: 97/591 ( The Tramp Dentists )\n",
            "Currently at source number: 98/591 ( O 'Brien @-@ class destroyer )\n",
            "Currently at source number: 99/591 ( Don 't Take It, Babe , It Just Ain 't Your Story )\n",
            "Currently at source number: 100/591 ( Far Away Places ( Mad Men ) )\n",
            "Currently at source number: 101/591 ( Old Pine Church )\n",
            "Currently at source number: 102/591 ( Toronto Magnetic and Meteorological Observatory )\n",
            "Currently at source number: 103/591 ( Ten Commandments in Catholic theology )\n",
            "Currently at source number: 104/591 ( Silver Bullet ( roller coaster ) )\n",
            "Currently at source number: 105/591 ( Mexico City Metropolitan Cathedral )\n",
            "Currently at source number: 106/591 ( Cell nucleus )\n",
            "Currently at source number: 107/591 ( Lady in the Lake trial )\n",
            "Currently at source number: 108/591 ( Languedoc @-@ Roussillon wine )\n",
            "Currently at source number: 109/591 ( Youth on the Prow , and Pleasure at the Helm )\n",
            "Currently at source number: 110/591 ( Tower Building of the Little Rock Arsenal )\n",
            "Currently at source number: 111/591 ( God 's Choice )\n",
            "Currently at source number: 112/591 ( West End Girls )\n",
            "Currently at source number: 113/591 ( Italian cruiser Aretusa )\n",
            "Currently at source number: 114/591 ( Henry HoÊ»olulu Pitman )\n",
            "Currently at source number: 115/591 ( Cyclone Herbie )\n",
            "Currently at source number: 116/591 ( HMS Comet () )\n",
            "Currently at source number: 117/591 ( Berhtwald )\n",
            "Currently at source number: 118/591 ( The General in His Labyrinth )\n",
            "Currently at source number: 119/591 ( Mothers of the Disappeared )\n",
            "Currently at source number: 120/591 ( Hurricane Tanya ( 1995 ) )\n",
            "Currently at source number: 121/591 ( 1998 National League Wild Card tie @-@ breaker game )\n",
            "Currently at source number: 122/591 ( Arbeideren ( Hamar ) )\n",
            "Currently at source number: 123/591 ( Galveston , Texas )\n",
            "Currently at source number: 124/591 ( Copia ( museum ) )\n",
            "Currently at source number: 125/591 ( Hugh Foliot )\n",
            "Currently at source number: 126/591 ( USS Breese ( DD @-@ 122 ) )\n",
            "Currently at source number: 127/591 ( Lisa the Simpson )\n",
            "Currently at source number: 128/591 ( Marauders ( Star Trek : Enterprise ) )\n",
            "Currently at source number: 129/591 ( Charmbracelet )\n",
            "Currently at source number: 130/591 ( Baltimore mayoral election , 1999 )\n",
            "Currently at source number: 131/591 ( Oribi )\n",
            "Currently at source number: 132/591 ( Domnall mac Murchada )\n",
            "Currently at source number: 133/591 ( Snow ( visual novel ) )\n",
            "Currently at source number: 134/591 ( World War I Memorial ( East Providence , Rhode Island ) )\n",
            "Currently at source number: 135/591 ( Exploration of Jupiter )\n",
            "Currently at source number: 136/591 ( Something Borrowed ( Torchwood ) )\n",
            "Currently at source number: 137/591 ( Kalyanasundara )\n",
            "Currently at source number: 138/591 ( Rachel Green )\n",
            "Currently at source number: 139/591 ( New York State Route 38 )\n",
            "Currently at source number: 140/591 ( Valkyria Chronicles III )\n",
            "Currently at source number: 141/591 ( Oldham )\n",
            "Currently at source number: 142/591 ( Norsk Spisevognselskap )\n",
            "Currently at source number: 143/591 ( The Son Also Draws )\n",
            "Currently at source number: 144/591 ( Trinsey v. Pennsylvania )\n",
            "Currently at source number: 145/591 ( A Month in the Country ( film ) )\n",
            "Currently at source number: 146/591 ( Hadji Ali )\n",
            "Currently at source number: 147/591 ( Frank Slide )\n",
            "Currently at source number: 148/591 ( Soviet cruiser Krasnyi Kavkaz )\n",
            "Currently at source number: 149/591 ( Star )\n",
            "Currently at source number: 150/591 ( Lost Horizons ( Lemon Jelly album ) )\n",
            "Currently at source number: 151/591 ( Irresistible ( The X @-@ Files ) )\n",
            "Currently at source number: 152/591 ( Lock Haven , Pennsylvania )\n",
            "Currently at source number: 153/591 ( RÃ©union ibis )\n",
            "Currently at source number: 154/591 ( M @-@ 114 ( Michigan highway ) )\n",
            "Currently at source number: 155/591 ( Liu Kang )\n",
            "Currently at source number: 156/591 ( Architecture of the Song dynasty )\n",
            "Currently at source number: 157/591 ( Burning of women in England )\n",
            "Currently at source number: 158/591 ( Loverboy ( Mariah Carey song ) )\n",
            "Currently at source number: 159/591 ( Blackwyche )\n",
            "Currently at source number: 160/591 ( LÃ¡grimas CÃ¡lidas )\n",
            "Currently at source number: 161/591 ( Marshall Applewhite )\n",
            "Currently at source number: 162/591 ( Ed Barrow )\n",
            "Currently at source number: 163/591 ( August ( Fringe ) )\n",
            "Currently at source number: 164/591 ( White Dog ( Gary novel ) )\n",
            "Currently at source number: 165/591 ( Waterfall Gully , South Australia )\n",
            "Currently at source number: 166/591 ( SMS ZrÃ­nyi )\n",
            "Currently at source number: 167/591 ( Moment of Surrender )\n",
            "Currently at source number: 168/591 ( Paranthodon )\n",
            "Currently at source number: 169/591 ( 1940 Atlantic hurricane season )\n",
            "Currently at source number: 170/591 ( Portuguese ironclad Vasco da Gama )\n",
            "Currently at source number: 171/591 ( Biddenden Maids )\n",
            "Currently at source number: 172/591 ( 1806 Great Coastal hurricane )\n",
            "Currently at source number: 173/591 ( Don 't You Wanna Stay )\n",
            "Currently at source number: 174/591 ( Devin Townsend )\n",
            "Currently at source number: 175/591 ( England national rugby union team )\n",
            "Currently at source number: 176/591 ( Jane Dudley , Duchess of Northumberland )\n",
            "Currently at source number: 177/591 ( Spanish Hill )\n",
            "Currently at source number: 178/591 ( In Bloom )\n",
            "Currently at source number: 179/591 ( Mole cricket )\n",
            "Currently at source number: 180/591 ( Wales national rugby union team )\n",
            "Currently at source number: 181/591 ( USS Illinois ( BB @-@ 7 ) )\n",
            "Currently at source number: 182/591 ( Christmas 1994 nor 'easter )\n",
            "Currently at source number: 183/591 ( Yamaha NS @-@ 10 )\n",
            "Currently at source number: 184/591 ( I Am Unicorn )\n",
            "Currently at source number: 185/591 ( Hurricane Felicia ( 2009 ) )\n",
            "Currently at source number: 186/591 ( Yo @-@ Yo ( Nicola Roberts song ) )\n",
            "Currently at source number: 187/591 ( The Food Album )\n",
            "Currently at source number: 188/591 ( Islais Creek )\n",
            "Currently at source number: 189/591 ( Super Mario Land )\n",
            "Currently at source number: 190/591 ( Ceratopsia )\n",
            "Currently at source number: 191/591 ( Santa @-@ Fe ( Bob Dylan song ) )\n",
            "Currently at source number: 192/591 ( Strand , London )\n",
            "Currently at source number: 193/591 ( Live & Kicking )\n",
            "Currently at source number: 194/591 ( Weather buoy )\n",
            "Currently at source number: 195/591 ( Elgin Cathedral )\n",
            "Currently at source number: 196/591 ( Hi , Infidelity )\n",
            "Currently at source number: 197/591 ( Polka Party ! )\n",
            "Currently at source number: 198/591 ( Van Morrison : Too Late to Stop Now )\n",
            "Currently at source number: 199/591 ( True Blue ( Madonna song ) )\n",
            "Currently at source number: 200/591 ( Dover Athletic F.C. )\n",
            "Currently at source number: 201/591 ( June 1941 uprising in eastern Herzegovina )\n",
            "Currently at source number: 202/591 ( Stay @-@ at @-@ home dad )\n",
            "Currently at source number: 203/591 ( Stephanolepis cirrhifer )\n",
            "Currently at source number: 204/591 ( M @-@ 47 ( Michigan highway ) )\n",
            "Currently at source number: 205/591 ( G.I. Joe : Retaliation )\n",
            "Currently at source number: 206/591 ( Perry the)\n",
            "Currently at source number: 207/591 ( 1986 Peach Bowl )\n",
            "Currently at source number: 208/591 ( Midge ( Barbie ) )\n",
            "Currently at source number: 209/591 ( Tina Fey )\n",
            "Currently at source number: 210/591 ( Westminster Assembly )\n",
            "Currently at source number: 211/591 ( Boise National Forest )\n",
            "Currently at source number: 212/591 ( History of Bradford City)\n",
            "Currently at source number: 213/591 ( Tropical Storm Domoina )\n",
            "Currently at source number: 214/591 ( Corpus Christi Bay )\n",
            "Currently at source number: 215/591 ( Canning Dam )\n",
            "Currently at source number: 216/591 ( Tropical Storm Brenda ( 1960 ) )\n",
            "Currently at source number: 217/591 ( Giacomo Meyerbeer )\n",
            "Currently at source number: 218/591 ( First Battle of Maryang San )\n",
            "Currently at source number: 219/591 ( Curtis Woodhouse )\n",
            "Currently at source number: 220/591 ( Mount Jackson ( Antarctica ) )\n",
            "Currently at source number: 221/591 ( Stanley Price Weir )\n",
            "Currently at source number: 222/591 ( Rosemary 's Baby ( 30 Rock ) )\n",
            "Currently at source number: 223/591 ( 1981 European Cup Final )\n",
            "Currently at source number: 224/591 ( Kir 'Shara )\n",
            "Currently at source number: 225/591 ( Acute myeloid leukemia )\n",
            "Currently at source number: 226/591 ( Perfect Dark ( 2010 video game ) )\n",
            "Currently at source number: 227/591 ( Sentence spacing )\n",
            "Currently at source number: 228/591 ( Edward Creutz )\n",
            "Currently at source number: 229/591 ( Cambodian Campaign )\n",
            "Currently at source number: 230/591 ( Music of Chrono Cross )\n",
            "Currently at source number: 231/591 ( Rob Howard )\n",
            "Currently at source number: 232/591 ( Gertrude Barrows Bennett )\n",
            "Currently at source number: 233/591 ( Jifna )\n",
            "Currently at source number: 234/591 ( S.R. 819 )\n",
            "Currently at source number: 235/591 ( SMS Markgraf )\n",
            "Currently at source number: 236/591 ( Polish culture during World War II )\n",
            "Currently at source number: 237/591 ( Antimony )\n",
            "Currently at source number: 238/591 ( Tropical Storm Olaf ( 1997 ) )\n",
            "Currently at source number: 239/591 ( Hugh Walpole )\n",
            "Currently at source number: 240/591 ( Richard Cresswell )\n",
            "Currently at source number: 241/591 ( Who Am I ( Casting Crowns song ) )\n",
            "Currently at source number: 242/591 ( Josce de Dinan )\n",
            "Currently at source number: 243/591 ( Erving Goffman )\n",
            "Currently at source number: 244/591 ( Key ( basketball ) )\n",
            "Currently at source number: 245/591 ( 130th Engineer Brigade ( United States ) )\n",
            "Currently at source number: 246/591 ( Stop ! ! Hibari @-@ kun ! )\n",
            "Currently at source number: 247/591 ( Pattycake ( gorilla ) )\n",
            "Currently at source number: 248/591 ( Sister Wives )\n",
            "Currently at source number: 249/591 ( SM U @-@ 3 ( Austria @-@ Hungary ) )\n",
            "Currently at source number: 250/591 ( St Caffo 's Church , Llangaffo )\n",
            "Currently at source number: 251/591 ( Old Baltimore Pike )\n",
            "Currently at source number: 252/591 ( Tiber Oil Field )\n",
            "Currently at source number: 253/591 ( Battle of Hubbardton )\n",
            "Currently at source number: 254/591 ( Jane 's Attack Squadron )\n",
            "Currently at source number: 255/591 ( Hoyt Wilhelm )\n",
            "Currently at source number: 256/591 ( Journey ( 2012 video game ) )\n",
            "Currently at source number: 257/591 ( Astraeus hygrometricus )\n",
            "Currently at source number: 258/591 ( Sinclair Sovereign )\n",
            "Currently at source number: 259/591 ( Ceres ( dwarf planet ) )\n",
            "Currently at source number: 260/591 ( French cruiser Sully )\n",
            "Currently at source number: 261/591 ( Washington State Route 516 )\n",
            "Currently at source number: 262/591 ( Sweet Love ( Chris Brown song ) )\n",
            "Currently at source number: 263/591 ( First Ostend Raid )\n",
            "Currently at source number: 264/591 ( The Boat Race 1900 )\n",
            "Currently at source number: 265/591 ( U2 concert in Sarajevo )\n",
            "Currently at source number: 266/591 ( 2 / 4th Machine Gun Battalion ( Australia ) )\n",
            "Currently at source number: 267/591 ( Man Down ( song ) )\n",
            "Currently at source number: 268/591 ( Tintin in the Congo )\n",
            "Currently at source number: 269/591 ( Charles Eaton ( RAAF officer ) )\n",
            "Currently at source number: 270/591 ( 2010 Alabama Crimson Tide football team )\n",
            "Currently at source number: 271/591 ( Roger Federer )\n",
            "Currently at source number: 272/591 ( K @-@ 22 ( Kansas highway ) )\n",
            "Currently at source number: 273/591 ( Nicole Franklin )\n",
            "Currently at source number: 274/591 ( West Hendford Cricket Ground , Yeovil )\n",
            "Currently at source number: 275/591 ( Ãmar mac Arailt )\n",
            "Currently at source number: 276/591 ( Henry of Grosmont , 1st Duke of Lancaster )\n",
            "Currently at source number: 277/591 ( Arizona State Route 67 )\n",
            "Currently at source number: 278/591 ( Rockstar 101 )\n",
            "Currently at source number: 279/591 ( Love Me Like You )\n",
            "Currently at source number: 280/591 ( Sclerodermatineae )\n",
            "Currently at source number: 281/591 ( L.A.M.B. )\n",
            "Currently at source number: 282/591 ( Clocks ( song ) )\n",
            "Currently at source number: 283/591 ( Crush ( video game ) )\n",
            "Currently at source number: 284/591 ( Temple of Eshmun )\n",
            "Currently at source number: 285/591 ( Iguanodon )\n",
            "Currently at source number: 286/591 ( Sandwich Day )\n",
            "Currently at source number: 287/591 ( Croatian independence referendum , 1991 )\n",
            "Currently at source number: 288/591 ( Truth in Numbers ? )\n",
            "Currently at source number: 289/591 ( Pokiri )\n",
            "Currently at source number: 290/591 ( Banai ( goddess ) )\n",
            "Currently at source number: 291/591 ( Etymology of Wicca )\n",
            "Currently at source number: 292/591 ( The Stolen Eagle )\n",
            "Currently at source number: 293/591 ( Battle of Merville Gun Battery )\n",
            "Currently at source number: 294/591 ( The Tempest ( album ) )\n",
            "Currently at source number: 295/591 ( Eva PerÃ³n )\n",
            "Currently at source number: 296/591 ( 1955 Atlantic hurricane season )\n",
            "Currently at source number: 297/591 ( Route 261 ( Delaware â Pennsylvania ) )\n",
            "Currently at source number: 298/591 ( Otra Nota )\n",
            "Currently at source number: 299/591 ( Awakening ( Star Trek : Enterprise ) )\n",
            "Currently at source number: 300/591 ( John Cullen )\n",
            "Currently at source number: 301/591 ( Karamokho Alfa )\n",
            "Currently at source number: 302/591 ( Michelle Rzepecki )\n",
            "Currently at source number: 303/591 ( Boy @-@ Scoutz ' n the Hood )\n",
            "Currently at source number: 304/591 ( Politics of Croatia )\n",
            "Currently at source number: 305/591 ( Super Science Stories )\n",
            "Currently at source number: 306/591 ( 2010 Haiti earthquake )\n",
            "Currently at source number: 307/591 ( Cole Hamels )\n",
            "Currently at source number: 308/591 ( Art Ross )\n",
            "Currently at source number: 309/591 ( New York State Route 448 )\n",
            "Currently at source number: 310/591 ( Subtropical Storm Alpha ( 1972 ) )\n",
            "Currently at source number: 311/591 ( George N. Briggs )\n",
            "Currently at source number: 312/591 ( Andrew Johnston ( singer ) )\n",
            "Currently at source number: 313/591 ( The Actor 's Children )\n",
            "Currently at source number: 314/591 ( Qedarite )\n",
            "Currently at source number: 315/591 ( Elephanta Caves )\n",
            "Currently at source number: 316/591 ( 1981 Peach Bowl ( January ) )\n",
            "Currently at source number: 317/591 ( Rebbie Jackson )\n",
            "Currently at source number: 318/591 ( The One I Love ( manga ) )\n",
            "Currently at source number: 319/591 ( John of Brienne )\n",
            "Currently at source number: 320/591 ( Hoysala literature )\n",
            "Currently at source number: 321/591 ( James Robert Baker )\n",
            "Currently at source number: 322/591 ( Art in Medieval Scotland )\n",
            "Currently at source number: 323/591 ( Sang Pencerah )\n",
            "Currently at source number: 324/591 ( Grade I listed buildings in Somerset )\n",
            "Currently at source number: 325/591 ( Directed acyclic graph )\n",
            "Currently at source number: 326/591 ( WASP @-@ 44 )\n",
            "Currently at source number: 327/591 ( Odyssey Number Five )\n",
            "Currently at source number: 328/591 ( The Convict ( 1910 film ) )\n",
            "Currently at source number: 329/591 ( Tristan ( horse ) )\n",
            "Currently at source number: 330/591 ( Ulysses ( poem ) )\n",
            "Currently at source number: 331/591 ( Jack and Jill ( nursery rhyme ) )\n",
            "Currently at source number: 332/591 ( Allah )\n",
            "Currently at source number: 333/591 ( Romanian Land Forces )\n",
            "Currently at source number: 334/591 ( Bart vs. Australia )\n",
            "Currently at source number: 335/591 ( T30 Howitzer Motor Carriage )\n",
            "Currently at source number: 336/591 ( Halo : Uprising )\n",
            "Currently at source number: 337/591 ( You Only Live Twice ( film ) )\n",
            "Currently at source number: 338/591 ( Laurence Olivier )\n",
            "Currently at source number: 339/591 ( Track and field )\n",
            "Currently at source number: 340/591 ( Yoko Shimomura )\n",
            "Currently at source number: 341/591 ( Condom )\n",
            "Currently at source number: 342/591 ( Wilhelm Busch )\n",
            "Currently at source number: 343/591 ( The Family Jewels ( Marina and the Diamonds album ) )\n",
            "Currently at source number: 344/591 ( Royal prerogative in the United Kingdom )\n",
            "Currently at source number: 345/591 ( Patriarchal Cathedral of the Holy Ascension of God )\n",
            "Currently at source number: 346/591 ( Roxas ( Kingdom Hearts ) )\n",
            "Currently at source number: 347/591 ( Texas A & M Singing Cadets )\n",
            "Currently at source number: 348/591 ( Battle of Binh)\n",
            "Currently at source number: 349/591 ( Harold Innis )\n",
            "Currently at source number: 350/591 ( Type 94 Nambu pistol )\n",
            "Currently at source number: 351/591 ( Inocybe praetervisa )\n",
            "Currently at source number: 352/591 ( Saint Leonard Catholic Church ( Madison , Nebraska ) )\n",
            "Currently at source number: 353/591 ( Project Chanology )\n",
            "Currently at source number: 354/591 ( New York State Route 368 )\n",
            "Currently at source number: 355/591 ( Mozambican War of Independence )\n",
            "Currently at source number: 356/591 ( Territorial era of Minnesota )\n",
            "Currently at source number: 357/591 ( James Nesbitt )\n",
            "Currently at source number: 358/591 ( Skye )\n",
            "Currently at source number: 359/591 ( Thomas Quiney )\n",
            "Currently at source number: 360/591 ( The Fox , the Wolf and the Husbandman )\n",
            "Currently at source number: 361/591 ( Bossy ( Lindsay Lohan song ) )\n",
            "Currently at source number: 362/591 ( Underneath ( The X @-@ Files ) )\n",
            "Currently at source number: 363/591 ( Arikamedu )\n",
            "Currently at source number: 364/591 ( Shaoguan incident )\n",
            "Currently at source number: 365/591 ( Humpty Dumpty )\n",
            "Currently at source number: 366/591 ( Bob Dylan )\n",
            "Currently at source number: 367/591 ( Kakapo )\n",
            "Currently at source number: 368/591 ( Berg ( station ) )\n",
            "Currently at source number: 369/591 ( Noisy miner )\n",
            "Currently at source number: 370/591 ( Michael Jordan )\n",
            "Currently at source number: 371/591 ( Crash Boom Bang ! )\n",
            "Currently at source number: 372/591 ( Cougar )\n",
            "Currently at source number: 373/591 ( Joe Nathan )\n",
            "Currently at source number: 374/591 ( Blackburn Firecrest )\n",
            "Currently at source number: 375/591 ( Zygoballus sexpunctatus )\n",
            "Currently at source number: 376/591 ( Forward Intelligence Team )\n",
            "Currently at source number: 377/591 ( Adams River ( British Columbia ) )\n",
            "Currently at source number: 378/591 ( Bodyline )\n",
            "Currently at source number: 379/591 ( World War Z )\n",
            "Currently at source number: 380/591 ( Hellblazer )\n",
            "Currently at source number: 381/591 ( History of artificial intelligence )\n",
            "Currently at source number: 382/591 ( Plunketts Creek ( Loyalsock Creek ) )\n",
            "Currently at source number: 383/591 ( USS Atlanta ( 1861 ) )\n",
            "Currently at source number: 384/591 ( Voyage : Inspired by Jules Verne )\n",
            "Currently at source number: 385/591 ( Coldrum Long Barrow )\n",
            "Currently at source number: 386/591 ( Back to Tennessee ( song ) )\n",
            "Currently at source number: 387/591 ( Murder of Tom ap Rhys Pryce )\n",
            "Currently at source number: 388/591 ( Why Does It Hurt So Bad )\n",
            "Currently at source number: 389/591 ( Johnny McNichol )\n",
            "Currently at source number: 390/591 ( T. Arthur Cottam )\n",
            "Currently at source number: 391/591 ( Trials and Tribble @-@ ations )\n",
            "Currently at source number: 392/591 ( Mega Man & Bass )\n",
            "Currently at source number: 393/591 ( 2011 â 12 Columbus Blue Jackets season )\n",
            "Currently at source number: 394/591 ( The Magdalen Reading )\n",
            "Currently at source number: 395/591 ( Caught Up ( Usher song ) )\n",
            "Currently at source number: 396/591 ( Common starling )\n",
            "Currently at source number: 397/591 ( Hurricane Lorenzo ( 2007 ) )\n",
            "Currently at source number: 398/591 ( The Good Terrorist )\n",
            "Currently at source number: 399/591 ( New Jersey Route 65 )\n",
            "Currently at source number: 400/591 ( Mark Stockwell )\n",
            "Currently at source number: 401/591 ( Dangerously in Love Tour )\n",
            "Currently at source number: 402/591 ( Le souper de Beaucaire )\n",
            "Currently at source number: 403/591 ( HMS Black Prince ( 1904 ) )\n",
            "Currently at source number: 404/591 ( 1990 Pacific hurricane season )\n",
            "Currently at source number: 405/591 ( The Archaeology of Ritual and Magic )\n",
            "Currently at source number: 406/591 ( Chasing Vermeer )\n",
            "Currently at source number: 407/591 ( Aston Villa F.C. )\n",
            "Currently at source number: 408/591 ( Daydream ( Mariah Carey album ) )\n",
            "Currently at source number: 409/591 ( Stuart McCall )\n",
            "Currently at source number: 410/591 ( Florida State Road 878 )\n",
            "Currently at source number: 411/591 ( Business School ( The Office ) )\n",
            "Currently at source number: 412/591 ( Kedok Ketawa )\n",
            "Currently at source number: 413/591 ( Welsh National Opera )\n",
            "Currently at source number: 414/591 ( Isabella Beeton )\n",
            "Currently at source number: 415/591 ( Middle Colonies )\n",
            "Currently at source number: 416/591 ( Frederick Reines )\n",
            "Currently at source number: 417/591 ( M @-@ 122 ( Michigan highway ) )\n",
            "Currently at source number: 418/591 ( Imagine ( John Lennon song ) )\n",
            "Currently at source number: 419/591 ( Black @-@ tailed jackrabbit )\n",
            "Currently at source number: 420/591 ( Independence Day ( India ) )\n",
            "Currently at source number: 421/591 ( Glorious First of June )\n",
            "Currently at source number: 422/591 ( Residence of the United States Ambassador to the United Nations )\n",
            "Currently at source number: 423/591 ( Hurricane Ingrid )\n",
            "Currently at source number: 424/591 ( Moro River Campaign )\n",
            "Currently at source number: 425/591 ( The Boat Race 1999 )\n",
            "Currently at source number: 426/591 ( Rhode Island Route 4 )\n",
            "Currently at source number: 427/591 ( Europium )\n",
            "Currently at source number: 428/591 ( North @-@ Eastern Area Command ( RAAF ) )\n",
            "Currently at source number: 429/591 ( Kaboom ( Parks and Recreation ) )\n",
            "Currently at source number: 430/591 ( Survivor Series ( 1992 ) )\n",
            "Currently at source number: 431/591 ( Illinois ( Sufjan Stevens album ) )\n",
            "Currently at source number: 432/591 ( Dota 2 )\n",
            "Currently at source number: 433/591 ( Partington )\n",
            "Currently at source number: 434/591 ( Alice in Chains )\n",
            "Currently at source number: 435/591 ( Landing at Anzac Cove )\n",
            "Currently at source number: 436/591 ( M @-@ 5 ( Michigan highway ) )\n",
            "Currently at source number: 437/591 ( Sholay )\n",
            "Currently at source number: 438/591 ( Memory Almost Full )\n",
            "Currently at source number: 439/591 ( Florida Atlantic University )\n",
            "Currently at source number: 440/591 ( Ouw Peh Tjoa )\n",
            "Currently at source number: 441/591 ( Saprang Kalayanamitr )\n",
            "Currently at source number: 442/591 ( Road to the North Pole )\n",
            "Currently at source number: 443/591 ( Corythosaurus )\n",
            "Currently at source number: 444/591 ( Of Human Feelings )\n",
            "Currently at source number: 445/591 ( Hurricane Uleki )\n",
            "Currently at source number: 446/591 ( Richard Nixon presidential campaign , 1968 )\n",
            "Currently at source number: 447/591 ( Maryland Route 194 )\n",
            "Currently at source number: 448/591 ( Berkley Bedell )\n",
            "Currently at source number: 449/591 ( Cadmium )\n",
            "Currently at source number: 450/591 ( On the Pulse of Morning )\n",
            "Currently at source number: 451/591 ( Zagreb Synagogue )\n",
            "Currently at source number: 452/591 ( Joyful , Joyful )\n",
            "Currently at source number: 453/591 ( The Importance of Being Earnest )\n",
            "Currently at source number: 454/591 ( WASP @-@ 13b )\n",
            "Currently at source number: 455/591 ( Hurricane Omar ( 2008 ) )\n",
            "Currently at source number: 456/591 ( First @-@ move advantage in chess )\n",
            "Currently at source number: 457/591 ( Cater 2 U )\n",
            "Currently at source number: 458/591 ( Parliament Act 1911 )\n",
            "Currently at source number: 459/591 ( Crown Fountain )\n",
            "Currently at source number: 460/591 ( Hurricane Dot ( 1959 ) )\n",
            "Currently at source number: 461/591 ( USS O 'Brien ( DD @-@ 51 ) )\n",
            "Currently at source number: 462/591 ( Sovetsky Soyuz @-@ class battleship )\n",
            "Currently at source number: 463/591 ( Transit of Venus )\n",
            "Currently at source number: 464/591 ( Ernie Cooksey )\n",
            "Currently at source number: 465/591 ( Mariana ( poem ) )\n",
            "Currently at source number: 466/591 ( A4232 road )\n",
            "Currently at source number: 467/591 ( SS El Sol )\n",
            "Currently at source number: 468/591 ( No. 79 Wing RAAF )\n",
            "Currently at source number: 469/591 ( Papal conclave , 1769 )\n",
            "Currently at source number: 470/591 ( Christine Hakim )\n",
            "Currently at source number: 471/591 ( DuMont Television Network )\n",
            "Currently at source number: 472/591 ( Tales of Destiny 2 )\n",
            "Currently at source number: 473/591 ( Carre 's Grammar School )\n",
            "Currently at source number: 474/591 ( Lactarius indigo )\n",
            "Currently at source number: 475/591 ( Somerset County Cricket Club in 2009 )\n",
            "Currently at source number: 476/591 ( Mogadishu )\n",
            "Currently at source number: 477/591 ( Diamond stingray )\n",
            "Currently at source number: 478/591 ( Djedkare Isesi )\n",
            "Currently at source number: 479/591 ( Khoo@-@)\n",
            "Currently at source number: 480/591 ( Hoover Dam )\n",
            "Currently at source number: 481/591 ( Leslie Andrew )\n",
            "Currently at source number: 482/591 ( Ãlfric of Abingdon )\n",
            "Currently at source number: 483/591 ( Principe Amedeo @-@ class ironclad )\n",
            "Currently at source number: 484/591 ( If I Never See Your Face Again )\n",
            "Currently at source number: 485/591 ( Plain maskray )\n",
            "Currently at source number: 486/591 ( Hydnellum peckii )\n",
            "Currently at source number: 487/591 ( Gregorian Tower )\n",
            "Currently at source number: 488/591 ( Rocky Mountain Horse )\n",
            "Currently at source number: 489/591 ( Kyra ( Charmed ) )\n",
            "Currently at source number: 490/591 ( Derfflinger @-@ class battlecruiser )\n",
            "Currently at source number: 491/591 ( Church of Christ Pantocrator , Nesebar )\n",
            "Currently at source number: 492/591 ( 2016 Spanish Grand Prix )\n",
            "Currently at source number: 493/591 ( Harajuku Lovers Tour )\n",
            "Currently at source number: 494/591 ( Portrait of Monsieur Bertin )\n",
            "Currently at source number: 495/591 ( My Boo ( Usher and Alicia Keys song ) )\n",
            "Currently at source number: 496/591 ( St Mary 's Church , Rhodogeidio )\n",
            "Currently at source number: 497/591 ( Ancient Egyptian deities )\n",
            "Currently at source number: 498/591 ( Central Area Command ( RAAF ) )\n",
            "Currently at source number: 499/591 ( Wrapped in Red )\n",
            "Currently at source number: 500/591 ( Crosby Garrett Helmet )\n",
            "Currently at source number: 501/591 ( St Mary 's Church ,Alderley )\n",
            "Currently at source number: 502/591 ( Not Quite Hollywood : The Wild , Untold Story of Ozploitation ! )\n",
            "Currently at source number: 503/591 ( You 're Gonna Love Tomorrow )\n",
            "Currently at source number: 504/591 ( Stefan Wever )\n",
            "Currently at source number: 505/591 ( Adam Stansfield )\n",
            "Currently at source number: 506/591 ( Zhou Tong ( archer ) )\n",
            "Currently at source number: 507/591 ( The Clean Tech Revolution )\n",
            "Currently at source number: 508/591 ( The Amps )\n",
            "Currently at source number: 509/591 ( The Dreamscape )\n",
            "Currently at source number: 510/591 ( Washington State Route 221 )\n",
            "Currently at source number: 511/591 ( The Wave ( Miike Snow song ) )\n",
            "Currently at source number: 512/591 ( Trees ( poem ) )\n",
            "Currently at source number: 513/591 ( 2008 Bahrain Grand Prix )\n",
            "Currently at source number: 514/591 ( Thunderbirds ( TV series ) )\n",
            "Currently at source number: 515/591 ( Bath Assembly Rooms )\n",
            "Currently at source number: 516/591 ( George Calvert , 1st Baron Baltimore )\n",
            "Currently at source number: 517/591 ( Laborintus II ( 2012 recording ) )\n",
            "Currently at source number: 518/591 ( Loose ( Nelly Furtado album ) )\n",
            "Currently at source number: 519/591 ( Guitar Hero )\n",
            "Currently at source number: 520/591 ( Banksia violacea )\n",
            "Currently at source number: 521/591 ( Georgian scripts )\n",
            "Currently at source number: 522/591 ( Fear of Flying ( The Simpsons ) )\n",
            "Currently at source number: 523/591 ( Battle of Tellicherry )\n",
            "Currently at source number: 524/591 ( Chapter 1 ( House of Cards ) )\n",
            "Currently at source number: 525/591 ( Ontario Highway 36 )\n",
            "Currently at source number: 526/591 ( Miss Meyers )\n",
            "Currently at source number: 527/591 ( There 's Got to Be a Way )\n",
            "Currently at source number: 528/591 ( Development of Fez )\n",
            "Currently at source number: 529/591 ( Martin Keamy )\n",
            "Currently at source number: 530/591 ( Verpa bohemica )\n",
            "Currently at source number: 531/591 ( Ghost in the Shell ( 1995 film ) )\n",
            "Currently at source number: 532/591 ( The Secret ( The Office ) )\n",
            "Currently at source number: 533/591 ( Barbarian II : The Dungeon of Drax )\n",
            "Currently at source number: 534/591 ( New York State Route 185 )\n",
            "Currently at source number: 535/591 ( Amylostereum )\n",
            "Currently at source number: 536/591 ( Tommy Lawton )\n",
            "Currently at source number: 537/591 ( Transportation in Omaha )\n",
            "Currently at source number: 538/591 ( Species ( film ) )\n",
            "Currently at source number: 539/591 ( Until the Whole World Hears )\n",
            "Currently at source number: 540/591 ( The Litigators )\n",
            "Currently at source number: 541/591 ( Species of Allosaurus )\n",
            "Currently at source number: 542/591 ( December 1964 South Vietnamese coup )\n",
            "Currently at source number: 543/591 ( Norman Finkelstein )\n",
            "Currently at source number: 544/591 ( Sarnia )\n",
            "Currently at source number: 545/591 ( We 'll Always Have Paris ( Star Trek : The Next Generation ) )\n",
            "Currently at source number: 546/591 ( The Feast of the Goat )\n",
            "Currently at source number: 547/591 ( E. W. Hornung )\n",
            "Currently at source number: 548/591 ( Johnson â Corey â Chaykovsky reaction )\n",
            "Currently at source number: 549/591 ( Kaimanawa horse )\n",
            "Currently at source number: 550/591 ( First Light ( Rebecca Stead novel ) )\n",
            "Currently at source number: 551/591 ( Last Exit on Brooklyn )\n",
            "Currently at source number: 552/591 ( Brandon Minor )\n",
            "Currently at source number: 553/591 ( M @-@ 6 ( Michigan highway ) )\n",
            "Currently at source number: 554/591 ( History of Braathens SAFE ( 1946 â 93 ) )\n",
            "Currently at source number: 555/591 ( Crazy in Love )\n",
            "Currently at source number: 556/591 ( Burns ' Heir )\n",
            "Currently at source number: 557/591 ( Jacob deGrom )\n",
            "Currently at source number: 558/591 ( Arnhem Oosterbeek War Cemetery )\n",
            "Currently at source number: 559/591 ( Burn )\n",
            "Currently at source number: 560/591 ( Grammy Award for Best Concept Music Video )\n",
            "Currently at source number: 561/591 ( Mumia Abu @-@ Jamal )\n",
            "Currently at source number: 562/591 ( SMS Erzherzog Ferdinand Max )\n",
            "Currently at source number: 563/591 ( Utah State Route 61 )\n",
            "Currently at source number: 564/591 ( Maggie Simpson )\n",
            "Currently at source number: 565/591 ( Hurricane Flossy ( 1956 ) )\n",
            "Currently at source number: 566/591 ( Jacqueline Fernandez )\n",
            "Currently at source number: 567/591 ( God of War video game collections )\n",
            "Currently at source number: 568/591 ( Flash Gordon Strange Adventure Magazine )\n",
            "Currently at source number: 569/591 ( Protomycena )\n",
            "Currently at source number: 570/591 ( South of Heaven )\n",
            "Currently at source number: 571/591 ( 2nd Battalion 9th Marines )\n",
            "Currently at source number: 572/591 ( Battle of Romani )\n",
            "Currently at source number: 573/591 ( Fort Glanville Conservation Park )\n",
            "Currently at source number: 574/591 ( New York State Route 164 )\n",
            "Currently at source number: 575/591 ( Ace Attorney )\n",
            "Currently at source number: 576/591 ( St Nazaire Raid )\n",
            "Currently at source number: 577/591 ( Krak des Chevaliers )\n",
            "Currently at source number: 578/591 ( Mortimer Wheeler )\n",
            "Currently at source number: 579/591 ( TautiÅ¡ka giesmÄ )\n",
            "Currently at source number: 580/591 ( Ode on Indolence )\n",
            "Currently at source number: 581/591 ( Galentine 's Day )\n",
            "Currently at source number: 582/591 ( M @-@ 81 ( Michigan highway ) )\n",
            "Currently at source number: 583/591 ( Hibiscus ( restaurant ) )\n",
            "Currently at source number: 584/591 ( Vistara )\n",
            "Currently at source number: 585/591 ( ToninÃ¡ )\n",
            "Currently at source number: 586/591 ( Invisible rail )\n",
            "Currently at source number: 587/591 ( Xenon )\n",
            "Currently at source number: 588/591 ( Gold dollar )\n",
            "NLP Train Sentence Creation Time: 1715.90 seconds\n",
            "\n",
            "[' Tropical Storm( 2008 ) ', ' Calvin', ' The Boat Race 2008 ', ' Angel of Death ( Slayer song ) ', \" 2011 â 12 Michigan Wolverines men 's basketball team \", ' Firstinvasion of Burma ', ' New Jersey Route 29 ', ' Daniel', ' New Jersey Route 50 ', ' Military history of Australia ', ' Battle of theRiver ', ' TheBlues ', ' Homarus gammarus ', ' Tim Richmond ', ' High Five Interchange ', ' Meridian , Mississippi ', ' M @-@ 82 ( Michigan highway ) ', ' Sonic the Hedgehog ( 1991 video game ) ', ' U.S. Route 2 in Michigan ', ' 1985assassination plot ', ' Hurricane( 2011 ) ', ' Home MadePie ', ' Exit Through the@-@ E @-@ Mart ', ' Ward Churchill ', ' Mycena', \" Battle of Sullivan 's Island \", ' Air Rhodesia Flight', ' Back Off', ' Where the Streets Have No Name ', ' B of the Bang ', ' Randy Blythecase ', ' Fort Scott National Historic Site ', ' Hu', ' Capel', ' Die Another Day ', ' Raid on Manila ( 1798 ) ', ' 1888 â 89 New Zealand Native football team ', \" Battle of's Ridge \", ' J. C. W.', ' Mount', ' Scientology in Germany ', ' The Same Old Story ', ' Training Day ( The Office ) ', ' Butterfly World Tour ', ' Frank', ' Papa', ' Japanese battleship Asahi ']\n",
            "Currently at source number: 0/47 ( Tropical Storm( 2008 ) )\n",
            "Currently at source number: 1/47 ( Calvin)\n",
            "Currently at source number: 2/47 ( The Boat Race 2008 )\n",
            "Currently at source number: 3/47 ( Angel of Death ( Slayer song ) )\n",
            "Currently at source number: 4/47 ( 2011 â 12 Michigan Wolverines men 's basketball team )\n",
            "Currently at source number: 5/47 ( Firstinvasion of Burma )\n",
            "Currently at source number: 6/47 ( New Jersey Route 29 )\n",
            "Currently at source number: 7/47 ( Daniel)\n",
            "Currently at source number: 8/47 ( New Jersey Route 50 )\n",
            "Currently at source number: 9/47 ( Military history of Australia )\n",
            "Currently at source number: 10/47 ( Battle of theRiver )\n",
            "Currently at source number: 11/47 ( TheBlues )\n",
            "Currently at source number: 12/47 ( Homarus gammarus )\n",
            "Currently at source number: 13/47 ( Tim Richmond )\n",
            "Currently at source number: 14/47 ( High Five Interchange )\n",
            "Currently at source number: 15/47 ( Meridian , Mississippi )\n",
            "Currently at source number: 16/47 ( M @-@ 82 ( Michigan highway ) )\n",
            "Currently at source number: 17/47 ( Sonic the Hedgehog ( 1991 video game ) )\n",
            "Currently at source number: 18/47 ( U.S. Route 2 in Michigan )\n",
            "Currently at source number: 19/47 ( 1985assassination plot )\n",
            "Currently at source number: 20/47 ( Hurricane( 2011 ) )\n",
            "Currently at source number: 21/47 ( Home MadePie )\n",
            "Currently at source number: 22/47 ( Exit Through the@-@ E @-@ Mart )\n",
            "Currently at source number: 23/47 ( Ward Churchill )\n",
            "Currently at source number: 24/47 ( Mycena)\n",
            "Currently at source number: 25/47 ( Battle of Sullivan 's Island )\n",
            "Currently at source number: 26/47 ( Air Rhodesia Flight)\n",
            "Currently at source number: 27/47 ( Back Off)\n",
            "Currently at source number: 28/47 ( Where the Streets Have No Name )\n",
            "Currently at source number: 29/47 ( B of the Bang )\n",
            "Currently at source number: 30/47 ( Randy Blythecase )\n",
            "Currently at source number: 31/47 ( Fort Scott National Historic Site )\n",
            "Currently at source number: 32/47 ( Hu)\n",
            "Currently at source number: 33/47 ( Capel)\n",
            "Currently at source number: 34/47 ( Die Another Day )\n",
            "Currently at source number: 35/47 ( Raid on Manila ( 1798 ) )\n",
            "Currently at source number: 36/47 ( 1888 â 89 New Zealand Native football team )\n",
            "Currently at source number: 37/47 ( Battle of's Ridge )\n",
            "Currently at source number: 38/47 ( J. C. W.)\n",
            "Currently at source number: 39/47 ( Mount)\n",
            "Currently at source number: 40/47 ( Scientology in Germany )\n",
            "Currently at source number: 41/47 ( The Same Old Story )\n",
            "Currently at source number: 42/47 ( Training Day ( The Office ) )\n",
            "Currently at source number: 43/47 ( Butterfly World Tour )\n",
            "Currently at source number: 44/47 ( Frank)\n",
            "NLP Test Sentence Creation Time: 161.02 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from subprocess import run\n",
        "sys.path.append('/tf/.local/lib/python3.11/site-packages')\n",
        "run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"], check=True)\n",
        "run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"nltk\"], check=True)\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "start_time = timeit.default_timer()\n",
        "titles, sources, nltk_train_source_structure, nltk_train_found = create_source_structure(train_text, \"train-nltk\", train_titles, train_sources, nltk.sent_tokenize)\n",
        "nltk_time = timeit.default_timer() - start_time\n",
        "print(f\"NLTK Train Sentence Creation Time: {nltk_time:.2f} seconds\\n\")\n",
        "\n",
        "start_time = timeit.default_timer()\n",
        "titles, sources, nltk_test_source_structure, nltk_test_found = create_source_structure(test_text, \"test-nltk\", test_titles, test_sources, nltk.sent_tokenize)\n",
        "nltk_time = timeit.default_timer() - start_time\n",
        "print(f\"NLTK Test Sentence Creation Time: {nltk_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl5buLpGFEn8",
        "outputId": "144c252b-8490-4d9f-8735-9cedb387275f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pip in ./.local/lib/python3.11/site-packages (24.3.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /tf/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently at source number: 0/591 ( 2013 â 14 York City F.C. season )\n",
            "Currently at source number: 1/591 ( Big Boy ( song ) )\n",
            "Currently at source number: 2/591 ( The Remix ( Lady Gaga album ) )\n",
            "Currently at source number: 3/591 ( New Year 's Eve ( Up All Night ) )\n",
            "Currently at source number: 4/591 ( Geopyxis carbonaria )\n",
            "Currently at source number: 5/591 ( Cyclone Graham )\n",
            "Currently at source number: 6/591 ( M @-@ 108 ( Michigan highway ) )\n",
            "Currently at source number: 7/591 ( Simon Bradstreet )\n",
            "Currently at source number: 8/591 ( Ghost in the Shell : Stand Alone Complex - Solid State Society )\n",
            "Currently at source number: 9/591 ( Tropical Storm Jose ( 2005 ) )\n",
            "Currently at source number: 10/591 ( California State Route 243 )\n",
            "Currently at source number: 11/591 ( Dave Sisler )\n",
            "Currently at source number: 12/591 ( Gambia women 's national football team )\n",
            "Currently at source number: 13/591 ( Katherine Pulaski )\n",
            "Currently at source number: 14/591 ( Thom Darden )\n",
            "Currently at source number: 15/591 ( Vitamin D ( Glee ) )\n",
            "Currently at source number: 16/591 ( Fastra II )\n",
            "Currently at source number: 17/591 ( Livin ' the Dream )\n",
            "Currently at source number: 18/591 ( HMS Marlborough ( 1912 ) )\n",
            "Currently at source number: 19/591 ( The Sixth Extinction )\n",
            "Currently at source number: 20/591 ( Greens Ledge Light )\n",
            "Currently at source number: 21/591 ( Hannah Primrose , Countess of Rosebery )\n",
            "Currently at source number: 22/591 ( Mycena galericulata )\n",
            "Currently at source number: 23/591 ( Chagas disease )\n",
            "Currently at source number: 24/591 ( LiSA ( Japanese musician , born 1987 ) )\n",
            "Currently at source number: 25/591 ( Sorraia )\n",
            "Currently at source number: 26/591 ( Varanasi )\n",
            "Currently at source number: 27/591 ( Missouri River )\n",
            "Currently at source number: 28/591 ( Magadheera )\n",
            "Currently at source number: 29/591 ( HMS Boreas () )\n",
            "Currently at source number: 30/591 ( Cape lobster )\n",
            "Currently at source number: 31/591 ( Kitsune )\n",
            "Currently at source number: 32/591 ( Paul Thomas Anderson )\n",
            "Currently at source number: 33/591 ( General aviation in the United Kingdom )\n",
            "Currently at source number: 34/591 ( Gaboon viper )\n",
            "Currently at source number: 35/591 ( Lloyd Mathews )\n",
            "Currently at source number: 36/591 ( Jeremi WiÅniowiecki )\n",
            "Currently at source number: 37/591 ( 1939 Pacific hurricane season )\n",
            "Currently at source number: 38/591 ( St Peulan 's Church , Llanbeulan )\n",
            "Currently at source number: 39/591 ( Tawny nurse shark )\n",
            "Currently at source number: 40/591 ( Tropical Storm Abby ( 1964 ) )\n",
            "Currently at source number: 41/591 ( McAllister Tower Apartments )\n",
            "Currently at source number: 42/591 ( Odaenathus )\n",
            "Currently at source number: 43/591 ( Forbidden Fruit ( J. Cole song ) )\n",
            "Currently at source number: 44/591 ( Arihant @-@ class submarine )\n",
            "Currently at source number: 45/591 ( Charles @-@ Valentin Alkan )\n",
            "Currently at source number: 46/591 ( Jenova Chen )\n",
            "Currently at source number: 47/591 ( Mitsuyo Maeda )\n",
            "Currently at source number: 48/591 ( AIL Storm )\n",
            "Currently at source number: 49/591 ( Gold Beach )\n",
            "Currently at source number: 50/591 ( HMS Hostile () )\n",
            "Currently at source number: 51/591 ( Stanley Green )\n",
            "Currently at source number: 52/591 ( Hope Highway )\n",
            "Currently at source number: 53/591 ( Jin â Song Wars )\n",
            "Currently at source number: 54/591 ( Treaty of Ciudad JuÃ¡rez )\n",
            "Currently at source number: 55/591 ( Max Mosley )\n",
            "Currently at source number: 56/591 ( Superman : Escape from Krypton )\n",
            "Currently at source number: 57/591 ( Nebraska Highway 88 )\n",
            "Currently at source number: 58/591 ( Hurricane Abby ( 1960 ) )\n",
            "Currently at source number: 59/591 ( Haifa )\n",
            "Currently at source number: 60/591 ( Robbie Fowler )\n",
            "Currently at source number: 61/591 ( Protein )\n",
            "Currently at source number: 62/591 ( Tupolev Tu @-@ 12 )\n",
            "Currently at source number: 63/591 ( Freakum Dress )\n",
            "Currently at source number: 64/591 ( Plum cake )\n",
            "Currently at source number: 65/591 ( Aerith Gainsborough )\n",
            "Currently at source number: 66/591 ( Nina Simone )\n",
            "Currently at source number: 67/591 ( Simon de Montfort 's Parliament )\n",
            "Currently at source number: 68/591 ( Comair Flight 5191 )\n",
            "Currently at source number: 69/591 ( Ode to a Nightingale )\n",
            "Currently at source number: 70/591 ( The Crab with the Golden Claws )\n",
            "Currently at source number: 71/591 ( President Evil )\n",
            "Currently at source number: 72/591 ( Tessa NoÃ«l )\n",
            "Currently at source number: 73/591 ( The Secret of Monkey Island )\n",
            "Currently at source number: 74/591 ( Gerard ( archbishop of York ) )\n",
            "Currently at source number: 75/591 ( Typhoon Imbudo )\n",
            "Currently at source number: 76/591 ( Rio de Janeiro bid for the 2016 Summer Olympics )\n",
            "Currently at source number: 77/591 ( Action of 13 September 1810 )\n",
            "Currently at source number: 78/591 ( Amanita muscaria )\n",
            "Currently at source number: 79/591 ( Civilian Public Service )\n",
            "Currently at source number: 80/591 ( Ohio State Route 319 )\n",
            "Currently at source number: 81/591 ( 1973 Atlantic hurricane season )\n",
            "Currently at source number: 82/591 ( Mutinus elegans )\n",
            "Currently at source number: 83/591 ( Winston Tunnel )\n",
            "Currently at source number: 84/591 ( Leg before wicket )\n",
            "Currently at source number: 85/591 ( Corn crake )\n",
            "Currently at source number: 86/591 ( Chris Turner ( American football ) )\n",
            "Currently at source number: 87/591 ( 7 Independent Company ( Rhodesia ) )\n",
            "Currently at source number: 88/591 ( Bassline ( Chris Brown song ) )\n",
            "Currently at source number: 89/591 ( Monty Can 't Buy Me Love )\n",
            "Currently at source number: 90/591 ( Ha ' K 'in Xook )\n",
            "Currently at source number: 91/591 ( Fern Hobbs )\n",
            "Currently at source number: 92/591 ( Gregory Helms )\n",
            "Currently at source number: 93/591 ( 2007 Hawaii Bowl )\n",
            "Currently at source number: 94/591 ( Fernando Torres )\n",
            "Currently at source number: 95/591 ( The Goat Puzzle )\n",
            "Currently at source number: 96/591 ( The Moth ( Lost ) )\n",
            "Currently at source number: 97/591 ( The Tramp Dentists )\n",
            "Currently at source number: 98/591 ( O 'Brien @-@ class destroyer )\n",
            "Currently at source number: 99/591 ( Don 't Take It, Babe , It Just Ain 't Your Story )\n",
            "Currently at source number: 100/591 ( Far Away Places ( Mad Men ) )\n",
            "Currently at source number: 101/591 ( Old Pine Church )\n",
            "Currently at source number: 102/591 ( Toronto Magnetic and Meteorological Observatory )\n",
            "Currently at source number: 103/591 ( Ten Commandments in Catholic theology )\n",
            "Currently at source number: 104/591 ( Silver Bullet ( roller coaster ) )\n",
            "Currently at source number: 105/591 ( Mexico City Metropolitan Cathedral )\n",
            "Currently at source number: 106/591 ( Cell nucleus )\n",
            "Currently at source number: 107/591 ( Lady in the Lake trial )\n",
            "Currently at source number: 108/591 ( Languedoc @-@ Roussillon wine )\n",
            "Currently at source number: 109/591 ( Youth on the Prow , and Pleasure at the Helm )\n",
            "Currently at source number: 110/591 ( Tower Building of the Little Rock Arsenal )\n",
            "Currently at source number: 111/591 ( God 's Choice )\n",
            "Currently at source number: 112/591 ( West End Girls )\n",
            "Currently at source number: 113/591 ( Italian cruiser Aretusa )\n",
            "Currently at source number: 114/591 ( Henry HoÊ»olulu Pitman )\n",
            "Currently at source number: 115/591 ( Cyclone Herbie )\n",
            "Currently at source number: 116/591 ( HMS Comet () )\n",
            "Currently at source number: 117/591 ( Berhtwald )\n",
            "Currently at source number: 118/591 ( The General in His Labyrinth )\n",
            "Currently at source number: 119/591 ( Mothers of the Disappeared )\n",
            "Currently at source number: 120/591 ( Hurricane Tanya ( 1995 ) )\n",
            "Currently at source number: 121/591 ( 1998 National League Wild Card tie @-@ breaker game )\n",
            "Currently at source number: 122/591 ( Arbeideren ( Hamar ) )\n",
            "Currently at source number: 123/591 ( Galveston , Texas )\n",
            "Currently at source number: 124/591 ( Copia ( museum ) )\n",
            "Currently at source number: 125/591 ( Hugh Foliot )\n",
            "Currently at source number: 126/591 ( USS Breese ( DD @-@ 122 ) )\n",
            "Currently at source number: 127/591 ( Lisa the Simpson )\n",
            "Currently at source number: 128/591 ( Marauders ( Star Trek : Enterprise ) )\n",
            "Currently at source number: 129/591 ( Charmbracelet )\n",
            "Currently at source number: 130/591 ( Baltimore mayoral election , 1999 )\n",
            "Currently at source number: 131/591 ( Oribi )\n",
            "Currently at source number: 132/591 ( Domnall mac Murchada )\n",
            "Currently at source number: 133/591 ( Snow ( visual novel ) )\n",
            "Currently at source number: 134/591 ( World War I Memorial ( East Providence , Rhode Island ) )\n",
            "Currently at source number: 135/591 ( Exploration of Jupiter )\n",
            "Currently at source number: 136/591 ( Something Borrowed ( Torchwood ) )\n",
            "Currently at source number: 137/591 ( Kalyanasundara )\n",
            "Currently at source number: 138/591 ( Rachel Green )\n",
            "Currently at source number: 139/591 ( New York State Route 38 )\n",
            "Currently at source number: 140/591 ( Valkyria Chronicles III )\n",
            "Currently at source number: 141/591 ( Oldham )\n",
            "Currently at source number: 142/591 ( Norsk Spisevognselskap )\n",
            "Currently at source number: 143/591 ( The Son Also Draws )\n",
            "Currently at source number: 144/591 ( Trinsey v. Pennsylvania )\n",
            "Currently at source number: 145/591 ( A Month in the Country ( film ) )\n",
            "Currently at source number: 146/591 ( Hadji Ali )\n",
            "Currently at source number: 147/591 ( Frank Slide )\n",
            "Currently at source number: 148/591 ( Soviet cruiser Krasnyi Kavkaz )\n",
            "Currently at source number: 149/591 ( Star )\n",
            "Currently at source number: 150/591 ( Lost Horizons ( Lemon Jelly album ) )\n",
            "Currently at source number: 151/591 ( Irresistible ( The X @-@ Files ) )\n",
            "Currently at source number: 152/591 ( Lock Haven , Pennsylvania )\n",
            "Currently at source number: 153/591 ( RÃ©union ibis )\n",
            "Currently at source number: 154/591 ( M @-@ 114 ( Michigan highway ) )\n",
            "Currently at source number: 155/591 ( Liu Kang )\n",
            "Currently at source number: 156/591 ( Architecture of the Song dynasty )\n",
            "Currently at source number: 157/591 ( Burning of women in England )\n",
            "Currently at source number: 158/591 ( Loverboy ( Mariah Carey song ) )\n",
            "Currently at source number: 159/591 ( Blackwyche )\n",
            "Currently at source number: 160/591 ( LÃ¡grimas CÃ¡lidas )\n",
            "Currently at source number: 161/591 ( Marshall Applewhite )\n",
            "Currently at source number: 162/591 ( Ed Barrow )\n",
            "Currently at source number: 163/591 ( August ( Fringe ) )\n",
            "Currently at source number: 164/591 ( White Dog ( Gary novel ) )\n",
            "Currently at source number: 165/591 ( Waterfall Gully , South Australia )\n",
            "Currently at source number: 166/591 ( SMS ZrÃ­nyi )\n",
            "Currently at source number: 167/591 ( Moment of Surrender )\n",
            "Currently at source number: 168/591 ( Paranthodon )\n",
            "Currently at source number: 169/591 ( 1940 Atlantic hurricane season )\n",
            "Currently at source number: 170/591 ( Portuguese ironclad Vasco da Gama )\n",
            "Currently at source number: 171/591 ( Biddenden Maids )\n",
            "Currently at source number: 172/591 ( 1806 Great Coastal hurricane )\n",
            "Currently at source number: 173/591 ( Don 't You Wanna Stay )\n",
            "Currently at source number: 174/591 ( Devin Townsend )\n",
            "Currently at source number: 175/591 ( England national rugby union team )\n",
            "Currently at source number: 176/591 ( Jane Dudley , Duchess of Northumberland )\n",
            "Currently at source number: 177/591 ( Spanish Hill )\n",
            "Currently at source number: 178/591 ( In Bloom )\n",
            "Currently at source number: 179/591 ( Mole cricket )\n",
            "Currently at source number: 180/591 ( Wales national rugby union team )\n",
            "Currently at source number: 181/591 ( USS Illinois ( BB @-@ 7 ) )\n",
            "Currently at source number: 182/591 ( Christmas 1994 nor 'easter )\n",
            "Currently at source number: 183/591 ( Yamaha NS @-@ 10 )\n",
            "Currently at source number: 184/591 ( I Am Unicorn )\n",
            "Currently at source number: 185/591 ( Hurricane Felicia ( 2009 ) )\n",
            "Currently at source number: 186/591 ( Yo @-@ Yo ( Nicola Roberts song ) )\n",
            "Currently at source number: 187/591 ( The Food Album )\n",
            "Currently at source number: 188/591 ( Islais Creek )\n",
            "Currently at source number: 189/591 ( Super Mario Land )\n",
            "Currently at source number: 190/591 ( Ceratopsia )\n",
            "Currently at source number: 191/591 ( Santa @-@ Fe ( Bob Dylan song ) )\n",
            "Currently at source number: 192/591 ( Strand , London )\n",
            "Currently at source number: 193/591 ( Live & Kicking )\n",
            "Currently at source number: 194/591 ( Weather buoy )\n",
            "Currently at source number: 195/591 ( Elgin Cathedral )\n",
            "Currently at source number: 196/591 ( Hi , Infidelity )\n",
            "Currently at source number: 197/591 ( Polka Party ! )\n",
            "Currently at source number: 198/591 ( Van Morrison : Too Late to Stop Now )\n",
            "Currently at source number: 199/591 ( True Blue ( Madonna song ) )\n",
            "Currently at source number: 200/591 ( Dover Athletic F.C. )\n",
            "Currently at source number: 201/591 ( June 1941 uprising in eastern Herzegovina )\n",
            "Currently at source number: 202/591 ( Stay @-@ at @-@ home dad )\n",
            "Currently at source number: 203/591 ( Stephanolepis cirrhifer )\n",
            "Currently at source number: 204/591 ( M @-@ 47 ( Michigan highway ) )\n",
            "Currently at source number: 205/591 ( G.I. Joe : Retaliation )\n",
            "Currently at source number: 206/591 ( Perry the)\n",
            "Currently at source number: 207/591 ( 1986 Peach Bowl )\n",
            "Currently at source number: 208/591 ( Midge ( Barbie ) )\n",
            "Currently at source number: 209/591 ( Tina Fey )\n",
            "Currently at source number: 210/591 ( Westminster Assembly )\n",
            "Currently at source number: 211/591 ( Boise National Forest )\n",
            "Currently at source number: 212/591 ( History of Bradford City)\n",
            "Currently at source number: 213/591 ( Tropical Storm Domoina )\n",
            "Currently at source number: 214/591 ( Corpus Christi Bay )\n",
            "Currently at source number: 215/591 ( Canning Dam )\n",
            "Currently at source number: 216/591 ( Tropical Storm Brenda ( 1960 ) )\n",
            "Currently at source number: 217/591 ( Giacomo Meyerbeer )\n",
            "Currently at source number: 218/591 ( First Battle of Maryang San )\n",
            "Currently at source number: 219/591 ( Curtis Woodhouse )\n",
            "Currently at source number: 220/591 ( Mount Jackson ( Antarctica ) )\n",
            "Currently at source number: 221/591 ( Stanley Price Weir )\n",
            "Currently at source number: 222/591 ( Rosemary 's Baby ( 30 Rock ) )\n",
            "Currently at source number: 223/591 ( 1981 European Cup Final )\n",
            "Currently at source number: 224/591 ( Kir 'Shara )\n",
            "Currently at source number: 225/591 ( Acute myeloid leukemia )\n",
            "Currently at source number: 226/591 ( Perfect Dark ( 2010 video game ) )\n",
            "Currently at source number: 227/591 ( Sentence spacing )\n",
            "Currently at source number: 228/591 ( Edward Creutz )\n",
            "Currently at source number: 229/591 ( Cambodian Campaign )\n",
            "Currently at source number: 230/591 ( Music of Chrono Cross )\n",
            "Currently at source number: 231/591 ( Rob Howard )\n",
            "Currently at source number: 232/591 ( Gertrude Barrows Bennett )\n",
            "Currently at source number: 233/591 ( Jifna )\n",
            "Currently at source number: 234/591 ( S.R. 819 )\n",
            "Currently at source number: 235/591 ( SMS Markgraf )\n",
            "Currently at source number: 236/591 ( Polish culture during World War II )\n",
            "Currently at source number: 237/591 ( Antimony )\n",
            "Currently at source number: 238/591 ( Tropical Storm Olaf ( 1997 ) )\n",
            "Currently at source number: 239/591 ( Hugh Walpole )\n",
            "Currently at source number: 240/591 ( Richard Cresswell )\n",
            "Currently at source number: 241/591 ( Who Am I ( Casting Crowns song ) )\n",
            "Currently at source number: 242/591 ( Josce de Dinan )\n",
            "Currently at source number: 243/591 ( Erving Goffman )\n",
            "Currently at source number: 244/591 ( Key ( basketball ) )\n",
            "Currently at source number: 245/591 ( 130th Engineer Brigade ( United States ) )\n",
            "Currently at source number: 246/591 ( Stop ! ! Hibari @-@ kun ! )\n",
            "Currently at source number: 247/591 ( Pattycake ( gorilla ) )\n",
            "Currently at source number: 248/591 ( Sister Wives )\n",
            "Currently at source number: 249/591 ( SM U @-@ 3 ( Austria @-@ Hungary ) )\n",
            "Currently at source number: 250/591 ( St Caffo 's Church , Llangaffo )\n",
            "Currently at source number: 251/591 ( Old Baltimore Pike )\n",
            "Currently at source number: 252/591 ( Tiber Oil Field )\n",
            "Currently at source number: 253/591 ( Battle of Hubbardton )\n",
            "Currently at source number: 254/591 ( Jane 's Attack Squadron )\n",
            "Currently at source number: 255/591 ( Hoyt Wilhelm )\n",
            "Currently at source number: 256/591 ( Journey ( 2012 video game ) )\n",
            "Currently at source number: 257/591 ( Astraeus hygrometricus )\n",
            "Currently at source number: 258/591 ( Sinclair Sovereign )\n",
            "Currently at source number: 259/591 ( Ceres ( dwarf planet ) )\n",
            "Currently at source number: 260/591 ( French cruiser Sully )\n",
            "Currently at source number: 261/591 ( Washington State Route 516 )\n",
            "Currently at source number: 262/591 ( Sweet Love ( Chris Brown song ) )\n",
            "Currently at source number: 263/591 ( First Ostend Raid )\n",
            "Currently at source number: 264/591 ( The Boat Race 1900 )\n",
            "Currently at source number: 265/591 ( U2 concert in Sarajevo )\n",
            "Currently at source number: 266/591 ( 2 / 4th Machine Gun Battalion ( Australia ) )\n",
            "Currently at source number: 267/591 ( Man Down ( song ) )\n",
            "Currently at source number: 268/591 ( Tintin in the Congo )\n",
            "Currently at source number: 269/591 ( Charles Eaton ( RAAF officer ) )\n",
            "Currently at source number: 270/591 ( 2010 Alabama Crimson Tide football team )\n",
            "Currently at source number: 271/591 ( Roger Federer )\n",
            "Currently at source number: 272/591 ( K @-@ 22 ( Kansas highway ) )\n",
            "Currently at source number: 273/591 ( Nicole Franklin )\n",
            "Currently at source number: 274/591 ( West Hendford Cricket Ground , Yeovil )\n",
            "Currently at source number: 275/591 ( Ãmar mac Arailt )\n",
            "Currently at source number: 276/591 ( Henry of Grosmont , 1st Duke of Lancaster )\n",
            "Currently at source number: 277/591 ( Arizona State Route 67 )\n",
            "Currently at source number: 278/591 ( Rockstar 101 )\n",
            "Currently at source number: 279/591 ( Love Me Like You )\n",
            "Currently at source number: 280/591 ( Sclerodermatineae )\n",
            "Currently at source number: 281/591 ( L.A.M.B. )\n",
            "Currently at source number: 282/591 ( Clocks ( song ) )\n",
            "Currently at source number: 283/591 ( Crush ( video game ) )\n",
            "Currently at source number: 284/591 ( Temple of Eshmun )\n",
            "Currently at source number: 285/591 ( Iguanodon )\n",
            "Currently at source number: 286/591 ( Sandwich Day )\n",
            "Currently at source number: 287/591 ( Croatian independence referendum , 1991 )\n",
            "Currently at source number: 288/591 ( Truth in Numbers ? )\n",
            "Currently at source number: 289/591 ( Pokiri )\n",
            "Currently at source number: 290/591 ( Banai ( goddess ) )\n",
            "Currently at source number: 291/591 ( Etymology of Wicca )\n",
            "Currently at source number: 292/591 ( The Stolen Eagle )\n",
            "Currently at source number: 293/591 ( Battle of Merville Gun Battery )\n",
            "Currently at source number: 294/591 ( The Tempest ( album ) )\n",
            "Currently at source number: 295/591 ( Eva PerÃ³n )\n",
            "Currently at source number: 296/591 ( 1955 Atlantic hurricane season )\n",
            "Currently at source number: 297/591 ( Route 261 ( Delaware â Pennsylvania ) )\n",
            "Currently at source number: 298/591 ( Otra Nota )\n",
            "Currently at source number: 299/591 ( Awakening ( Star Trek : Enterprise ) )\n",
            "Currently at source number: 300/591 ( John Cullen )\n",
            "Currently at source number: 301/591 ( Karamokho Alfa )\n",
            "Currently at source number: 302/591 ( Michelle Rzepecki )\n",
            "Currently at source number: 303/591 ( Boy @-@ Scoutz ' n the Hood )\n",
            "Currently at source number: 304/591 ( Politics of Croatia )\n",
            "Currently at source number: 305/591 ( Super Science Stories )\n",
            "Currently at source number: 306/591 ( 2010 Haiti earthquake )\n",
            "Currently at source number: 307/591 ( Cole Hamels )\n",
            "Currently at source number: 308/591 ( Art Ross )\n",
            "Currently at source number: 309/591 ( New York State Route 448 )\n",
            "Currently at source number: 310/591 ( Subtropical Storm Alpha ( 1972 ) )\n",
            "Currently at source number: 311/591 ( George N. Briggs )\n",
            "Currently at source number: 312/591 ( Andrew Johnston ( singer ) )\n",
            "Currently at source number: 313/591 ( The Actor 's Children )\n",
            "Currently at source number: 314/591 ( Qedarite )\n",
            "Currently at source number: 315/591 ( Elephanta Caves )\n",
            "Currently at source number: 316/591 ( 1981 Peach Bowl ( January ) )\n",
            "Currently at source number: 317/591 ( Rebbie Jackson )\n",
            "Currently at source number: 318/591 ( The One I Love ( manga ) )\n",
            "Currently at source number: 319/591 ( John of Brienne )\n",
            "Currently at source number: 320/591 ( Hoysala literature )\n",
            "Currently at source number: 321/591 ( James Robert Baker )\n",
            "Currently at source number: 322/591 ( Art in Medieval Scotland )\n",
            "Currently at source number: 323/591 ( Sang Pencerah )\n",
            "Currently at source number: 324/591 ( Grade I listed buildings in Somerset )\n",
            "Currently at source number: 325/591 ( Directed acyclic graph )\n",
            "Currently at source number: 326/591 ( WASP @-@ 44 )\n",
            "Currently at source number: 327/591 ( Odyssey Number Five )\n",
            "Currently at source number: 328/591 ( The Convict ( 1910 film ) )\n",
            "Currently at source number: 329/591 ( Tristan ( horse ) )\n",
            "Currently at source number: 330/591 ( Ulysses ( poem ) )\n",
            "Currently at source number: 331/591 ( Jack and Jill ( nursery rhyme ) )\n",
            "Currently at source number: 332/591 ( Allah )\n",
            "Currently at source number: 333/591 ( Romanian Land Forces )\n",
            "Currently at source number: 334/591 ( Bart vs. Australia )\n",
            "Currently at source number: 335/591 ( T30 Howitzer Motor Carriage )\n",
            "Currently at source number: 336/591 ( Halo : Uprising )\n",
            "Currently at source number: 337/591 ( You Only Live Twice ( film ) )\n",
            "Currently at source number: 338/591 ( Laurence Olivier )\n",
            "Currently at source number: 339/591 ( Track and field )\n",
            "Currently at source number: 340/591 ( Yoko Shimomura )\n",
            "Currently at source number: 341/591 ( Condom )\n",
            "Currently at source number: 342/591 ( Wilhelm Busch )\n",
            "Currently at source number: 343/591 ( The Family Jewels ( Marina and the Diamonds album ) )\n",
            "Currently at source number: 344/591 ( Royal prerogative in the United Kingdom )\n",
            "Currently at source number: 345/591 ( Patriarchal Cathedral of the Holy Ascension of God )\n",
            "Currently at source number: 346/591 ( Roxas ( Kingdom Hearts ) )\n",
            "Currently at source number: 347/591 ( Texas A & M Singing Cadets )\n",
            "Currently at source number: 348/591 ( Battle of Binh)\n",
            "Currently at source number: 349/591 ( Harold Innis )\n",
            "Currently at source number: 350/591 ( Type 94 Nambu pistol )\n",
            "Currently at source number: 351/591 ( Inocybe praetervisa )\n",
            "Currently at source number: 352/591 ( Saint Leonard Catholic Church ( Madison , Nebraska ) )\n",
            "Currently at source number: 353/591 ( Project Chanology )\n",
            "Currently at source number: 354/591 ( New York State Route 368 )\n",
            "Currently at source number: 355/591 ( Mozambican War of Independence )\n",
            "Currently at source number: 356/591 ( Territorial era of Minnesota )\n",
            "Currently at source number: 357/591 ( James Nesbitt )\n",
            "Currently at source number: 358/591 ( Skye )\n",
            "Currently at source number: 359/591 ( Thomas Quiney )\n",
            "Currently at source number: 360/591 ( The Fox , the Wolf and the Husbandman )\n",
            "Currently at source number: 361/591 ( Bossy ( Lindsay Lohan song ) )\n",
            "Currently at source number: 362/591 ( Underneath ( The X @-@ Files ) )\n",
            "Currently at source number: 363/591 ( Arikamedu )\n",
            "Currently at source number: 364/591 ( Shaoguan incident )\n",
            "Currently at source number: 365/591 ( Humpty Dumpty )\n",
            "Currently at source number: 366/591 ( Bob Dylan )\n",
            "Currently at source number: 367/591 ( Kakapo )\n",
            "Currently at source number: 368/591 ( Berg ( station ) )\n",
            "Currently at source number: 369/591 ( Noisy miner )\n",
            "Currently at source number: 370/591 ( Michael Jordan )\n",
            "Currently at source number: 371/591 ( Crash Boom Bang ! )\n",
            "Currently at source number: 372/591 ( Cougar )\n",
            "Currently at source number: 373/591 ( Joe Nathan )\n",
            "Currently at source number: 374/591 ( Blackburn Firecrest )\n",
            "Currently at source number: 375/591 ( Zygoballus sexpunctatus )\n",
            "Currently at source number: 376/591 ( Forward Intelligence Team )\n",
            "Currently at source number: 377/591 ( Adams River ( British Columbia ) )\n",
            "Currently at source number: 378/591 ( Bodyline )\n",
            "Currently at source number: 379/591 ( World War Z )\n",
            "Currently at source number: 380/591 ( Hellblazer )\n",
            "Currently at source number: 381/591 ( History of artificial intelligence )\n",
            "Currently at source number: 382/591 ( Plunketts Creek ( Loyalsock Creek ) )\n",
            "Currently at source number: 383/591 ( USS Atlanta ( 1861 ) )\n",
            "Currently at source number: 384/591 ( Voyage : Inspired by Jules Verne )\n",
            "Currently at source number: 385/591 ( Coldrum Long Barrow )\n",
            "Currently at source number: 386/591 ( Back to Tennessee ( song ) )\n",
            "Currently at source number: 387/591 ( Murder of Tom ap Rhys Pryce )\n",
            "Currently at source number: 388/591 ( Why Does It Hurt So Bad )\n",
            "Currently at source number: 389/591 ( Johnny McNichol )\n",
            "Currently at source number: 390/591 ( T. Arthur Cottam )\n",
            "Currently at source number: 391/591 ( Trials and Tribble @-@ ations )\n",
            "Currently at source number: 392/591 ( Mega Man & Bass )\n",
            "Currently at source number: 393/591 ( 2011 â 12 Columbus Blue Jackets season )\n",
            "Currently at source number: 394/591 ( The Magdalen Reading )\n",
            "Currently at source number: 395/591 ( Caught Up ( Usher song ) )\n",
            "Currently at source number: 396/591 ( Common starling )\n",
            "Currently at source number: 397/591 ( Hurricane Lorenzo ( 2007 ) )\n",
            "Currently at source number: 398/591 ( The Good Terrorist )\n",
            "Currently at source number: 399/591 ( New Jersey Route 65 )\n",
            "Currently at source number: 400/591 ( Mark Stockwell )\n",
            "Currently at source number: 401/591 ( Dangerously in Love Tour )\n",
            "Currently at source number: 402/591 ( Le souper de Beaucaire )\n",
            "Currently at source number: 403/591 ( HMS Black Prince ( 1904 ) )\n",
            "Currently at source number: 404/591 ( 1990 Pacific hurricane season )\n",
            "Currently at source number: 405/591 ( The Archaeology of Ritual and Magic )\n",
            "Currently at source number: 406/591 ( Chasing Vermeer )\n",
            "Currently at source number: 407/591 ( Aston Villa F.C. )\n",
            "Currently at source number: 408/591 ( Daydream ( Mariah Carey album ) )\n",
            "Currently at source number: 409/591 ( Stuart McCall )\n",
            "Currently at source number: 410/591 ( Florida State Road 878 )\n",
            "Currently at source number: 411/591 ( Business School ( The Office ) )\n",
            "Currently at source number: 412/591 ( Kedok Ketawa )\n",
            "Currently at source number: 413/591 ( Welsh National Opera )\n",
            "Currently at source number: 414/591 ( Isabella Beeton )\n",
            "Currently at source number: 415/591 ( Middle Colonies )\n",
            "Currently at source number: 416/591 ( Frederick Reines )\n",
            "Currently at source number: 417/591 ( M @-@ 122 ( Michigan highway ) )\n",
            "Currently at source number: 418/591 ( Imagine ( John Lennon song ) )\n",
            "Currently at source number: 419/591 ( Black @-@ tailed jackrabbit )\n",
            "Currently at source number: 420/591 ( Independence Day ( India ) )\n",
            "Currently at source number: 421/591 ( Glorious First of June )\n",
            "Currently at source number: 422/591 ( Residence of the United States Ambassador to the United Nations )\n",
            "Currently at source number: 423/591 ( Hurricane Ingrid )\n",
            "Currently at source number: 424/591 ( Moro River Campaign )\n",
            "Currently at source number: 425/591 ( The Boat Race 1999 )\n",
            "Currently at source number: 426/591 ( Rhode Island Route 4 )\n",
            "Currently at source number: 427/591 ( Europium )\n",
            "Currently at source number: 428/591 ( North @-@ Eastern Area Command ( RAAF ) )\n",
            "Currently at source number: 429/591 ( Kaboom ( Parks and Recreation ) )\n",
            "Currently at source number: 430/591 ( Survivor Series ( 1992 ) )\n",
            "Currently at source number: 431/591 ( Illinois ( Sufjan Stevens album ) )\n",
            "Currently at source number: 432/591 ( Dota 2 )\n",
            "Currently at source number: 433/591 ( Partington )\n",
            "Currently at source number: 434/591 ( Alice in Chains )\n",
            "Currently at source number: 435/591 ( Landing at Anzac Cove )\n",
            "Currently at source number: 436/591 ( M @-@ 5 ( Michigan highway ) )\n",
            "Currently at source number: 437/591 ( Sholay )\n",
            "Currently at source number: 438/591 ( Memory Almost Full )\n",
            "Currently at source number: 439/591 ( Florida Atlantic University )\n",
            "Currently at source number: 440/591 ( Ouw Peh Tjoa )\n",
            "Currently at source number: 441/591 ( Saprang Kalayanamitr )\n",
            "Currently at source number: 442/591 ( Road to the North Pole )\n",
            "Currently at source number: 443/591 ( Corythosaurus )\n",
            "Currently at source number: 444/591 ( Of Human Feelings )\n",
            "Currently at source number: 445/591 ( Hurricane Uleki )\n",
            "Currently at source number: 446/591 ( Richard Nixon presidential campaign , 1968 )\n",
            "Currently at source number: 447/591 ( Maryland Route 194 )\n",
            "Currently at source number: 448/591 ( Berkley Bedell )\n",
            "Currently at source number: 449/591 ( Cadmium )\n",
            "Currently at source number: 450/591 ( On the Pulse of Morning )\n",
            "Currently at source number: 451/591 ( Zagreb Synagogue )\n",
            "Currently at source number: 452/591 ( Joyful , Joyful )\n",
            "Currently at source number: 453/591 ( The Importance of Being Earnest )\n",
            "Currently at source number: 454/591 ( WASP @-@ 13b )\n",
            "Currently at source number: 455/591 ( Hurricane Omar ( 2008 ) )\n",
            "Currently at source number: 456/591 ( First @-@ move advantage in chess )\n",
            "Currently at source number: 457/591 ( Cater 2 U )\n",
            "Currently at source number: 458/591 ( Parliament Act 1911 )\n",
            "Currently at source number: 459/591 ( Crown Fountain )\n",
            "Currently at source number: 460/591 ( Hurricane Dot ( 1959 ) )\n",
            "Currently at source number: 461/591 ( USS O 'Brien ( DD @-@ 51 ) )\n",
            "Currently at source number: 462/591 ( Sovetsky Soyuz @-@ class battleship )\n",
            "Currently at source number: 463/591 ( Transit of Venus )\n",
            "Currently at source number: 464/591 ( Ernie Cooksey )\n",
            "Currently at source number: 465/591 ( Mariana ( poem ) )\n",
            "Currently at source number: 466/591 ( A4232 road )\n",
            "Currently at source number: 467/591 ( SS El Sol )\n",
            "Currently at source number: 468/591 ( No. 79 Wing RAAF )\n",
            "Currently at source number: 469/591 ( Papal conclave , 1769 )\n",
            "Currently at source number: 470/591 ( Christine Hakim )\n",
            "Currently at source number: 471/591 ( DuMont Television Network )\n",
            "Currently at source number: 472/591 ( Tales of Destiny 2 )\n",
            "Currently at source number: 473/591 ( Carre 's Grammar School )\n",
            "Currently at source number: 474/591 ( Lactarius indigo )\n",
            "Currently at source number: 475/591 ( Somerset County Cricket Club in 2009 )\n",
            "Currently at source number: 476/591 ( Mogadishu )\n",
            "Currently at source number: 477/591 ( Diamond stingray )\n",
            "Currently at source number: 478/591 ( Djedkare Isesi )\n",
            "Currently at source number: 479/591 ( Khoo@-@)\n",
            "Currently at source number: 480/591 ( Hoover Dam )\n",
            "Currently at source number: 481/591 ( Leslie Andrew )\n",
            "Currently at source number: 482/591 ( Ãlfric of Abingdon )\n",
            "Currently at source number: 483/591 ( Principe Amedeo @-@ class ironclad )\n",
            "Currently at source number: 484/591 ( If I Never See Your Face Again )\n",
            "Currently at source number: 485/591 ( Plain maskray )\n",
            "Currently at source number: 486/591 ( Hydnellum peckii )\n",
            "Currently at source number: 487/591 ( Gregorian Tower )\n",
            "Currently at source number: 488/591 ( Rocky Mountain Horse )\n",
            "Currently at source number: 489/591 ( Kyra ( Charmed ) )\n",
            "Currently at source number: 490/591 ( Derfflinger @-@ class battlecruiser )\n",
            "Currently at source number: 491/591 ( Church of Christ Pantocrator , Nesebar )\n",
            "Currently at source number: 492/591 ( 2016 Spanish Grand Prix )\n",
            "Currently at source number: 493/591 ( Harajuku Lovers Tour )\n",
            "Currently at source number: 494/591 ( Portrait of Monsieur Bertin )\n",
            "Currently at source number: 495/591 ( My Boo ( Usher and Alicia Keys song ) )\n",
            "Currently at source number: 496/591 ( St Mary 's Church , Rhodogeidio )\n",
            "Currently at source number: 497/591 ( Ancient Egyptian deities )\n",
            "Currently at source number: 498/591 ( Central Area Command ( RAAF ) )\n",
            "Currently at source number: 499/591 ( Wrapped in Red )\n",
            "Currently at source number: 500/591 ( Crosby Garrett Helmet )\n",
            "Currently at source number: 501/591 ( St Mary 's Church ,Alderley )\n",
            "Currently at source number: 502/591 ( Not Quite Hollywood : The Wild , Untold Story of Ozploitation ! )\n",
            "Currently at source number: 503/591 ( You 're Gonna Love Tomorrow )\n",
            "Currently at source number: 504/591 ( Stefan Wever )\n",
            "Currently at source number: 505/591 ( Adam Stansfield )\n",
            "Currently at source number: 506/591 ( Zhou Tong ( archer ) )\n",
            "Currently at source number: 507/591 ( The Clean Tech Revolution )\n",
            "Currently at source number: 508/591 ( The Amps )\n",
            "Currently at source number: 509/591 ( The Dreamscape )\n",
            "Currently at source number: 510/591 ( Washington State Route 221 )\n",
            "Currently at source number: 511/591 ( The Wave ( Miike Snow song ) )\n",
            "Currently at source number: 512/591 ( Trees ( poem ) )\n",
            "Currently at source number: 513/591 ( 2008 Bahrain Grand Prix )\n",
            "Currently at source number: 514/591 ( Thunderbirds ( TV series ) )\n",
            "Currently at source number: 515/591 ( Bath Assembly Rooms )\n",
            "Currently at source number: 516/591 ( George Calvert , 1st Baron Baltimore )\n",
            "Currently at source number: 517/591 ( Laborintus II ( 2012 recording ) )\n",
            "Currently at source number: 518/591 ( Loose ( Nelly Furtado album ) )\n",
            "Currently at source number: 519/591 ( Guitar Hero )\n",
            "Currently at source number: 520/591 ( Banksia violacea )\n",
            "Currently at source number: 521/591 ( Georgian scripts )\n",
            "Currently at source number: 522/591 ( Fear of Flying ( The Simpsons ) )\n",
            "Currently at source number: 523/591 ( Battle of Tellicherry )\n",
            "Currently at source number: 524/591 ( Chapter 1 ( House of Cards ) )\n",
            "Currently at source number: 525/591 ( Ontario Highway 36 )\n",
            "Currently at source number: 526/591 ( Miss Meyers )\n",
            "Currently at source number: 527/591 ( There 's Got to Be a Way )\n",
            "Currently at source number: 528/591 ( Development of Fez )\n",
            "Currently at source number: 529/591 ( Martin Keamy )\n",
            "Currently at source number: 530/591 ( Verpa bohemica )\n",
            "Currently at source number: 531/591 ( Ghost in the Shell ( 1995 film ) )\n",
            "Currently at source number: 532/591 ( The Secret ( The Office ) )\n",
            "Currently at source number: 533/591 ( Barbarian II : The Dungeon of Drax )\n",
            "Currently at source number: 534/591 ( New York State Route 185 )\n",
            "Currently at source number: 535/591 ( Amylostereum )\n",
            "Currently at source number: 536/591 ( Tommy Lawton )\n",
            "Currently at source number: 537/591 ( Transportation in Omaha )\n",
            "Currently at source number: 538/591 ( Species ( film ) )\n",
            "Currently at source number: 539/591 ( Until the Whole World Hears )\n",
            "Currently at source number: 540/591 ( The Litigators )\n",
            "Currently at source number: 541/591 ( Species of Allosaurus )\n",
            "Currently at source number: 542/591 ( December 1964 South Vietnamese coup )\n",
            "Currently at source number: 543/591 ( Norman Finkelstein )\n",
            "Currently at source number: 544/591 ( Sarnia )\n",
            "Currently at source number: 545/591 ( We 'll Always Have Paris ( Star Trek : The Next Generation ) )\n",
            "Currently at source number: 546/591 ( The Feast of the Goat )\n",
            "Currently at source number: 547/591 ( E. W. Hornung )\n",
            "Currently at source number: 548/591 ( Johnson â Corey â Chaykovsky reaction )\n",
            "Currently at source number: 549/591 ( Kaimanawa horse )\n",
            "Currently at source number: 550/591 ( First Light ( Rebecca Stead novel ) )\n",
            "Currently at source number: 551/591 ( Last Exit on Brooklyn )\n",
            "Currently at source number: 552/591 ( Brandon Minor )\n",
            "Currently at source number: 553/591 ( M @-@ 6 ( Michigan highway ) )\n",
            "Currently at source number: 554/591 ( History of Braathens SAFE ( 1946 â 93 ) )\n",
            "Currently at source number: 555/591 ( Crazy in Love )\n",
            "Currently at source number: 556/591 ( Burns ' Heir )\n",
            "Currently at source number: 557/591 ( Jacob deGrom )\n",
            "Currently at source number: 558/591 ( Arnhem Oosterbeek War Cemetery )\n",
            "Currently at source number: 559/591 ( Burn )\n",
            "Currently at source number: 560/591 ( Grammy Award for Best Concept Music Video )\n",
            "Currently at source number: 561/591 ( Mumia Abu @-@ Jamal )\n",
            "Currently at source number: 562/591 ( SMS Erzherzog Ferdinand Max )\n",
            "Currently at source number: 563/591 ( Utah State Route 61 )\n",
            "Currently at source number: 564/591 ( Maggie Simpson )\n",
            "Currently at source number: 565/591 ( Hurricane Flossy ( 1956 ) )\n",
            "Currently at source number: 566/591 ( Jacqueline Fernandez )\n",
            "Currently at source number: 567/591 ( God of War video game collections )\n",
            "Currently at source number: 568/591 ( Flash Gordon Strange Adventure Magazine )\n",
            "Currently at source number: 569/591 ( Protomycena )\n",
            "Currently at source number: 570/591 ( South of Heaven )\n",
            "Currently at source number: 571/591 ( 2nd Battalion 9th Marines )\n",
            "Currently at source number: 572/591 ( Battle of Romani )\n",
            "Currently at source number: 573/591 ( Fort Glanville Conservation Park )\n",
            "Currently at source number: 574/591 ( New York State Route 164 )\n",
            "Currently at source number: 575/591 ( Ace Attorney )\n",
            "Currently at source number: 576/591 ( St Nazaire Raid )\n",
            "Currently at source number: 577/591 ( Krak des Chevaliers )\n",
            "Currently at source number: 578/591 ( Mortimer Wheeler )\n",
            "Currently at source number: 579/591 ( TautiÅ¡ka giesmÄ )\n",
            "Currently at source number: 580/591 ( Ode on Indolence )\n",
            "Currently at source number: 581/591 ( Galentine 's Day )\n",
            "Currently at source number: 582/591 ( M @-@ 81 ( Michigan highway ) )\n",
            "Currently at source number: 583/591 ( Hibiscus ( restaurant ) )\n",
            "Currently at source number: 584/591 ( Vistara )\n",
            "Currently at source number: 585/591 ( ToninÃ¡ )\n",
            "Currently at source number: 586/591 ( Invisible rail )\n",
            "Currently at source number: 587/591 ( Xenon )\n",
            "Currently at source number: 588/591 ( Gold dollar )\n",
            "NLTK Train Sentence Creation Time: 1.41 seconds\n",
            "\n",
            "Currently at source number: 0/47 ( Tropical Storm( 2008 ) )\n",
            "Currently at source number: 1/47 ( Calvin)\n",
            "Currently at source number: 2/47 ( The Boat Race 2008 )\n",
            "Currently at source number: 3/47 ( Angel of Death ( Slayer song ) )\n",
            "Currently at source number: 4/47 ( 2011 â 12 Michigan Wolverines men 's basketball team )\n",
            "Currently at source number: 5/47 ( Firstinvasion of Burma )\n",
            "Currently at source number: 6/47 ( New Jersey Route 29 )\n",
            "Currently at source number: 7/47 ( Daniel)\n",
            "Currently at source number: 8/47 ( New Jersey Route 50 )\n",
            "Currently at source number: 9/47 ( Military history of Australia )\n",
            "Currently at source number: 10/47 ( Battle of theRiver )\n",
            "Currently at source number: 11/47 ( TheBlues )\n",
            "Currently at source number: 12/47 ( Homarus gammarus )\n",
            "Currently at source number: 13/47 ( Tim Richmond )\n",
            "Currently at source number: 14/47 ( High Five Interchange )\n",
            "Currently at source number: 15/47 ( Meridian , Mississippi )\n",
            "Currently at source number: 16/47 ( M @-@ 82 ( Michigan highway ) )\n",
            "Currently at source number: 17/47 ( Sonic the Hedgehog ( 1991 video game ) )\n",
            "Currently at source number: 18/47 ( U.S. Route 2 in Michigan )\n",
            "Currently at source number: 19/47 ( 1985assassination plot )\n",
            "Currently at source number: 20/47 ( Hurricane( 2011 ) )\n",
            "Currently at source number: 21/47 ( Home MadePie )\n",
            "Currently at source number: 22/47 ( Exit Through the@-@ E @-@ Mart )\n",
            "Currently at source number: 23/47 ( Ward Churchill )\n",
            "Currently at source number: 24/47 ( Mycena)\n",
            "Currently at source number: 25/47 ( Battle of Sullivan 's Island )\n",
            "Currently at source number: 26/47 ( Air Rhodesia Flight)\n",
            "Currently at source number: 27/47 ( Back Off)\n",
            "Currently at source number: 28/47 ( Where the Streets Have No Name )\n",
            "Currently at source number: 29/47 ( B of the Bang )\n",
            "Currently at source number: 30/47 ( Randy Blythecase )\n",
            "Currently at source number: 31/47 ( Fort Scott National Historic Site )\n",
            "Currently at source number: 32/47 ( Hu)\n",
            "Currently at source number: 33/47 ( Capel)\n",
            "Currently at source number: 34/47 ( Die Another Day )\n",
            "Currently at source number: 35/47 ( Raid on Manila ( 1798 ) )\n",
            "Currently at source number: 36/47 ( 1888 â 89 New Zealand Native football team )\n",
            "Currently at source number: 37/47 ( Battle of's Ridge )\n",
            "Currently at source number: 38/47 ( J. C. W.)\n",
            "Currently at source number: 39/47 ( Mount)\n",
            "Currently at source number: 40/47 ( Scientology in Germany )\n",
            "Currently at source number: 41/47 ( The Same Old Story )\n",
            "Currently at source number: 42/47 ( Training Day ( The Office ) )\n",
            "Currently at source number: 43/47 ( Butterfly World Tour )\n",
            "Currently at source number: 44/47 ( Frank)\n",
            "NLTK Test Sentence Creation Time: 0.14 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train-NLP:\", [sum(len(sentences) for sentences in nlp_train_source_structure)], \"Found:\", nlp_train_found)\n",
        "print(\"Test-NLP:\", [sum(len(sentences) for sentences in nlp_test_source_structure)], \"Found:\", nlp_test_found)\n",
        "print(\"Train-NLTK:\", [sum(len(sentences) for sentences in nltk_train_source_structure)], \"Found:\", nltk_train_found)\n",
        "print(\"Test-NLTK:\", [sum(len(sentences) for sentences in nltk_test_source_structure)], \"Found:\", nltk_test_found)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP5xwQcv7cTi",
        "outputId": "f32a6f73-013d-47a4-996b-8b0fa443de02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train-NLP: [71412] Found: 73039\n",
            "Test-NLP: [7156] Found: 7320\n",
            "Train-NLTK: [76561] Found: 76900\n",
            "Test-NLTK: [7834] Found: 7869\n"
          ]
        }
      ]
    }
  ]
}